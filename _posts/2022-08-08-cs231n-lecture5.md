---
layout: post
title: CS231n Lecture5 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html) 강의와 2022년 슬라이드를 바탕으로 작성되었습니다.




<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183777442-2c82fd25-224c-468e-9f3b-0304e79b1542.png">
</p>

<br>





# Recap: Image Classification with Linear Classifier

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183777842-b2b9a170-c93b-4da3-9223-c38cf9e4654c.png">
</p>

Linear Classifier를 다시 생각해보면 3가지 관점이 있었습니다.
* **<span style="background-color: #fff5b1">Algebric Viewpoint</span>**
    * 이 관점에서는 Linear Classifier를 단순히 행렬과 벡터의 곱에 bias 벡터를 더한 것으로 봅니다.
    * 이렇게 바라보면 Bias Trick한 장점이 있습니다.
        * Bias Trick은 bias를 weight의 마지막 열로 추가시키고, data vector의 마지막 원소로 1을 추가하면, 단순히 곱하기만 하면 간단히 계산할 수 있습니다.
    * 또 다른 이점은 Linear Classifier의 결과가 이름대로 결과도 Linear 하다는 것을 한눈에 알 수 있습니다.
* **<span style="background-color: #fff5b1">Visual Viewpoint(Template Matching)</span>**
    * Weight의 각 행을 input image의 크기로 변환하고 각각 image와 내적을 하고 weight의 각 행에 대응하는 bias를 더해주는 관점이 있는데, 이를 Visual Viewpoint 또는 template matching이라 합니다.
        * 왜냐하면 classifier가 wieght을 학습하는 것을 카테고리 당 하나의 이미지 템플릿을 학습한다고 볼 수 있기 때문입니다.
    * Linear Classifier는 카테고리 당 1개의 템플릿만 학습 가능하지만, 이미지 속의 물체는 항상 같은 방향, 같은 자세, 같은 색상 등으로 존재하지 않습니다.
* **<span style="background-color: #fff5b1">Geometric Viewpoint</span>**
    * 이 관점에서는 Linear Classifier에서 이미지는 고차원의 유클리드 공간에서 존재하며, 각 카테고리는 각각 1개의 초평면이 존재하며 이 초평면이 이미지를 2등분한다고 봅니다.
    * 이러한 관점에서 보면 고차원 공간에서의 Linear Classifier를 3차원인 우리가 완전히 이해하기 어려울 수 있지만, 적어도 Linear Classifier가 어떤 것은 할 수 있고, 어떤 것은 할 수 없는지 그 한계를 이해하는 것에 도움이 됩니다.

<br>




# Problem: Linear Classifiers are not very powerful

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183782879-30b3964f-03b9-472c-9461-4f4f4b266a5e.png">
</p>

Linear Classifier는 간단한만큼, Geometric Viewpoint나 Visual Viewpoint에서 확인할 수 있듯이, **<span style="color:red">한계</span>**가 많습니다.
* Visual Viewpoint
    * Linear classifiers는 class당 하나의 template만 학습합니다.
* Geometric Viewpoint
    * Linear classifiers는 오직 linear decision boundaries만 그릴 수 있습니다.

<br>
<br>




# Last Time: Neural Networks

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183783110-ba27b082-a1ea-4cdc-87d6-cca3377c340c.png">
</p>

또한 Neural Networks와 선형 함수들을 살펴보았습니다.
* 선형 레이어를 쌓고 그 사이에 비선형 레이어를 추가하여 Neural Network를 만들었습니다.
* 다양한 종류의 class를 올바르게 분류하기 위해 "중간 단계의 template"을 학습시켰습니다. 그리고 이 template들을 결합해서 최종 클래스 스코어를 계산합니다.

하지만 학습시 다차원 배열로 구성된 이미지를 벡터로 만들어서 처리했는데, 이 과정에서 **<span style="color:red">spatial structure는 사라지게 되었습니다.</span>** 그래서 이번 시간에는 spatial structure를 처리할 수 있는 Convolutional Network에 대해 배울 것입니다.

<br>
<br>





# Next: Convolutional Neural Networks

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183783757-3ae5be0b-38dd-42ba-bc3c-03b40fe35c13.png">
</p>

**<span style="color:red">Convolutional Layer</span>** 는 기본적으로 NN과 비교할때 **<span style="color:red">"Spatial Structure(공간적 구조)"</span>** 를 유지합니다.

<br>
<br>





# The history of nNeural Networks and CNNs

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183783991-aa9cda58-4516-4c71-a6fb-35f6581a5927.png">
</p>

* 1957년, Frank Rosenblatt가 Mark I Perceptron machine을 개발했습니다.
    * 이 기계는 "perceptron"을 구현한 최초의 기계
    * "Perceptron"은 $Wx + b$ 와 유사한 함수를 사용하며, 출력 값이 0 또는 1입니다.
    * 가중치 W를 Update 하는 Update Rule이 존재합니다.
    * 하지만 당시에는 backprob이라는 개념이 없어서, 단지 W를 이리저리 조절하면서 맞추는 식이었습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183784175-649d4fce-9ab8-45ff-9272-972ec3898b8c.png">
</p>

* 1960년, Widrow와 Hoff가 Adaline and Madaline을 개발했습니다.
    * 최초의 Multilayer Perceptron Network
    * 비로소 Neural network와 비슷한 모양을 하기 시작하긴 했지만, 아직 Backprob같은 학습 알고리즘은 없었습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183784212-fca01d61-f134-48a7-873a-edad5099b02d.png">
</p>

* 1986년, 최초의 Backporp을 Rumelhart가 제안하였습니다.
    * Chain rule과 Update rule을 볼 수 있습니다.
    * 이때 최초로 network를 학습시키는 것에 관한 개념이 정립되기 시작했습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183784258-f4fd5612-0616-4db4-9a3f-e83b6393106b.png">
</p>

* 하지만 그 이후로 NN을 더 크게 만들지는 못했으며, 한동안은 새로운 이론이 나오지 못했고, 널리 쓰이지도 못 했습니다.
* 하지만 2000년대부터 다시 발전하기 시작했습니다.
* 2006년, Geoff Hinton 과 Ruslan Salakhutdinov의 논문에서 DNN의 학습가능성을 선보였고, 실제로 아주 효과적이라는 것을 보여주었습니다.
    * 하지만 아직까지 모던한 NN는 아니었고, backprop이 가능하려면 아주 세심하게 초기화를 해야 했습니다.
    * 그래서 전처리 과정이 필요했고, 초기화를 위해 RBM을 이용해서 각 히든레이어 가중치를 학습시켜야 했습니다.
    * 초기화된 hidden layer를 이용해서 전체 신경망을 backprop하거나 fine tune하는 것이었습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183784906-afd730bc-d492-4405-af16-09a28ce81dca.png">
</p>

* 실제 neural networks이 유행하기 시작한 때는 2012년입니다.
    * neural networks이 음성인식에서 아주 좋은 성능을 보였습니다.
        * Hintin lab에서 나온 것인데 acoustic modeling과 speech recognition에 관한 것.
    * 또한 2012년에는 Hinton lab의 Alex Krizhevsky에서 **<span style="background-color: #fff5b1">영상 인식에 관한 landmark paper</span>** 가 등장합니다.
        * ImageNet Classification에서 최초로 neural networks(ConNets)을 사용했고, 결과는 정말 놀라웠습니다.
        * AlexNet은 ImageNet benchmark의 Error를 극적으로 감소시켰습니다.

<br>
<br>





# How did CNN become famous?

구체적으로 "CNN이 어떻게 유명해졌는지" 에 대해 살펴보겠습니다.
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183784928-0ce51046-e38d-464a-907a-3f0c1d0cef5a.png">
</p>

* 1950년대, Hubel과 Wiesel이 일차시각피질의 뉴런에 관한 연구를 수행했습니다.
    * 고양이의 뇌에 전극을 꽂는 실험을 했고, 고양이에게 다양한 자극을 주며 실험을 했습니다.
    * 이 실험에서 **<span style="background-color: #fff5b1">뉴런이 oriented edges와 shapes같은 것에 반응</span>** 한다는 것을 알아냈습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183784963-755c79c7-2aa2-4238-822e-ccfadc218034.png">
</p>

* 이 실험에서 내린 몇 가지 결론은 아주 중요했습니다.
* 그중 하나는 바로 피질 내부에 **<span style="background-color: #fff5b1">지형적인 매핑(topographical mapping)</span>** 이 있다는 것입니다.
    * 피질 내 서로 인접해 있는 세포들은 visual field내에 어떤 지역성을 띄고 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183784990-cea2277c-ccb8-4bb9-8ab4-0f9bb58e7f5c.png">
</p>

* 또한 **<span style="background-color: #fff5b1">뉴런들이 계층구조를 지닌다는 것</span>**도 발견했습니다.
    * 다양한 종류의 시각자극을 관찰하면서 시각 신호가 가장 먼저 도달하는 곳이 바로 Retinal ganglion 이라는 것을 발견합니다.
        * Retinal ganglion cell은 원형으로 생긴 지역입니다.
    * 가장 상위에는 Simple cells이 있는데, 이 세포들은 다양한 edges의 방향과 빛의 방향에 반응했습니다.
    * 또한 Simple Cells이 Complex cells과 연결되어 있다는 것을 발견했습니다.
    * Complex cells는 빛의 방향 뿐만 아니라 움직임에서 반응했습니다.
    * 복잡도가 증가함게 따라, 가령  hypercomplex cells은 끝 점(end point)과 같은것에 반응하게 되는 것입니다.
* 이런 결과로부터 **<span style="color:red">"corner"</span>** 나 **<span style="color:red">"blob"</span>** 에 대한 아이디어를 얻기 시작했습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786082-63976d91-662e-4a66-a9b8-969492fb0d53.png">
</p>

* 1980년, **<span style="background-color: #fff5b1">neocognitron</span>** 은 Hubel과 Wiesel이 발견한 simple/complex cells의 아이디어를 사용한 최초의 NN입니다.
    * Fukishima는 simple/complex cells을 교차시켰습니다.
    * Simple cells은 학습가능한 parameters를 가지고 있고, Complex cells은 pooling과 같은 것으로 구현했는데 작은 변화에 Simple cells보다 좀 더 강건합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786103-d8873f79-8dab-4d01-b924-409ce981124d.png">
</p>

* 1998년 **<span style="background-color: #fff5b1">Yann LeCun</span>** 이 최초로 NN을 학습시키기 위해 Backprob과 gradient-based learning을 적용했습니다.
    * 문서 및 우편번호의 숫자 인식에 아주 잘 동작했습니다.
    * 하지만 아직 이 Network를 더 크게 만들 수는 없었으며, 데이터가 단순했습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786129-41bb8a9a-a34c-4e81-a3a6-f3e44df6aaf3.png">
</p>

* 이후 **<span style="background-color: #fff5b1">2012년 Alex Krizhevsky(AlexNet)</span>** 가 CNN의 현대화 바람을 이르켰습니다.
    * Yann LeCun의 CNN과 크게 달라보이진 않으며, 다만 더 크고 깊어졌습니다.
* 가장 중요한 점은 지금은 ImageNet dataset과 같이 대규모의 데이터를 활용할 수 있다는 것입니다.
* 또한 GPU의 힘도 있었습니다.

<br>
<br>





# Fast-forward to today: ConvNets are everywhere

ConvNet이 어디 쓰이는지, 어떤 Task들이 있는지 살펴보겠습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786492-1f74a0c0-9d54-4a7f-8cef-a7ec9ce09fea.png">
</p>

* ConvNets은 모든 곳에 쓰입니다.
* AlexNet의 ImageNet 데이터 분류 결과를 살펴보면, 이미지 검색에 정말 좋은 성능을 보이고 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786528-84ab30a0-37cc-4fd8-9123-ac107ffb76f3.png">
</p>

* Detection에서도 ConvNet을 사용합니다.
    * 영상 내에 객체가 어디에 있는지를 아주 잘 찾아냅니다.
* segmentation은 단지 네모박스만 치는 것이 아니라 나무나 사람 등을 구별하는데 픽셀 하나 하나에 모두 레이블링하는 것입니다.
    * 이런 알고리즘은 자율주행 자동차에 사용할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786549-0c339a85-44ec-4dfe-b55a-852c966281e8.png">
</p>

* 대부분의 작업은 GPU가 수행할 수 있으며, 병렬처리를 통해 ConvNet을 아주 효과적으로 훈련하고 실행시킬 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786609-5e73b71f-8ffb-4de8-8841-f02c4fbb7ac6.png">
</p>

* 위 슬라이드 모두는 Convnet을 활용할 수 있는 다양한 애플리케이션의 예 입니다.
* 얼굴인식의 예를 보면 얼굴 이미지를 입력으로 받아서 이 사람이 누구인지에 대한 확률을 추정할 수 있습니다.
* 또한 ConvNets을 비디오에도 활용할 수 있는데, 단일 이미지의 정보 뿐만 아니라 시간적 정보도 같이 활용하는 방법입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786629-73d39a58-eee3-45c8-96b9-470fde9d7ec6.png">
</p>

* pose recognition도 가능합니다.
    * 어깨나 팔꿈치와 같은 다양한 관절들을 인식해 낼 수 있습니다.
* Convnet을 가지고 게임도 할 수 있습니다.
    * 강화학습을 통해서 Atari 게임을 하거나, 바둑을 두는 모습도 볼 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786647-5d23a1fc-eca3-4036-bb4a-c0e880cc76e6.png">
</p>

* 또한 의학 영상을 가지고 해석을 하거나 진단을 하는데도 이용할 수 있습니다.
* 또한 은하를 분류하거나 표지판을 인식하는데도 쓰입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786662-1dcdb843-de0c-4aba-bbf1-39229e21eff6.png">
</p>

* 또한 항공지도를 가지고 어디가 길이고 어디가 건물인지를 인식하기도 합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786673-6d89feb8-5182-4911-840a-09c93a16e4b9.png">
</p>

* Image Captioning도 있습니다.
    * 이미지가 주어지면 이미지에 대한 설명을 문장으로 만들어 내는 것입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183786685-ded8d5f7-3d05-4776-b233-2a99d1911062.png">
</p>


* 또한 Neural Network를 이용해 예술작품도 만들어 낼 수 있습니다.
    * 왼쪽은 Deep Dream 알고리즘의 결과
* 또한 Style Transfer라는 방법은 원본 이미지를 가지고 특정 화풍으로 다시 그려주는 알고리즘도 있습니다.

그럼, 지금부터 Convolutional Neural Network가 어떻게 작동하는지를 살펴보겠습니다.

<br>
<br>





# Convolutional Neural Networks(CNNs)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183788425-f865558c-efeb-4b1d-9225-8e13a22f72d0.png">
</p>

<br>





# Recap: Fully Connected Layer

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183788966-d87efb6c-d3bb-40a1-b0c8-cc7c7afa38fb.png">
</p>

* Fully Connected Layer의 하는 일은 어떤 벡터를 가지고 연산을 하는 것이었습니다.
* Fully connected layer에 $32\ ×\ 32\ ×\ 3$ 의 input image가 입력되었을 때, $32\ ×\ 32\ ×\ 3$ 의 matrix가 1차원적으로 펼쳐져서 $1\ ×\ 3072$ 의 shape으로 input vector가 만들어지게 되고, $1\ ×\ 3072$ size의 input vector가 $3072\ ×\ 10$ size의 W 행렬(가중치 행렬)과 dot product(내적)되어 결과적으로 $1\ ×\ 10$ size의 output vector(activation)가 만들어지게 됩니다.
    * 10개의 출력이 있으며, **<span style="background-color: #fff5b1">각 1개의 출력은 Neuron의 한 값</span>** 이라고 할 수 있습니다.
* **<span style="color:red">이 과정에서 이미지 데이터의 주변 위치 정보에 대한 내용이 무시됩니다.</span>**
    * 이러한 문제를 해결하기 위한 방법이 바로 Convolution layer를 이용한 **<span style="background-color: #fff5b1">CNN</span>** 입니다.
    * **<span style="background-color: #fff5b1">CNN은 기존의 input image에 대한 구조를 보존</span>** 시키며 **<span style="background-color: #fff5b1">주변 픽셀에 대한 정보 (위치정보)를 고려</span>** 할 수 있게 만든 방법 입니다.
    * **<span style="color:red">즉, 기존의 FC Layer가 입력 이미지를 길게 쭉 폈다면, CNN은 기존의 이미지 구조를 그대로 유지하게 됩니다.</span>**

<br>





# Convolution Layer

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183790082-fc22817c-8fe7-4228-8427-f4a5f35dff69.png">
</p>

* Convolutional Layer는 입력보다 작은 크기의 **<span style="background-color: #fff5b1">filter가 이미지 위로 슬라이딩하면서 공간적으로 dot product를 수행하는 방식으로 동작</span>** 합니다.
    * 여기서 dot product는 같은 위치의 input image와 filter의 픽셀간 곱셈 연산 후 모두 더하는 연산을 의미합니다.
* 또한, **<span style="color:red">filter의 depth는 input image의 depth와 항상 동일</span>** 합니다.
    * filter의 width나 height은 필터의 크기를 얼마나 크게 할 것이냐에 따라 임의로 정할 수 있지만, 채널(depth)는 항상 input data 의 채널수(depth)와 동일해야합니다.
    * 여기서 하나의 필터는 아주 작은 부분만을 취합니다. 즉 전체 $32\ x\ 32$ 이미지의 $5\ x\ 5$ 만 취합니다.
    * 하지만 깊이를 보면 전체 깊이를 전부 취합니다.
    * 그러므로 슬라이드의 filter의 shape은 $5\ x\ 5\ x\ 3$ 입니다.
    * 이 **<span style="color:red">filter</span>** 에 해당하는 matrix가 바로 **<span style="color:red">가중치(W)행렬</span>** 입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183791236-e16f1c2f-2c9f-4374-b4ab-cda2e56e4f6b.png">
</p>

* 반복하면, Convolutional Layer는 filter를 가지고 image에 대해 dot product를 input channel별로 수행하고, 이들을 모두 더해서 하나의 값을 출력합니다.
    * 즉, 같은 위치의 픽셀끼리 element wise 곱셈 후 모두 더하는 것을 channel별로 수행하고, channel별 결과들을 모두 더해서 한개의 값을 출력하게 됩니다.
* convolution 연산시 **<span style="background-color: #fff5b1">$W^Tx + b$</span>** 라고 표기한 이유는, 내적을 수학적으로 표현하기 위해서 1D로 펼친 후 dot product하는 것과 같은 결과이기 때문입니다.
    * 각 원소끼리 Convolution을 하는 거나 쭉 펴서 내적을 하는거나 똑같은 일을 하는 것이기 떄문입니다.
    * 즉 필터를 이미지에 겹쳐놓고 해당하는 값들을 서로 곱하는데, **<span style="background-color: #fff5b1">실제로는 모두 펴서 벡터간 내적을 구하는 것입니다.</span>**

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183792550-86453968-5fde-4be0-8ab3-8829a764d78c.png">
</p>

* Convolution 연산시 슬라이딩은 이미지의 좌상단부터 시작하여, 필터의 중앙에 값들을 모으게 됩니다.
* filter와 겹쳐진 input 이미지 부분에 대해 내적(dot product)를 하여 하나의 값이 나오게 되고, filter가 일정 간격으로 움직여(sliding) 전체 이미지에 대해 이 작업을 반복하면 최종의 **<span style="color:red">activation map (output matrix)</span>** 이 생성됩니다.
* 출력 행렬의 크기는 슬라이드를 어떻게 하느냐에 따라 다르게 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183793732-dfb37f91-7768-4b50-ba7b-44f1b3925c80.png">
</p>

* 정리하면 Filter를 통해 출력된 각 위치에서의 값들을 이미지 형태로 합친 것을 **<span style="color:red">Activation Map</span>** 이라고 합니다.
* 보통 Convolution Layer에서는 **<span style="background-color: #fff5b1">여러개의 filter를 사용</span>** 합니다.
    * **<span style="color:red">filter마다 다른 특징을 추출하고 싶기 때문입니다.</span>**
    * 즉 한 Layer에서 원하는 만큼 여러개의 필터를 사용할 수 있습니다.
* 여러 개의 filter를 사용하면, 더 많은 수의 activation map을 출력할 수 있습니다.
    * 즉, output channel의 개수는 조절 가능합니다.
* 위 슬라이드는 $3\ x\ 5\ x\ 5$ 크기의 filter를 통해 총 6개의 output channel을 출력한 것이며, 이때 filter의 shape는 다음과 같습니다.
    * 6개의 $3\ x\ 5\ x\ 5$ 크기의 filter를 사용하면 6개의 Activation Map이 각각 $1\ x\ 28\ x\ 28$ 크기를 가집니다.
    * $Channel_out\ ×\ Channel_in\ ×\ Height_filter\ ×\ Width_filter\ =\ 6\ ×\ 3\ ×\ 28\ ×\ 28$
    * 따라서, Convolutional layer에서 filter는 4차원의 shape를 가지게 됩니다.
* 또한 이 과정에서 이미지 전체적인 spatial structure는 사라질 수 있지만, **<span style="color:red">local spatial structure는 보존</span>** 될 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183798672-5eefb77d-1e7f-449a-83b2-ac03a6133338.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183798760-ffac0652-c7cc-433e-b705-757051bd4011.png">
</p>

또한 당연히 Batch 개념을 적용할 수 있으며, **<span style="background-color: #fff5b1">Batch로 처리</span>** 를 한다면 위 그림과 같이 출력됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183798946-955d4722-4aa3-4521-ad0c-ec5d90c7b1cd.png">
</p>

* Convolutional Network 구조를 가진 CNN 에서는 위와 같이 **<span style="background-color: #fff5b1">Convolution layer가 연속된 형태로 만들어져 깊은 Network를 형성</span>** 합니다.
    * 보통 Linear한 Convolution layer에 비선형성을 부여하기 위해 ReLU와 같은 NonLinearity activation function를 붙여서 한 층을 이루게 됩니다.
    * 즉, Conv-ReLU 의 형태의 layer가 반복된다고 생각하면 됩니다.
    * 또한 pooling layer가 적용되기도 합니다.
    * 그리고 각 Layer의 출력은 다음 Layer의 입력이 됩니다.
* 정리하면 이러한 각 filter는 input data와 연산되어 각각의 activation map을 출력합니다. 즉, 각 layer 에 있는 filter 들이 **<span style="color:red">계층별로 학습</span>** 이 되게 됩니다.

<br>
<br>





# What do convolutional filters learn?

그렇다면 convolutional filters들은 어떤것을 학습하는지 의문이 듭니다.

## Linear classifier: One template per class

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183802089-d1e30ca6-a55f-40ee-b0cb-6a0e6542adc9.png" style="zoom:40%;">
</p>

Linear classifier는 class당 하나의 weights vector만 가지고 있기에, **<span style="background-color: #fff5b1">class당 한개의 template을 학습하기에 하나의 template을 가진 형태</span>** 로 볼 수 있습니다.

<br>

## MLP: Bank of whole-image templates

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183807246-6eb538da-5cd7-4363-b3a7-ad3cbcebc494.png" style="zoom:40%;">
</p>

Multi-Layer Perceptron을 살펴 본다면, hidden layer의 크기에 대응하는 **<span style="background-color: #fff5b1">$W$ 의 크기만큼 여러개의 template을 갖는(Bank of whole-image templates을 갖는) 형태</span>** 입니다.

<br>

## Convolutional Filters

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183807931-90c31770-02fb-4921-ba5d-fd6a424160d1.png">
</p>

위 슬라이드는 각 filter 가 어떻게 학습이 되는지 시각적으로 보기 위해 VGG-16 이라는 Conv layer를 이용한 네트워크에 강아지 사진을 학습시키고, 그에 따라 각 layer 에서 학습이 되어 나오는 filter를 시각적으로 나타냈습니다.

* Convolution Layer도 filter들이 **<span style="color:red">계층별로 학습</span>** 이 됩니다.
* Convolution Layer는 Fully-Connected Layer가 입력과 동일한 이미지의 템플릿을 학습하는 것과 달리, **<span style="color:red">filter가 local template을 학습</span>** 한다는 점이 특징입니다.
    * **앞 쪽**(input 과 가까운 쪽, 초기 layer)에 있는 layer의 filter에서는 **<span style="background-color: #fff5b1">모서리(edge)</span>** 와 같은 단순한 **<span style="background-color: #fff5b1">low-level features</span>** 를 학습합니다.
    * **중간**에 있는 layer의 filter는 **<span style="background-color: #fff5b1">코너(corner)</span>** 와 **<span style="background-color: #fff5b1">얼룩(blobs)</span>** 과 같이 조금 더 복잡한 **<span style="background-color: #fff5b1">midle-level features</span>** 를 학습합니다.
    * 그리고 **더 깊은** layer에 있는 filter는 객체와 닮은 듯한 **<span style="background-color: #fff5b1">high-level feature</span>** 를 학습하게 됩니다.
* 그리고 **output에서의 feature vector의 각 원소는 각 filter(local pattern)와 얼마나 일치하는지를 나타내는데,** 이건 Hubol과 Wiesel의 연구에서 고양이가 시각적인 local pattern에 반응한 것과 비슷합니다.
    * 네트워크에 앞쪽에서는 단순한 것들일 처리하고, 뒤로 갈수록 점점 더 복잡해 지는 식입니다.


특히 여기에서 시각화 한 것은 다음과 같습니다.
* 각 그리드의 요소가 **<span style="color:red">하나의 뉴런(필터)</span>** 입니다.
* 그리고 시각화 시킨 이 뉴런의 모습은 바로 **<span style="color:red">이미지가 어떻게 생겨야 해당 뉴런의 활성을 최대화시킬 수 있는지</span>** 를 나타내는 것입니다.
* 즉 이미지가 뉴런과 비슷하게 생겼으면 출력 값을 큰 값을 가지게 됩니다.


정리하면 다음과 같습니다.
* 각 layer의 필터는 위와 같이 hierarchical하게 feature를 학습합니다.
    * 낮은 layer의 filter일수록, 저수준의 feature를 학습합니다.
        * Ex) 가장자리, 선 등
    * 높은 layer의 filter일수록, 고수준의 feature를 학습합니다.
        * Ex) 코너, 동그라미 등
* Layer는 계층에 따라 단순/복잡한 특징이 존재합니다.
* 기본적으로 그리드의 각 요소는 각 뉴런의 활성을 최대화시키는 입력의 모양을 나타내게 됩니다.
    * 그런 의미에서 뉴런이 어떻게 생겼는지를 의미합니다.

<br>





<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183821613-f7e49b6e-f564-4b8e-bba8-b0d9931bf5b9.png">
</p>

* 위와 같은 다양한 filter와 input image가 convolution 연산되어 각 필터가 만든 출력값이자 다양한 Activation map을 생성합니다.
* activation map를 보면 이미지에서 어떤 부분이 filter 에 크게 반응하는지를 알 수 있습니다.

<br>





<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183822119-53db64c6-2b22-409b-8d3a-07204fa8877f.png">
</p>

결과적으로 CNN이 어떻게 수행되는지를 살펴보면 입력 이미지는 여러 레이어를 통과하게 됩니다.
* 가령 첫 번째 Conv Layer후에는 non-linear layer를 통과합니다.
* Conv, ReLU, Conv, ReLU를 하고나면 pooling layer를 거치게 됩니다.
* 그리고 CNN의 끝단에는 마지막 Conv 출력 모두와 연결되어 있으며 최종 스코어를 계산하기 위해 FC-Layer가 있습니다.

<br>
<br>





# Spatial Dimensions: Stride

위에서 $32\ ×\ 32\ ×\ 3$ size의 이미지에 $5\ ×\ 5\ ×\ 3$ filter로 convolution 연산을 진행했을때, $28\ ×\ 28\ ×\ 1$ 의 output activation map 이 나오는 것을 확인했습니다.

그렇다면 어떻게 해당 size의 output이 생성되는지 자세히 살펴보겠습니다. 간단한 예시로 $7 x 7$ 입력에 $3 x 3$ 필터가 있다고 할 때, 필터를 이미지의 좌상단부터 진행됩니다.

<br>



## 7x7 input(spatially) assume 3x3 filter applied with stride 1

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183853511-54f8f1f0-87b4-46bd-90d6-7cbbeb036753.png">
</p>

* stride가 1이기 때문에 5x5 Output이 나옵니다.

<br>



## 7x7 input(spatially) assume 3x3 filter applied with stride 2

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183854465-5221e432-6141-4f59-bd59-3c0e60069fd8.png">
</p>

* stride가 2이기 때문에 3x3 Output이 나옵니다.

<br>



## 7x7 input(spatially) assume 3x3 filter applied with stride 3

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183854628-8f2dde26-0ab4-402c-a5a3-5a0da5ea39c3.png">
</p>

* stride 3 을 적용한다면, 이미지 size, filter size 와 맞지 않게 됩니다.
    * 즉 이미지에대해 슬라이딩해도 필터가 모든 이미지를 커버할 수 없습니다.
* 따라서 이렇게 잘 맞지않는 stride 를 적용하게 된다면 잘 동작하지않게 됩니다.
    * 불균형한 결과를 볼 수도 있기 때문입니다.

<br>



## Output size 계산

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183855017-242ac9d0-67fa-4165-8edb-7d7977af684a.png">
</p>

* Output(Activation Map)의 가로, 세로 크기는 다음의 식으로 계산할 수 있습니다.
    * $(N\ −\ F)\ /\ stride\ +\ 1$
    * N: 입력 차원
    * F: 필터 사이즈
* 여기서, stride는 filter가 한번에 움직이는 크기를 의미합니다.

<br>





# Spatial Dimensions: Padding + Stride

stride만을 사용할 때 문제점들을 살펴보겠습니다.

* filter를 통해 convolution을 거치면 **<span style="color:red">가장자리에 있는 이미지 데이터들은 filter의 중앙에 닿지 않는 문제점.</span>**
    * **즉, 가장자리 부분 이미지 데이터의 특징들은 filter를 통해 잘 추출이 되지 못합니다.**
* CNN 에서 여러 개의 convolutional layer을 거치다 보면 **<span style="color:red">점점 output size가 작아지는 문제점.</span>**
    * 이렇게 된다면, **깊은 CNN 구조에서는 output의 size가 너무 작아져 input 이미지의 특징들을 잘 못 나타낼 수도 있습니다.**

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183856802-d8f76f23-3b25-4c4d-b5b3-4dcf816ba382.png">
</p>

* 출력의 size를 의도대로 만들어 주기 위해 가장 흔히 쓰는 기법은 **<span style="color:red">zero-padding</span>** 입니다.
    * 즉 Image의 테두리에 Padding을 추가해 Output의 크기를 동일하게 유지할 수도 있습니다.
    * Padding은 일반적으로 zero padding이 가장 잘 동작합니다.
* Padding을 추가하는 것은 filter를 통과하면서 크기가 너무 작아지면 많은 정보를 잃게 되기 때문에 이를 방지하기 위해서 사용하는 방법 중 하나입니다.
* 위 슬라이드 예시는, $7\ ×\ 7$ size의 이미지에 zero padding with 1 (1칸 zero padding)을 적용하여 $9\ x\ 9$ 의 input으로 만든다음, $3\ ×\ 3$ filter, stride 1 로 convolution 하였습니다.
    * 결과적으로 $7\ ×\ 7$ 의 output (activation map) 이 생성되는 것을 확인 할 수 있습니다.
* Output(Activation Map)의 가로, 세로 크기는 다음의 식으로 계산할 수 있습니다.
    * $(N\ +\ 2P\ -\ F)\ /\ stride\ +\ 1$
* 어떤 stride와 filter를 쓸건지를 일반적으로 정하는 방법이 있습니다.
    * 보통 filter는 $3 × 3$, $5 × 5$, $7 × 7$ 을 씁니다.
    * 또한 $F\ ×\ F$ size 의 filter with stride 1로 수행되는 conv layer가 있다고 할때, $(F\ −\ 1)\ /\ 2$ 칸의 패딩을 사용합니다.
    * 보통 $3 × 3$ filter에는 stride를 1을 줍니다.
    * 보통 $5 × 5$ filter에는 stride를 2을 줍니다.
    * 보통 $7 × 7$ filter에는 stride를 3을 줍니다.

<br>



## Quiz 1

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183920812-2eeaad22-8284-4199-bceb-7a6a35f47c94.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183921568-ceedee53-1a8b-4d88-9f26-201836404006.png">
</p>

* Padding이 있는 경우의 Output의 가로, 세로 크기는 다음과 같이 계산합니다.
    * $(N\ +\ 2P\ -\ F)\ /\ stride\ +\ 1\ =\ (32\ +\ 2*2\ -\ 5)\ /\ 1\ +\ 1\ =\ 32$
* 따라서, 정답은 $32\ ×\ 32\ ×\ 10$ 입니다.

<br>



## Quiz 2

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183922649-eb441702-fa80-4d1a-a90c-3a0f48745d6a.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183922704-ef80a6a6-a79d-4c05-b278-a8f7a7d08c83.png">
</p>

* Parameter는 filter에 존재하고, bias term을 고려해야합니다.
* 먼저, bias term 1개를 고려한 각 filter의 parameter 수는 다음과 같다.
    * $5\ ×\ 5\ ×\ 3\ +\ 1\ =\ 76$
* 여기에 output channel의 크기를 곱하면, 전체 parameter수를 구할 수 있습니다.
    * $10\ ×\ 76\ =\ 760$

<br>





# Spatial Dimensions: Receptive Fields

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183924957-a3d2454c-72a9-42c5-9008-edfab5d79161.png">
</p>

* Convolution layer가 무엇을 하는지 생각해볼 수 있는 또다른 방식이 **<span style="color:red">Receptive field</span>** 입니다. 이는 **<span style="background-color: #fff5b1">output image의 각 spatial position이 input image에서 얼마만큼의 region에 영향을 받는가</span>** 를 뜻합니다.
* 다시 이야기를 하면, Convolution Layer에서의 Output의 한 원소는 해당 filter가 참조하는 local region의 영역의 값만 영향을 받는데, 이 **<span style="background-color: #fff5b1">local region을 'Receptive Field'</span>** 라 합니다.
* 그리고 Neural Network에서 Convolution Layer가 **transitive하게 연결되어 있는 경우** 해당 Layer의 Output의 원소는 그 Layer의 **<span style="background-color: #fff5b1">Input의 local region만이 아니라, 이전 Layer의 Input의 local region의 영향을 받는다</span>** 라고도 할 수 있습니다.
* 그래서 Receptive Field의 의미는 고정된 것이 아니고 보통 두 개의 의미로 사용됩니다.
    * 하나는 **<span style="color:red">Layer의 Input에서 영향을 받는 부분</span>** 을 말합니다.
    * 다른 하나는 **<span style="color:red">모델의 Input에서 영향을 받는 부분</span>** 을 말합니다.

<br>


<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183925706-822f17c9-9f38-437f-b007-8fdc8b738e54.png">
</p>

* 위 예시는 3-conv layers일때의 receptive field 예시입니다. output tensor부터 점점 확장해 나가며 $3 × 3$ regin이 $5 × 5$ region이 되고 $7 × 7$ region이 최종 receptive fild size가 됩니다.
* 이러한 receptive field size를 $1\ +\ L\ *\ (K\ -\ 1)$ 으로 계산할 수 있습니다.
* 하지만 input image의 해상도가 커질수록 그만큼 conv layer가 많아지며 (1024 x 1024 크기의 이미지에 kernel size가 위와같이 3일경우엔 500개 가량의 convlayer가 필요) output에서 각 spatial position이 매우 큰 receptive field size를 커버한다는 뜻이므로 좋지 않은 형태입니다.
* 위와같은 문제를 해결하기 위해 또다른 hyper parameter를 적용하여 downsample을 해줘야 합니다.

<br>





# Hyperparameters

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/184049655-0a163768-cfdc-40e1-ac5a-3d2fcd8c02cb.png">
</p>

* 어떤 필터를 쓸지, 몇개의 필터를 쓸건지, 필터 크기는 몇인지, stride는 몇으로 할지 zero-padding은 몇 으로 할지를 다 정해줘야합니다.
* 또한 앞서 말한 수식을 이용해서 출력의 사이즈가 어떻게 될 것인지, 전체 파라미터가 몇개가 될 것인지도 확인해 봐야 합니다.
* Convolutional layer의 하이퍼파라미터는 일반적으로 다음과 같이 설정합니다.
    * Filter의 크기 : $3\ ×\ 3$, $5\ ×\ 5$
    * Filter의 수 : 2의제곱
        * ex) 32, 64, 128, 512
    * Stride : 1 또는 2
    * Padding : 공간정보를 보존할 수 있는 무엇이든 가능

<br>
<br>





# 1x1 convolution

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183932487-403018c5-7025-423d-8ba7-2ffad588bb9a.png">
</p>

* $1 × 1$ Convolution은 공간적인 정보를 이용하지 않습니다.
    * 하지만 이 필터는 Depth만큼 연산을 수행하며, output filter의 수를 줄일 수 있습니다.
* $56\ ×\ 56\ ×\ 64$ size의 input에 $1\ ×\ 1\ ×\ 64$ filter 32개를 사용하여 convolution을 진행하면 $56\ ×\ 56\ ×\ 32$ size의 output이 생성되게 됩니다.
* 즉, $1\ ×\ 1\ ×\ D$ convolution layer는 **<span style="background-color: #fff5b1">차원을 줄여주는 역할</span>** 을 합니다.

<br>





# The brain/neuron view of CONV Layer

Brain Neuron 관점에서 Convolution layer에 대해 살펴보겠습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183937166-23eaba4f-c2fc-4188-8bfc-b00b417a935c.png">
</p>

* **<span style="background-color: #fff5b1">Fully Connected Layer</span>**
    * FC-layer의 뉴런들은 하나의 뉴런이 데이터의 모든 정보를 다 받아서 가중치(W) 와 bias를 이용해 계산하여 output을 출력했습니다.
* **<span style="background-color: #fff5b1">Convolutional Layer</span>**
    * 그러나 **<span style="color:red">Conv layer에서 하나의 뉴런(노드)이 전체 입력 데이터를 받아오는 것이 아니라는 점이 가장 큰 차이점</span>** 입니다.
    * 즉, 하나의 뉴런이 전체 입력데이터를 가져와 $W$, $b$ 를 이용해 계산을 하는 것이 아니라, **<span style="color:red">convolution layer의 하나의 filter의 크기만큼의 데이터만을 받아와 계산을 합니다.</span>**
    * 그러므로 ConvNet의 **<span style="color:red">각각의 뉴런은 Local connectivity</span>** 를 가집니다.

Convolutional Layer은 하나의 뉴런이 sliding 하면서 모든 입력 이미지(데이터)를 처리하는 것이 아니라, **<span style="background-color: #fff5b1">하나의 뉴런은 한 부분만 처리하고, filter가 sliding될때마다 또 다른 뉴런이 각각의 영역을 처리하는 방식</span>** 입니다.
* 이러한 많은 뉴런들이 모여서 전체 이미지를 처리하게 됩니다.
* spatial structure를 유지한 채로 Layer의 출력인 activation map을 만듭니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183939452-123bdd88-a413-4dc6-b246-5cd9c981a79b.png">
</p>

* 또한 위에서 언급했지만 **<span style="background-color: #fff5b1">한 뉴런이 담당하는 영역</span>** 을 **<span style="color:red">Receptive Field</span>** 라고 합니다.
* 다시 언급하면 "Receptive field" 란 **<span style="background-color: #fff5b1">한 뉴런이 한 번에 수용할 수 있는 영역</span>** 을 의미합니다.
* 즉, $5\ ×\ 5\ ×\ 3$ filter을 사용했을때, 각각의 뉴런의 receptive field 는 $5\ ×\ 5\ ×\ 3$ 라고 할 수 있습니다.

<br>





# Convolutional Layer와 Fully Connected Layer의 차이점

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183941285-cab62ce1-f28c-4ee6-b1bd-0f549cee2c40.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183941236-e0d33ab9-4807-4146-9f5a-8a3937ca8e5b.png">
</p>

위와 같이 $32\ ×\ 32\ ×\ 3$ 의 이미지에 5개의 $5\ ×\ 5\ ×\ 3$ filter를 사용한 convolution layer를 통과시키면 $28\ ×\ 28\ ×\ 5$ 의 output 이 생성됩니다.

* Fully Connected Layer에서 **<span style="background-color: #fff5b1">특징을 추출할때에는 전체 데이터를 모두 이용해서 feature를 추출합니다.</span>**
* Convolutional Layer에서는 **<span style="background-color: #fff5b1">한 receptive field의 데이터만을 이용해서 해당 구역만의 feature를 추출</span>** 한 것을 확인할 수 있습니다.
    * 한개의 receptive field 에서 5개의 값이 나온 것을 확인할 수 있습니다.
    * 정확하게 **<span style="color:red">같은 지역에서 추출된 서로다른 특징</span>** 이라 할 수 있습니다.
    * **<span style="color:red">각 필터는 서로 다른 특징을 추출하므로, 각 필터는 이미지에서 같은 지역을 돌더라도 filter 마다 서로 다른 특징을 뽑아낸다고 볼 수 있습니다.</span>**

<br>
<br>





# Pooling Layer

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/184038604-5eff2472-9a0c-4202-9f77-859974d9647d.png">
</p>

* **<span style="color:red">Pooling layer</span>** 는 **<span style="background-color: #fff5b1">데이터(features, representation)를 작게 만드는(downsampling)</span>** 하는 역할을 합니다.
    * 여기서 representation은 conv layer을 통과해 추출된 feature, activation map을 의미합니다.
    * representation이 작아지면 파라미터 수가 줄어듦으로써 모델이 좀 더 가벼워지는 효과가 있습니다.
* 즉, Pooling Layer는 **<span style="background-color: #fff5b1">Spatial downsampling을 수행하기 위한 목적으로 사용</span>** 되는 layer입니다.
    * pooling layer 를 통해 각 영역에 대한 정보의 손실을 최소화하며 downsampling 을 할 수 있습니다.
    * 참고) Convolutional Layer에서 stride를 크게 하는 것도 downsampling을 수행하는 방법 중 하나입니다.
        * 강의 녹화 기준으로, 최근의 연구들에서 Pooling Layer보다 stride를 크게 해서 수행하는 downsampling이 더 좋은 결과들을 얻고 있다고 합니다.
* Pooling Layer의 몇가지 특징은 다음과 같습니다.
    * Output channel의 수가 그대로 유지된다. (Convolutional layer와의 차이점)
        * 단순히 각 채널별로 filter를 움직이면서 값의 출력만을 반복합니다.
    * 학습되는 parameter가 없다.
    * Output의 가로, 세로 크기는 Convolutional layer에서와 같은 방식으로 계산할 수 있다.
        * $(N\ −\ F)\ /\ stride\ +\ 1$

<br>





# Max Pooling

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/184038612-4ef1b91e-e4a4-40e2-b1e9-fa0ee9b0afa4.png">
</p>

* **<span style="color:red">Max Pooling</span>** 은 filter에서 가장 큰 값만을 출력하는 방법이며, 가장 일반적인 방법입니다.
    * 직관적으로는 Neuron이 얼마나 많이 활성화되는가로 생각할 수 있는데, 이렇게 생각하는 것이 recognition이나 detection 등에서도 더 make sense하다고 볼 수 있습니다.
* Pooling Layer에서의 stride는 filter가 서로 안겹치게 하는 것이 일반적 입니다.
    * filter size 2, stride 2가 일반적.
* Downsampling에서 Pooling은 Convolution Layer에서의 stride 보다 더 선호되는데, Pooling에서는 어떠한 learnable parameter도 없으며 약간의 spatial shift에도 invariance가 보장되기 때문입니다.
    * 하지만, 최근에는 conv layer에서 stride를 조절해서 input의 이미지를 downsampling하는 방법들이 많이 쓰이고 있습니다.
    * 성능 또한, pooling layer를 따로 거치는 것보다 conv layer에서 stride를 조절해 자체적으로 input 데이터의 size를 줄이는 방법이 많이 사용되고 있습니다.



<br>





<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/184041183-fc312c3e-8697-4032-b1cd-55939933e501.png">
</p>

* Pooling layer의 하이퍼파라미터는 일반적으로 다음과 같이 설정합니다.
    * Filter의 크기 : $2\ x\ 2$, $3\ x\ 3$
    * Stride : 2
    * Padding : Pooling layer에서는 잘 사용하지 않음

<br>
<br>





# Fully Connected Layer(FC Layer)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/184041683-5dbf492c-b542-4018-96f7-8701ca1c7e56.png">
</p>

* CNN의 마지막에는 Fully Connected Layer를 통해 prediction을 수행합니다.
    * Convolutional Layer와 downsampling을 위한 Pooling로 이루어진 network에서 출력된 최종 output인 3차원 volume을 모두 펼친 후(stretch), 1차원 벡터로 만들어서 Fully Connected Layer에 입력으로 넣습니다.
* 즉, (Conv + Pooling으로) 공간 구조를 보존하며 추출해온 정보 모두를 모아서 추론(Inference)을 수행하는 것으로 이해할 수 있습니다.
* FC Layer를 거쳐 나온 값은 class별 score이고, **<span style="background-color: #fff5b1">각 값들은 각 필터가 가진 templete이 얼마나 활성화 되었는지를 표현합니다.</span>**

<br>
<br>





# Summary

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/184042739-ff4a02dc-82e6-479e-b7c1-8f9e0af4fd2e.png">
</p>

* 요약하면, CNN이 어떻게 동작하는지를 배웠습니다.
* 기본적으로는 Conv와 Pooling을 쌓아 올리다가 마지막에 FC Layer로 끝나게 됩니다.
* 네트워크의 필터는 점점 더 작아지고, 아키텍쳐는 점점 깊어지는 경향을 배웠습니다.
    * Pooling 이나 FC Layer를 점점 더 없애는 추세입니다.
    * 그냥 Conv Layer만 깊게 쌓는 것
* 전형적인 CNN 아키텍쳐는 Conv와 ReLU를 n번 반복합니다. 그리고 FC Layer가 이어집니다.
* Class score를 구하기 위해 softmax를 사용합니다.
* 정리하면 엄청 깊은 Conv, ReLU, Pool 시퀀스를 구성하게될 것이고, 그 다음 한두 번의 FC Layer가 이어지는 것입니다.




