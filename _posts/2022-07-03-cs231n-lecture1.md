---
layout: post
title: CS231n Lecture1 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html)을 바탕으로 작성되었습니다.



# Wekcime to CS231n

<img width="1132" alt="스크린샷 2022-07-04 오전 8 51 08" src="https://user-images.githubusercontent.com/77891754/177061555-a596bacc-8554-4cd3-baad-f979120d2ae5.png">

전 세계에서 매일 무수한 센서로부터 엄청나게 많은 시각 데이터가 쏟아져 나오고 있습니다.

CISCO에서 수행한 2015 ~ 2017년도까지의 한 통계자료에 따르면 인터넷 트래픽 중 80%의 지분은 바로 비디오 데이터입니다. 심지어 이 결과는 사진 같은 다른 데이터들을 모두 제외하고 비디오만 추산한 결과인데, 이 통계는 인터넷의 데이터 대부분이 시각 데이터라는 사실을 보여줍니다.

그러므로 시각 데이터들을 잘 활용할 수 있는 알고리즘을 잘 개발하는 것이 무엇보다 중요해졌습니다.

하지만 문제가 있는데, 이런 시각데이터는 해석하기 상당히 까다롭다는 점으로 사실상 이들을 이해하고 해석하는 일은 상당히 어렵습니다. 따라서 시각데이터로 서비스를 하려면 자동으로 시각데이터를 이해하고 분석하는 알고리즘을 개발하는 것이 관건입니다.

<img width="1135" alt="스크린샷 2022-07-03 오후 1 40 54" src="https://user-images.githubusercontent.com/77891754/177061508-dbcf55f7-97c6-480d-be67-6f7dd3ab3c6f.png">

컴퓨터 비전이라는 분야 주변에는 상당히 많은 분야가 존재하기 때문에 다양한 과학, 공학 분야들과 맞닥뜨리게 됩니다.

# Today's agenda
## A brief history of computer vision

비전(시각)과 컴퓨터 비전이 언제 어디에서 비롯됐고 현재는 어디쯤 왔는지를 살펴보겠습니다.

<img width="1134" alt="스크린샷 2022-07-03 오후 1 43 12" src="https://user-images.githubusercontent.com/77891754/177061515-50bd53ea-5004-4b55-b4df-f1611ea4eddf.png">

비전의 역사는 5억 4천만년전 시작되었는데, 그 시대의 지구 대부분은 물이었고 대부분 바다를 부유하는 일부 생물들만 존재했으며 눈(eyes)은 존재하지 않았습니다.

하지만 5억 4천만 년 전에 아주 놀라운 사건이 벌어졌는데, 동물학자들은 화석을 연구하면서 천만 년이라는 아주 짧은 시기 동안에 생물의 종이 폭발적으로 늘어났다는 것을 발견했습니다.

그 이유로 수많은 가설들이 있지만, 앤드류 파커(Andrew Parker)는 약 5억 4천만 년 전 최초의 눈(eyes)이 생겨났다는 것을 발견했습니다.

즉, 폭발적인 종 분화의 시기를 촉발시킨 것이며 생물들은 갑자기 볼 수 있게 되어서 능동적이게 되었으며, 일부 포식자들은 먹이를 찾아다니고 먹이들은 포식자로부터 달아나야만 했습니다.

그래서 비전의 도래는 진화적 군비경쟁을 촉발시켰고 생물들은 하나의 종으로
살아남으려면 빠르게 진화해야만 했습니다. 이것이 바로 비전의 태동입니다. 

우리 인간은 대뇌 피질의 50%가량의 뉴런이 시각처리에 관여하는데, Vision은 가장 큰 감각체계이며 우리가 생존하고, 일하고, 움직이고, 어떤 것들을 다루고, 의사소통하고, 오락을 즐기는 등 많은 것들을 가능하게 해줍니다.

비전은 동물들에게 중요하며 특히 지능을 가진 동물들에게 정말 중요합니다.


<img width="1134" alt="스크린샷 2022-07-03 오후 1 43 38" src="https://user-images.githubusercontent.com/77891754/177061520-b6aa6386-1382-4b42-b76e-a0ed5f11e875.png">

생물학자들은 비전의 매커니즘을 연구하기 시작했는데, 인간과 동물의 비전의 연구에 가장 영향력 있었을 뿐만 아니라 컴퓨터 비전에도 영감을 준 한 연구가 있었습니다. 1950/60년대 전기생리학을 이용한 Hubel과 Wiesel의 연구입니다.

그들이 묻고 싶었던 질문은 바로 "포유류의 시각적 처리 메커니즘은 무엇일까?" 였습니다. 그래서 그들은 고양이의 뇌를 연구하기로 합니다.

일차 시각 피질에는 다양한 종류의 세포가 있다는 것을 알았습니다. 그중 가장 중요한 세포가 있었는데 그 세포들은 아주 단순했습니다. 경계(edges)가 움직이면 이에 반응하는 세포들이었습니다.

물론 더 복잡한 세포들도 있긴 하지만, 주된 발견은 시각 처리가 처음에는 단순한 구조로 시작되며 그 정보가 통로를 거치면서 실제 세상을 제대로 인지할 수 있을 때까지 점점 복잡해진다는 것입니다.

<img width="1134" alt="스크린샷 2022-07-03 오후 1 43 51" src="https://user-images.githubusercontent.com/77891754/177061521-11c7333a-02e0-46df-bed7-015b39be8780.png">

컴퓨터 비전의 역사는 60년대 초반에 태동합니다.
Larry Roberts의 Block World 연구에서는 우리 눈에 보이는 사물들을 기하학적 모양으로 단순화시켰습니다.

이 연구의 목표는 우리 눈에 보이는 세상을 인식하고 그 모양을 재구성하는 일이었습니다.

<img width="1131" alt="스크린샷 2022-07-03 오후 1 44 18" src="https://user-images.githubusercontent.com/77891754/177061524-688f32d1-b82b-48bb-a2d7-1eed0a0ee95a.png">

또한 David Marr은 MIT의 비전 과학자였으며 그는 70년대 후기에 아주 유명한 책을 한 권 저술합니다.

이 책은 그가 비전을 무엇이라 생각하는지, 그리고 어떤 방향으로 컴퓨터 비전이 나아가야 하는지, 그리고 컴퓨터가 비전을 인식하게 하기 위해 어떤 방향으로 알고리즘을 개발해야 하는지를 다룬 책이었습니다.

<img width="1132" alt="스크린샷 2022-07-03 오후 1 44 30" src="https://user-images.githubusercontent.com/77891754/177061525-108be85d-818d-42b5-ac39-f043fa1953bc.png">

그의 저서에서, 우리가 눈으로 받아들인 "이미지"를 "최종적인 full 3D 표현"으로 만들려면 몇 단계의 과정을 거쳐야만 한다고 주장했습니다.

첫 단계는, 그가 부르길 "Primal Sketch"라고 하는 단계입니다. 이 과정은 주로 경계(edges), 막대(bars), 끝(ends), 가상의 선(virtual lines), 커브(curves), 경계(boundaries)가 표현되는 과정입니다.

이후의 다음 단계는, 그가 부르기로는 "2.5-D sketch"라는 단계이며 이 단계에서는 시각 장면을 구성하는 표면(surfaces) 정보, 깊이 정보, 레이어, 불연속 점과 같은 것들을 종합합니다.

그리고 결국에 그 모든 것을 한데 모아서 surface and volumetric primives의 형태의 계층적으로 조직화된 최종적인 3D 모델을 만들어 냅니다.

그리고 이런 방식은 "비전이 무엇인가"라는 것에 대한 아주 "이상적인" 사고과정이었으며 이런 방식의 사고방식은 실제로 수십 년간 컴퓨터 비전 분야를 지배했고 학생들이 컴퓨터 비전을 처음 입문하고 나서 "어떻게 시각정보를 분석할 수 있을까"라는 질문을 던졌을 때 직관적인 생각해 볼 수 있는 방법이었습니다.

<img width="1129" alt="스크린샷 2022-07-03 오후 1 44 43" src="https://user-images.githubusercontent.com/77891754/177061526-4ff2efb9-8e29-4b00-997d-c77afc3276b3.png">

70년대에는 또 다른 아주 중요한 연구들이 있었습니다.

"우리는 어떻게 해야 장난감 같은 단순한 블록 세계를 뛰어넘어서 실제 세계를 인식하고 표현할 수 있을까?"라는 질문을 하기 시작했습니다.

Stanford와 SRI에서 과학자들이 서로 비슷한 아이디어를 제안했습니다. 하나는 "generalized cylinder"이고 하나는 "pictorial structure"입니다.

기본 개념은 "모든 객체는 단순한 기하학적 형태로 표현할 수 있다"라는 것입니다. 가령 사람은 원통 모양을 조합해서 만들 수 있습니다. 또는 "주요 부위"와 "관절"로 표현할 수도 있을 것입니다.

두 방법 모두 단순한 모양과 기하학적인 구성을 이용해서 복잡한 객체를 단순화시키는 방법이며, 이러한 연구들은 수년간 다른 연구에 상당히 많은 영향을 미쳤습니다.

<img width="1130" alt="스크린샷 2022-07-03 오후 1 44 51" src="https://user-images.githubusercontent.com/77891754/177061527-4e086a70-1859-4d32-b64a-d997f9f53b88.png">

80년대 또 다른 사례로, David Lowe는 어떻게 하면 단순한 구조로
실제 세계를 재구성/인식할 수 있을지 고민했습니다.

David Lowe는 이 연구에서 동전을 인식하기 위해서 동전을 선(lines)과 경계(edges) 그리고 직선(straight lines) 그리고 이들의 조합을 이용해서 구성했습니다.

60/70/80년대에는 컴퓨터 비전으로 어떤 일을 할 수 있을까 고민한 시대였지만 너무 어려운 문제였고, 지금까지는 단순한 수준(toy example)에 불과했습니다.

즉 현실 세계에서 잘 동작할지를 생각해보면 많이 진보하지 못했다고 할 수 있습니다. 그래서 컴퓨터 비전 연구자들은 고민하다가 한가지 질문을 떠올리게 됩니다. 객체인식이 너무 어렵다면 우선 객체 분할(segmentation)이 우선이 아니었을까 라고 생각했습니다.

<img width="1131" alt="스크린샷 2022-07-03 오후 1 45 01" src="https://user-images.githubusercontent.com/77891754/177061528-70423654-9df0-48bb-8429-a97bcb0faaa1.png">

객체분할은 이미지의 각 픽셀을 의미 있는 방향으로 군집화하는 방법으로 픽셀을 모아놔도 사람을 정확히 인식할 수 없을지도 모르지만 적어도 배경인 픽셀과 사람이 속해 있을지도 모르는 픽셀을 가려낼 수는 있었습니다.

이를 "영상분할(Image Segmentation)"이라고 합니다.

<img width="1135" alt="스크린샷 2022-07-03 오후 1 45 22" src="https://user-images.githubusercontent.com/77891754/177061529-9b6878f5-2c4e-49c1-b528-0bb1fd9d4ee7.png">

그리고 컴퓨터 비전에서 유난히 발전 속도가 빨랐던 분야가 있었는데, 바로 "얼굴인식" 입니다.

대략 1999/2000년대에는 "기계학습", 특히나 "통계적 기계학습" 이라는 방법이 점차 탄력을 얻기 시작했는데, 가령 "Support Vector Machine", "Boosting", "Graphical models" 그리고 초기 "Neural Network" 등이 있었습니다.

그중 가장 큰 기여를 한 연구는 바로 Paul Viola와 Michael Jones이 AdaBoost를 이용해 실시간 얼굴인식에 성공한 것으로 이 연구는 당시 아주 대단한 성과였습니다.

연구 당시는 2001년이었고 컴퓨터는 여전히 엄청 느렸지만 그들의 얼굴인식 알고리즘은 실시간과 가깝게(near-real-time) 인식할 수 있었고 논문발표 5년이 지난 2006년에 Fujifilm은 실시간 얼굴인식을 지원하는 최초의 디지털카메라를 선보였습니다.

이는 기초 과학 연구의 성과를 실제 응용 제품으로 가장 빠르게 전달한 사례라고 할 수 있습니다.

<img width="1132" alt="스크린샷 2022-07-03 오후 1 45 31" src="https://user-images.githubusercontent.com/77891754/177061531-c732d094-f9fb-4141-87af-1cd4864814d5.png">

90년대 후반부터 2010년도까지의 시대를 풍미했던 알고리즘은 "특징기반 객체인식 알고리즘" 이었습니다. 아주 유명한 알고리즘이 바로 David Lowe의 SIFT feature입니다.

이 정지 표지판들을 서로 매칭하기는 상당히 어렵습니다. 카메라 앵글이 변할 수 있고, 겹치거나 화각이 변하고 빛도 변하고 객체 자체도 얼마든지 변할 수 있습니다.

하지만 그들은 객체의 특징 중 일부는 다양한 변화에 조금 더 강인하고 불변하다는 점을 발견했고 그리하여 객체인식은 객체에서 이와 같은 중요한 특징들을 찾아내고 그 특징들을 다른 객체에 매칭시켰습니다.

이미지 전체를 매칭하는 일보다 훨씬 쉬운 일로, 정지표지판 이미지에서 일부 SIFT 특징들을 추출하고 또 다른 정지 표지판에서도 특징을 추출하여 이를 식별하고 매칭합니다.

<img width="1133" alt="스크린샷 2022-07-03 오후 1 45 43" src="https://user-images.githubusercontent.com/77891754/177061532-df1633e2-0aa3-46ea-a9fe-acfc24c0e3d2.png">

이미지에 존재하는 "특징"을 사용하게 되면서 컴퓨터 비전은 또 한 번의 도약을 할 수 있었고, 장면 전체를 인식하기에 이르렀습니다.

한 예로, Spatial Pyramid Matching이 있습니다. 기본 아이디어는 우리가 특징들을 잘 뽑아낼 수만 있다면 그 특징들이 일종의 "단서"를 제공해 줄 수 있다는 것입니다.

해당 연구는 이미지 내의 여러 부분과 여러 해상도에서 추출한 특징을 하나의 특징 기술자로 표현하고 Support Vector Algorithm을 적용합니다.

<img width="1132" alt="스크린샷 2022-07-03 오후 1 45 58" src="https://user-images.githubusercontent.com/77891754/177061533-f8db32cf-7648-4cd2-8050-6e623f00f2e3.png">

위와같은 방식의 연구들은 사람 인식에도 탄력을 주었습니다. 즉 여러 특징들을 잘 조합해 보자는 시도들로, 사람인식과 관련된 연구들을 어떻게 해야 사람의 몸을 현실적으로 모델링할 수 있을지에 관련된 연구였습니다.

그중 하나는 "Histogram Of Gradients" 이고, 또 한가지는 "Deformable Part Models" 입니다.

<img width="1131" alt="스크린샷 2022-07-03 오후 1 46 06" src="https://user-images.githubusercontent.com/77891754/177061534-a637a865-0004-4592-a1bc-3a7779ca9c53.png">

60/70/80년대를 거치고 21세기를 맞이하고 있었고 하나의 변곡점을 마주하게 됩니다.

사진의 품질이 점점 좋아졌습니다. 인터넷과 디지털카메라의 발전은 더더욱 좋은 실험 데이터를 만들어 낼 수 있었습니다. 2000년대 초에 일궈낸 것 중 하나는 바로 컴퓨터 비전이 앞으로 풀어야 할 문제가 무엇인지의 정의를 어느 정도 내렸다는 것입니다.

물론 해결해야 할 다양한 문제가 있겠지만, 이 또한 아주 중요한 문제였습니다. 바로 "객체인식" 입니다.

객체인식 기술의 어디쯤 왔는지 측정해 보기 위해서 2000년대 초 Benchmark Dataset를 모으기 시작했고, 그 중 하나는 PASCAL Visual Object Challenge(VOC)입니다. 이 데이터셋에는 20개의 클래스가 있고 보이는 것들과 같이 기차, 비행기, 사람이 있고 소, 병, 고양이등도 있습니다. 데이터셋은 클래스당 수천 수만 개의 이미지들이 있었으며, 다양한 연구 집단에서 이를 통해 알고리즘의 자신들의 알고리즘을 테스트했고 얼마나 진보했는지를 지켜보았습니다.

2007년부터 2012년도까지의 표를 보면 객체인식 성능은 꾸준히 증가했습니다.

<img width="1134" alt="스크린샷 2022-07-03 오후 1 46 21" src="https://user-images.githubusercontent.com/77891754/177061535-aead4182-8c3c-4517-8abc-6bde84ebb2e1.png">

그 무렵, 대부분의 기계학습 알고리즘, raphical Model, SVM, AdaBoost 같은 기계학습 알고리즘들이 트레이닝 과정에서 Overfit을 하는 것 같았습니다. 이 문제의 원인 중 하나는 시각 데이터가 너무 복잡하다는 것입니다. 또 하나는 학습 데이가 부족해서 Overfiting이 훨씬 더 빠르게 발생했고 일반화 능력이 떨어졌습니다.

두 가지 motivation이 있었고, 하나는 이 세상의 모든 것들을 인식하고 싶다는 것이며, 또 하나는 기계학습의 Overfiting 문제를 극복해보자는 이 동기를 바탕으로 ImageNet 프로젝트가 시작되었습니다.

그 결과 ImageNet은 대략 15만 장에 달하는 이미지와 22만 가지의 클래스 카테고리를 보유하게 되었습니다. 당시 AI 분야에서 만든 가장 큰 데이터셋 이었으며 ImageNet 덕분에 객체인식은 다른 국면으로 접어들었습니다.

<img width="1131" alt="스크린샷 2022-07-03 오후 1 46 34" src="https://user-images.githubusercontent.com/77891754/177061536-b2f0ae37-cf03-4ee1-932c-60c65aa0f959.png">

하지만 ImageNet을 Benchmark에 어떻게 활용하는지가 큰 화두였고, 그래서 ImageNet 팀은 2009년부터 국제 규모의 대회를 주최했습니다. ILSVRC입니다.

이 대회를 위해서 1000개의 객체에서 140만 개의 test set 이미지를 엄선했으며, 해당 대회의 목적은 이미지 분류 문제를 푸는 알고리즘들을 테스트하기 위함이었습니다.

<img width="1132" alt="스크린샷 2022-07-03 오후 1 46 43" src="https://user-images.githubusercontent.com/77891754/177061539-5199dab0-110f-45ad-a8d2-4b0534296881.png">

Image Classification Challenge의 2010년도부터 2015년도까지의 결과입니다.

비록 컴퓨터 비전이 아직 객체인식의 모든 문제를 풀지는 못했지만, 진전이 있었다는 것은 사실입니다. 하지만 실생활에 적용하기에는 턱없이 부족했던 낮은 오류율이 인간의 수준으로 오기까지는 불과 몇 년뿐이 걸리지 않았습니다.

그리고 이 그래프에서절대로 놓쳐서 안 되는 특별한 순간은 바로 2012년입니다. 처음 2년 동안은 오류율이 약 25%를 맴돌았지만, 2012년에는 오류율이 16%로 거의 10%가량 떨어졌고 2012년도의 감소는 아주 중요합니다.

2012년도에 우승한 알고리즘은 convolutional neural network 모델로 CNN은 그 당시 다른 알고리즘들을 능가하고 ImageNet Challenge에서 우승하였습니다. CNN, Deep learning 모델은 컴퓨터 비전 분야의 진보를 이뤄냄으로써 CNN의 우수성을 입증하였습니다. 


## CS231n overview

### CS231n focuses on one of the most important problem of visual recognition - image classification

<img width="1124" alt="스크린샷 2022-07-03 오후 7 39 10" src="https://user-images.githubusercontent.com/77891754/177061540-971721d8-fd89-4c3e-b8ac-1f79c4fb1c9b.png">

Image Classification의 문제 정의를 해보자면 알고리즘이 이미지 한 장을 보고 몇 개의 고정된 카테고리 안에서 정답 하나를 고르는 것입니다.

이 방법이 다소 한정적이거나 인위적으로 보일 수도 있지만 사실 매우 일반적인데, 이 문제는 다양한 환경(industry, academia)에 적용될 수 있습니다. 가령 음식, 음식의 칼로리, 미술작품들 등을 인식해야 하는 다양한 제품에 적용할 수 있습니다.

따라서 image classification이라는 간단한 도구가 자체로도 유용할뿐더러 다양한 응용이 될 수도 있습니다.

### There is a number of visual recognition problems that are related to image classification, such as object detection, image captioning

<img width="1132" alt="스크린샷 2022-07-04 오전 8 47 22" src="https://user-images.githubusercontent.com/77891754/177061542-17f5ee18-2736-4790-9e9b-5d988d33952e.png">

위 문제들 모두 image classification 기반하에 일궈진 것들입니다.

하지만 object detection 문제는 classification과 조금 다릅니다. 이 이미지가 고양이다, 개다, 말이다 이렇게 하는 실제로 어디에 있는지 네모박스를 그릴 수 있어야 하며 네모박스를 객체의 위치에 정확히 그려 넣어야 합니다.

image captioning 은 이미지가 입력으로 주어지면 이미지를 묘사하는 적절한 문장을 생성해야 합니다. 해당 문제가 어렵고 복잡해 보이고 Image classification 과도 별로 관련이 없어 보일 수 있지만 image classification 기술을 이런 문제들에서 충분히 재사용할 수 있습니다.

### Convolutional Neural Networks(CNN) have become an important tool for object recognition

<img width="1131" alt="스크린샷 2022-07-04 오전 8 47 36" src="https://user-images.githubusercontent.com/77891754/177061543-abfc50ca-c64d-4b93-8a74-64df8a2a9136.png">

2011년에서 Lin et al의 알고리즘은 보시면 여전히 계층적(hierarchical)이며 여러 단계가 있습니다. 특징들을 뽑고, 지역 불변 특징들을 계산하고, pooling을 거치고 여러 단계를 거쳐서 최종적인 특징 기술자를 Linear SVM를 적용합니다. 핵심은 여전히 "계층적" 이라는 점입니다. edges를 뽑고 "불변 특징" 의 개념도 들어있습니다.

하지만 2012년 가장 획기적인 순간이었습니다. 당시 Toronto에서 Jeff Hinton 교수님의 연구실의 PHD였던 Alex Krizhevsky와 Ilya Sutskever는 7-Layer Convolutional neural network을 만들었습니다. AlexNet은 LSVRC'12 에서 아주 좋은 성과를 달성했습니다.

이후 ImageNet의 우승 트로피는 매년 Neural Network의 몫 이었고, 이러한 추세로 CNN은 매년 더 깊어져 갔습니다.

2014년에 네트워크가 훨씬 더 깊어졌습니다. Google의 GoogleNet 그리고 Oxford의 VGG가 바로 그 주인공이죠.

2015년에는 정말 대박입니다. MSRA의 Residual Network의 Layer 수는 152개에 육박합니다.

### Convolutional Neural Networks(CNN) were not invented overnight

<img width="1130" alt="스크린샷 2022-07-04 오전 8 47 50" src="https://user-images.githubusercontent.com/77891754/177061546-ad1a3523-fa61-4c8f-b5ae-31d93f36e24c.png">

하지만 한 가지 명심하셔야 할 점은 CNN이 2012년 ImageNet Challenge에서 빛을 본 것은 사실이지만 CNN이 2012년에 발명된 것은 아닙니다. 사실 CNN은 아주 오래전부터 존재했습니다.

1998년에 Jan LeCun과 Bell Labs와의 공동 과제로 숫자인식을 위해 CNN을 구축했습니다. 이들은 자필 수표 자동 판독과 우편주소 자동인식에 CNN을 적용하고 싶었습니다. 그들은 이미지를 입력으로 받아서 숫자와 문자를 인식할 수 있는 CNN을 만들었습니다.

CNN의 구조만 보자면 2012년의 AlexNet과 유사합니다.

그림처럼, raw pixel을 입력으로 받아 여러 Convolution Layer Layer를 거치고 Sub-Sampling, Fully Connected Layer를 거치게 됩니다.

그렇다면 왜 최근에야 갑자기 유명해진 것일까?

90년대 이래로 아주 큰 혁신들이 있었는데, 하나는 바로 계산능력이고, 또 하나는 오늘날의 PASCAL이나 ImageNet 같은 규모가 크고 잘 분류된 레이블들을 가진 데이터셋 때문입니다.

중요한 것은 CNN이 엄청 좋아 보이고 새로워 보이고 몇 해 전에 갑자기 툭 하고 튀어나온 것처럼 보이지만 그렇지 않다는 것입니다. CNN스러운 알고리즘들은 이미 아주 오래전부터 있었습니다.

### The quest for visual intelligence goes far beyond object recognition

그리고 또 한 가지 중요한 점은 컴퓨터 비전 연구의 목적은 "사람처럼 볼 수 있는" 기계를 만드는 것입니다.

사람들은 시각 체계를 통해 아주 많은 것들을 할 수 있으며 인간의 시각체계는 컴퓨터 비전보다 훨씬 더 강력합니다.

<img width="1128" alt="스크린샷 2022-07-04 오전 8 48 02" src="https://user-images.githubusercontent.com/77891754/177061547-f353c2f5-3abb-47fd-b063-efa22a82120a.png">

아직 풀지 못한 문제들의 예를 한번 살펴보겠습니다.

Semantic Segmentation 즉 Perceptual Grouping 같은 문제들로 이미지 전체를 레이블링하는 것 대신 모든 픽셀 하나하나를 이해하는 것입니다.

3D understanding은 실세계를 재구성하는 문제입니다.

<img width="1130" alt="스크린샷 2022-07-04 오전 8 48 13" src="https://user-images.githubusercontent.com/77891754/177061548-27f73d83-3838-48b4-b812-20d7b58ffc30.png">

위 프로젝트에서는 실제 세상에서 복잡한 것들을 일부 포착해 내려고 시도하고 있습니다. 이미지에 박스만 치는 게 아니라 커다란 의미론적 그래프로 표현하는 것으로 이 그래프는 객체를 식별하는 것을 넘어 그 장면에서의 객체 간의 관계, 객체의 성격, 행동 등을 나타낼 수 있습니다. 그리고 이런 방식을 이용한다면 실제 세상을 일부는 포착할 수 있지 않을까 예상합니다.

Image Classification만으로는 포착해 낼 수 없는 훨씬 더 다양한 일들이 존재합니다.

<img width="1131" alt="스크린샷 2022-07-04 오전 8 48 23" src="https://user-images.githubusercontent.com/77891754/177061550-e59d85fe-2a62-4513-932b-4e09eb9c5957.png">

또다른 연구로 임의의 사람들을 붙잡아서 위 사진을 아주 잠시 동안만 보여줬습니다. 사람들에게 아주 짧은 시간 동안만 이미지를 보여준 것입니다. 그런데 사람들은 이미지를 아주 잠깐만 봤음에도 이 같은 아주 긴 문장을 작성할 수 있었습니다.

주목할만한 결과였습니다. 인간은 이미지를 짧은 시간만 보더라도 이렇게 묘사할 수 있었습니다. 사람들이 이미지를 조금만 더 오래 볼 수만 있었다면 어땠을지 상상이 가실 것입니다. 이들이 누구이고 왜 저곳에서 게임을 하는지에 대해서 소설을 한 편 쓸 수 있을지 모르겠습니다. 외부 지식과 경험이 가미된다면 아마 끝도 없을 것입니다.

이미지의 내용을 아주 풍부하고 깊게 이해하는 것은 컴퓨터 비전 분야가 진정으로 추구하는 방향입니다.

<img width="1134" alt="스크린샷 2022-07-04 오전 8 48 43" src="https://user-images.githubusercontent.com/77891754/177074682-fbde0d6c-996a-4deb-ad7b-76258c59b276.png">

컴퓨터 비전이 정말 재미있는 분야이며 매우 유용하고, 아주 다양한 방법으로 이 세상에 기여할 수 있습니다. 또한 컴퓨터 비전은 의학 진단, 자율주행, 로보틱스 등 어디든 적용할 수 있습니다.

그리고 인간의 지능을 이해하기 위한 여러 핵심 아이디어들을 집대성하는 일종의 실마리가 될지도 모릅니다. 컴퓨터 비전은 정말 기상천외하고 재밌는 분야입니다.

