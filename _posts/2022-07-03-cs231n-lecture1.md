---
layout: post
title: CS231n Lecture1 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html)을 바탕으로 작성되었습니다.



# Wekcime to CS231n

<img width="1132" alt="스크린샷 2022-07-04 오전 8 51 08" src="https://user-images.githubusercontent.com/77891754/177061555-a596bacc-8554-4cd3-baad-f979120d2ae5.png">

전 세계에서 매일 무수한 센서로부터 엄청나게 많은 시각 데이터가 쏟아져 나오고 있습니다.

CISCO에서 수행한 2015 ~ 2017년도까지의 한 통계자료에 따르면 인터넷 트래픽 중 80%의 지분은 바로 비디오 데이터입니다. 심지어 이 결과는 사진 같은 다른 데이터들을 모두 제외하고 비디오만 추산한 결과인데, 이 통계는 인터넷의 데이터 대부분이 시각 데이터라는 사실을 보여줍니다.

그러므로 시각 데이터들을 잘 활용할 수 있는 알고리즘을 잘 개발하는 것이 무엇보다 중요해졌습니다.

하지만 문제가 있는데, 이런 시각데이터는 해석하기 상당히 까다롭다는 점으로 사실상 이들을 이해하고 해석하는 일은 상당히 어렵습니다. 따라서 시각데이터로 서비스를 하려면 자동으로 시각데이터를 이해하고 분석하는 알고리즘을 개발하는 것이 관건입니다.

<img width="1135" alt="스크린샷 2022-07-03 오후 1 40 54" src="https://user-images.githubusercontent.com/77891754/177061508-dbcf55f7-97c6-480d-be67-6f7dd3ab3c6f.png">

컴퓨터 비전이라는 분야 주변에는 상당히 많은 분야가 존재하기 때문에 다양한 과학, 공학 분야들과 맞닥뜨리게 됩니다.

# Today's agenda
## A brief history of computer vision

비전(시각)과 컴퓨터 비전이 언제 어디에서 비롯됐고 현재는 어디쯤 왔는지를 살펴보겠습니다.

<img width="1134" alt="스크린샷 2022-07-03 오후 1 43 12" src="https://user-images.githubusercontent.com/77891754/177061515-50bd53ea-5004-4b55-b4df-f1611ea4eddf.png">

비전의 역사는 5억 4천만년전 시작되었는데, 그 시대의 지구 대부분은 물이었고 대부분 바다를 부유하는 일부 생물들만 존재했으며 눈(eyes)은 존재하지 않았습니다.

하지만 5억 4천만 년 전에 아주 놀라운 사건이 벌어졌는데, 동물학자들은 화석을 연구하면서 천만 년이라는 아주 짧은 시기 동안에 생물의 종이 폭발적으로 늘어났다는 것을 발견했습니다.

그 이유로 수많은 가설들이 있지만, 앤드류 파커(Andrew Parker)는 약 5억 4천만 년 전 최초의 눈(eyes)이 생겨났다는 것을 발견했습니다.

즉, 폭발적인 종 분화의 시기를 촉발시킨 것이며 생물들은 갑자기 볼 수 있게 되어서 능동적이게 되었으며, 일부 포식자들은 먹이를 찾아다니고 먹이들은 포식자로부터 달아나야만 했습니다.

그래서 비전의 도래는 진화적 군비경쟁을 촉발시켰고 생물들은 하나의 종으로
살아남으려면 빠르게 진화해야만 했습니다. 이것이 바로 비전의 태동입니다. 

우리 인간은 대뇌 피질의 50%가량의 뉴런이 시각처리에 관여하는데, Vision은 가장 큰 감각체계이며 우리가 생존하고, 일하고, 움직이고, 어떤 것들을 다루고, 의사소통하고, 오락을 즐기는 등 많은 것들을 가능하게 해줍니다.

비전은 동물들에게 중요하며 특히 지능을 가진 동물들에게 정말 중요합니다.


<img width="1134" alt="스크린샷 2022-07-03 오후 1 43 38" src="https://user-images.githubusercontent.com/77891754/177061520-b6aa6386-1382-4b42-b76e-a0ed5f11e875.png">

생물학자들은 비전의 매커니즘을 연구하기 시작했는데, 인간과 동물의 비전의 연구에 가장 영향력 있었을 뿐만 아니라 컴퓨터 비전에도 영감을 준 한 연구가 있었습니다. 1950/60년대 전기생리학을 이용한 Hubel과 Wiesel의 연구입니다.

그들이 묻고 싶었던 질문은 바로 "포유류의 시각적 처리 메커니즘은 무엇일까?" 였습니다. 그래서 그들은 고양이의 뇌를 연구하기로 합니다.

일차 시각 피질에는 다양한 종류의 세포가 있다는 것을 알았습니다. 그중 가장 중요한 세포가 있었는데 그 세포들은 아주 단순했습니다. 경계(edges)가 움직이면 이에 반응하는 세포들이었습니다.

물론 더 복잡한 세포들도 있긴 하지만, 주된 발견은 시각 처리가 처음에는 단순한 구조로 시작되며 그 정보가 통로를 거치면서 실제 세상을 제대로 인지할 수 있을 때까지 점점 복잡해진다는 것입니다.

<img width="1134" alt="스크린샷 2022-07-03 오후 1 43 51" src="https://user-images.githubusercontent.com/77891754/177061521-11c7333a-02e0-46df-bed7-015b39be8780.png">

컴퓨터 비전의 역사는 60년대 초반에 태동합니다.
Larry Roberts의 Block World 연구에서는 우리 눈에 보이는 사물들을 기하학적 모양으로 단순화시켰습니다.

이 연구의 목표는 우리 눈에 보이는 세상을 인식하고 그 모양을 재구성하는 일이었습니다.

<img width="1131" alt="스크린샷 2022-07-03 오후 1 44 18" src="https://user-images.githubusercontent.com/77891754/177061524-688f32d1-b82b-48bb-a2d7-1eed0a0ee95a.png">

또한 David Marr은 MIT의 비전 과학자였으며 그는 70년대 후기에 아주 유명한 책을 한 권 저술합니다.

이 책은 그가 비전을 무엇이라 생각하는지, 그리고 어떤 방향으로 컴퓨터 비전이 나아가야 하는지, 그리고 컴퓨터가 비전을 인식하게 하기 위해 어떤 방향으로 알고리즘을 개발해야 하는지를 다룬 책이었습니다.

<img width="1132" alt="스크린샷 2022-07-03 오후 1 44 30" src="https://user-images.githubusercontent.com/77891754/177061525-108be85d-818d-42b5-ac39-f043fa1953bc.png">

그의 저서에서, 우리가 눈으로 받아들인 "이미지"를 "최종적인 full 3D 표현"으로 만들려면 몇 단계의 과정을 거쳐야만 한다고 주장했습니다.

첫 단계는, 그가 부르길 "Primal Sketch"라고 하는 단계입니다. 이 과정은 주로 경계(edges), 막대(bars), 끝(ends), 가상의 선(virtual lines), 커브(curves), 경계(boundaries)가 표현되는 과정입니다.

이후의 다음 단계는, 그가 부르기로는 "2.5-D sketch"라는 단계이며 이 단계에서는 시각 장면을 구성하는 표면(surfaces) 정보, 깊이 정보, 레이어, 불연속 점과 같은 것들을 종합합니다.

그리고 결국에 그 모든 것을 한데 모아서 surface and volumetric primives의 형태의 계층적으로 조직화된 최종적인 3D 모델을 만들어 냅니다.

그리고 이런 방식은 "비전이 무엇인가"라는 것에 대한 아주 "이상적인" 사고과정이었습니다. 그리고 이런 방식의 사고방식은 실제로 수십 년간 컴퓨터 비전 분야를 지배했으며 학생들이 컴퓨터 비전을 처음 입문하고 나서 "어떻게 시각정보를 분석할 수 있을까"라는 질문을 던졌을 때 직관적인 생각해 볼 수 있는 방법이었습니다.

<img width="1129" alt="스크린샷 2022-07-03 오후 1 44 43" src="https://user-images.githubusercontent.com/77891754/177061526-4ff2efb9-8e29-4b00-997d-c77afc3276b3.png">

70년대에는 또 다른 아주 중요한 연구들이 있었습니다.

"우리는 어떻게 해야 장난감 같은 단순한 블록 세계를 뛰어넘어서 실제 세계를 인식하고 표현할 수 있을까?"라는 질문을 하기 시작했습니다.

70년대를 생각해보면 그 당시에는 사용할 수 있는 데이터가 거의 없었습니다.

컴퓨터도 정말 느렸고 심지어 PC가 보급되기도 전이죠. 이 상황에서 컴퓨터 과학자들은 어떻게 해야 대상을 인식하고 표현할 수 있을지를 고민하기 시작했습니다.

Stanford와 SRI에서 과학자들이 서로 비슷한 아이디어를 제안했습니다.

하나는 "generalized cylinder"이고 하나는 "pictorial structure"입니다.

기본 개념은 "모든 객체는 단순한 기하학적 형태로 표현할 수 있다"라는 것입니다.

가령 사람은 원통 모양을 조합해서 만들 수 있습니다. (왼쪽 그림)
또는 "주요 부위"와 "관절"로 표현할 수도 있을 것입니다. (오른쪽 그림)

두 방법 모두 단순한 모양과 기하학적인 구성을 이용해서 복잡한 객체를 단순화시키는 방법입니다.

이러한 연구들은 수년간 다른 연구에 상당히 많은 영향을 미쳤습니다.

<img width="1130" alt="스크린샷 2022-07-03 오후 1 44 51" src="https://user-images.githubusercontent.com/77891754/177061527-4e086a70-1859-4d32-b64a-d997f9f53b88.png">
80년대 또 다른 사례로, David Lowe는 어떻게 하면 단순한 구조로
실제 세계를 재구성/인식할 수 있을지 고민했습니다.

David Lowe는 이 연구에서 면도기를 인식하기 위해서 면도기를

선(lines)과 경계(edges) 그리고 직선(straight lines)
그리고 이들의 조합을 이용해서 구성했습니다.

60/70/80년대에는 컴퓨터 비전으로 어떤 일을 할 수 있을까
고민한 시대였습니다. 하지만 너무 어려운 문제였습니다.

지금까지 제가 보여 드린 연구들이 모두 아주 대담했고
큰 야망을 가진 시도였지만

그들은 단순한 수준(toy example)에 불과했습니다.

현실 세계에서 잘 동작할지를 생각해보면
많이 진보하지 못했다고 할 수 있죠. 그래서 컴퓨터 비전 연구자들은

우리가 도대체 무슨 실수를 하고 있을까
고민하다가 한가지 질문을 떠올리게 됩니다.

객체인식이 너무 어렵다면 우선 객체 분할(segmentation)
이 우선이 아니었을까 라고 말이죠
<img width="1131" alt="스크린샷 2022-07-03 오후 1 45 01" src="https://user-images.githubusercontent.com/77891754/177061528-70423654-9df0-48bb-8429-a97bcb0faaa1.png">
객체분할은 이미지의 각 픽셀을 의미 있는
방향으로 군집화하는 방법입니다.

픽셀을 모아놔도 사람을 정확히
인식할 수 없을지도 모르지만

적어도 배경인 픽셀과 사람이 속해 있을지도
모르는 픽셀을 가려낼 수는 있었습니다.

이를 "영상분할(Image Segmentation)"이라고 합니다.
이 문제를 다룬 아주 중요한 연구가 있었는데

Berkeley 대학의 Jitendra Malik 교수와
그의 제자인 Jianbo Shi의 연구였습니다.

이 연구는 영상분할 문제를 해결하기 위해서
그래프 이론을 도입했습니다.
<img width="1135" alt="스크린샷 2022-07-03 오후 1 45 22" src="https://user-images.githubusercontent.com/77891754/177061529-9b6878f5-2c4e-49c1-b528-0bb1fd9d4ee7.png">
그리고 컴퓨터 비전에서 유난히
발전 속도가 빨랐던 분야가 있었습니다.

바로 "얼굴인식" 입니다. 인간에게 가장
중요한 부위 중 하나가 바로 얼굴이죠.

어쩌면 얼굴이 가장 중요할 수도 있겠군요.

대략 1999/2000년대에는 "기계학습", 특히나 "통계적 기계학습"
이라는 방법이 점차 탄력을 얻기 시작했습니다.

가령 "Support Vector Machine", "Boosting", "Graphical models"
그리고 초기 "Neural Network" 등이 있었습니다.

그중 가장 큰 기여를 한 연구는 바로

Paul Viola와 Michael Jones이 AdaBoost를
이용해 실시간 얼굴인식에 성공한 것입니다.

이 연구는 당시 아주 대단한 성과였습니다.
연구 당시는 2001년이었고

컴퓨터는 여전히 엄청 느렸습니다.
하지만 그들의 얼굴인식 알고리즘은

실시간과 가깝게(near-real-time) 인식할 수
있었고 논문발표 5년이 지난 -

2006년에 Fujifilm은 실시간 얼굴인식을 지원하는
최초의 디지털카메라를 선보였습니다.

이는 기초 과학 연구의 성과를 실제 응용 제품으로
가장 빠르게 전달한 사례라고 할 수 있습니다.
<img width="1132" alt="스크린샷 2022-07-03 오후 1 45 31" src="https://user-images.githubusercontent.com/77891754/177061531-c732d094-f9fb-4141-87af-1cd4864814d5.png">
자! 이제 다시 "어떻게 객체를 잘 인식할 것인가?"
라는 질문으로 다시 한번 돌아가 봅시다.

90년대 후반부터 2010년도까지의 시대를 풍미했던 알고리즘은
"특징기반 객체인식 알고리즘" 이었습니다. 이 시절 나온 -

아주 유명한 알고리즘이 바로 David Lowe의 SIFT
feature입니다. 그의 아이디어는 전체 객체를 -

가령, 여기 정지 표지판이 있습니다.
이 정지 표지판들을 서로 매칭하기는 상당히 어렵죠

카메라 앵글이 변할 수 있고, 겹치거나 화각이 변하고 빛도
변하고 객체 자체도 얼마든지 변할 수 있습니다.

하지만 그들은 객체의 특징 중 일부는 다양한 변화에 조금 더 강인하고
불변하다는 점을 발견했습니다.

그리하여 객체인식은 객체에서 이와 같은
중요한 특징들을 찾아내고

그 특징들을 다른 객체에 매칭시키는 과제가 되었습니다.
이미지 전체를 매칭하는 일보다 훨씬 쉬운 일이었죠.

이 그림은 그 논문에서 가져온 것입니다. 정지표지판
이미지에서 일부 SIFT 특징들을 추출하고

또 다른 정지 표지판에서도 특징을 추출하여
이를 식별하고 매칭합니다.
<img width="1133" alt="스크린샷 2022-07-03 오후 1 45 43" src="https://user-images.githubusercontent.com/77891754/177061532-df1633e2-0aa3-46ea-a9fe-acfc24c0e3d2.png">
이미지에 존재하는 "특징"을 사용하게 되면서

컴퓨터 비전은 또 한 번의 도약을 할 수 있었습니다.
그리고 장면 전체를 인식하기에 이르렀습니다.

한 예로, Spatial Pyramid Matching이 있습니다.
기본 아이디어는 우리가 특징들을 잘 뽑아낼 수만 있다면

그 특징들이 일종의 "단서"를 제공해 줄 수 있다는 것이었죠
이미지가 풍경인지, 부엌인지, 또는 고속도로인지 하는 것을 말이죠.

이 연구는 이미지 내의 여러 부분과 여러 해상도에서
추출한 특징을 하나의 특징 기술자로 표현하고

Support Vector Algorithm을 적용합니다.

이런 방식의 연구들은 사람 인식에도 탄력을 주었습니다.
<img width="1132" alt="스크린샷 2022-07-03 오후 1 45 58" src="https://user-images.githubusercontent.com/77891754/177061533-f8db32cf-7648-4cd2-8050-6e623f00f2e3.png">
여러 특징들을 잘 조합해 보자는 시도들이었죠
사람인식에 관련된 연구들도 아주 많았습니다.

사람인식과 관련된 연구들은 어떻게 해야 사람의 몸을
현실적으로 모델링할 수 있을지에 관련된 연구였습니다.

그중 하나는 "Histogram Of Gradients"
입니다. , 또 한가지는 -

"Deformable Part Models" 입니다.
그 당시에는 60/70/80년대를 거치고

21세기를 맞이하고 있었고
하나의 변곡점을 마주하게 됩니다.

사진의 품질이 점점 좋아졌습니다. 인터넷과 디지털카메라의 발전은
더더욱 좋은 실험 데이터를 만들어 낼 수 있었습니다.
<img width="1131" alt="스크린샷 2022-07-03 오후 1 46 06" src="https://user-images.githubusercontent.com/77891754/177061534-a637a865-0004-4592-a1bc-3a7779ca9c53.png">
2000년대 초에 일궈낸 것 중 하나는 바로 컴퓨터 비전이 앞으로
풀어야 할 문제가 무엇인지의 정의를 어느 정도 내렸다는 것입니다.

물론 해결해야 할 다양한 문제가 있겠지만, 이 또한
아주 중요한 문제였습니다. 바로 "객체인식" 입니다.

제가 여태 말씀드린 것들이 객체인식이지만 2000년대 초 -

우리는 Benchmark Dataset를 모으기 시작했습니다.
객체인식 기술의 어디쯤 왔는지 측정해 보기 위해서였죠

그 중 하나는 PASCAL Visual Object Challenge(VOC)
입니다. 이 데이터셋에는 20개의 클래스가 있고

여기 보이는 것들과 같이 기차, 비행기, 사람이 있고 소, 병, 고양이
등도 있는 것으로 기억합니다. 데이터셋은

클래스당 수천 수만 개의 이미지들이 있었으며,
다양한 연구 집단에서

이를 통해 알고리즘의 자신들의 알고리즘을 테스트했고

얼마나 진보했는지를 지켜보았습니다.
여기 2007년부터 2012년도까지의 표가 있습니다.

객체인식 성능은 꾸준히 증가했습니다. 많은 진보가 이루어졌죠
<img width="1134" alt="스크린샷 2022-07-03 오후 1 46 21" src="https://user-images.githubusercontent.com/77891754/177061535-aead4182-8c3c-4517-8abc-6bde84ebb2e1.png">
그 무렵, Princeton과 Stanford에 있던 그룹에서 더 어려운
질문을 던졌습니다. 우리는 이 세상의 모든 객체들을

인식할 준비가 되었는가? 였습니다.
이 질문은 한 사실로부터 비롯되었습니다.

거의 대부분의 기계학습 알고리즘에 해당하는 사실이었습니다.

Graphical Model, SVM, AdaBoost 같은 기계학습
알고리즘들이 트레이닝 과정에서 Overfit을 하는 것 같았습니다.

이 문제의 원인 중 하나는 시각 데이터가 너무 복잡하다는 것입니다.

모델의 입력은 복잡한 고차원 데이터였고, 이로 인해
모델을 fit하려면 더 많은 파라미터가 필요했죠

학습 데이터가 부족하면 Overfiting이 훨씬 더 빠르게 발생했고
일반화 능력이 떨어졌습니다.

우리에게는 두 가지 motivation이 있었습니다. 하나는
이 세상의 모든 것들을 인식하고 싶다는 것이고

또 하나는 기계학습의 Overfiting 문제를 극복해보자는 것이었죠

이 동기를 바탕으로 ImageNet 프로젝트를 시작했습니다.
구할 수 있는 모든 이미지를 담은 가장 큰 데이터셋을 만들고 싶었습니다.

이 데이터셋으로 모델을 학습시킬 수 있고 Benchmark도
할 수 있도록 말이죠. 프로젝트는 약 3년 정도 걸렸습니다.

어려운 일도 많았습니다. 우선 인터넷에서 수십억 장의 이미지를
다운받았고 WordNet이라는 Dictionary로 정리했습니다.

WordNet에는 수천 가지의 객체 클래스가 있습니다.

그리고 Clever Crowd Engineering trick을 도입했습니다.
Amazon Mechanical Turk에서 사용하는

이미지의 정렬, 정제, 레이블 등을 제공하는 플랫폼입니다.

그 결과 ImageNet은 대략 15만 장에 달하는 이미지와 22만 가지의
클래스 카테고리를 보유하게 되었습니다.

아마도 당시 AI 분야에서 만든 가장 큰 데이터셋 이었습니다.
ImageNet 덕분에 객체인식은 다른 국면으로 접어들었습니다.
<img width="1131" alt="스크린샷 2022-07-03 오후 1 46 34" src="https://user-images.githubusercontent.com/77891754/177061536-b2f0ae37-cf03-4ee1-932c-60c65aa0f959.png">
하지만 ImageNet을 Benchmark에 어떻게
활용하는지가 큰 화두였습니다.

그래서 ImageNet 팀은 2009년부터 국제 규모의 대회를
주최했습니다. ILSVRC입니다. 이 대회를 위해서

1000개의 객체에서 140만 개의 test set 이미지를 엄선했습니다.

이 대회의 목적은 이미지 분류 문제를 푸는
알고리즘들을 테스트하기 위함이었습니다.
<img width="1132" alt="스크린샷 2022-07-03 오후 1 46 43" src="https://user-images.githubusercontent.com/77891754/177061539-5199dab0-110f-45ad-a8d2-4b0534296881.png">
여기 예제 이미지들이 있습니다. 참가자들은 정답 후보를 총 5가지
고를 수 있습니다. 5개 중에 정답이 있으면 맞춘 것이죠

Image Classification Challenge의 결과입니다.
2010년도부터 2015년도까지의 결과입니다.

x은 연도를, y축은 오류율을 입니다.

좋은 소식으로는 오류율이 점차 감소하고 있습니다.

2012년도의 오류율은 사람보다 낮습니다. 여기에서의 사람은

Stanford의 한 PhD 학생입니다. 이 대회에 참가한 알고리즘처럼
테스트를 수행하며 몇 주를 보내야만 했습니다.

비록 컴퓨터 비전이 아직 객체인식의 모든 문제를
풀지는 못했지만, 진전이 있었다는 것은 사실입니다.

하지만 실생활에 적용하기에는 턱없이 부족했던 낮은 오류율이
인간의 수준으로 오기까지는

불과 몇 년뿐이 걸리지 않았습니다. 그리고 여러분이 이 그래프에서
절대로 놓쳐서 안 되는 특별한 순간이 있습니다.

바로 2012년입니다. 처음 2년 동안은 오류율이 약 25%를 맴돌았습니다.
2012년에는 오류율이 16%로 거의 10%가량 떨어졌고

물론 현재의 오류율이 더 낮지만 2012년도의 감소는 아주 중요합니다.

2012년도에 우승한 알고리즘은
convolutional neural network 모델입니다.

CNN은 그 당시 다른 알고리즘들을 능가하고
ImageNet Challenge에서 우승하였습니다.

우리가 한 학기 동안 배울 내용이 바로
Convolutional neural network에 관한 것입니다.

CNN 모델이 무엇인지 심도 깊게 다룰 것입니다.
CNN을 Deep Learning이라고도 합니다.

Deep Learning이 더 유명한 이름이겠군요

앞으로 CNN이 무엇인지, 어떤 법칙이 있는지, 어떤 선례가 있는지,
이 모델의 최근 동향은 어떠한지를 살펴볼 것입니다. 하지만 이 역사의

시작은 바로 2012년입니다. CNN, Deep learning 모델은
컴퓨터 비전 분야의 진보를 이뤄냄으로써 CNN의 우수성을

입증하였습니다. 자연어 처리나 음성 인식과 같은 다른
관련 분야들도 더불어서 말이죠. 소개는 이쯤 해두고

CS231n 수업 소개를 위해서 나머지 시간은
Justin에게 맡기도록 하겠습니다.




































## CS231n overview
Fei-Fei 교수님 감사합니다.
제가 여기서 이어받겠습니다.

지금부터는 주제를 바꿔서 우리 수업과 관련된
이야기를 해볼까 합니다.


# CS231n focuses on one of the most important problem of visual recognition - image classification

<img width="1124" alt="스크린샷 2022-07-03 오후 7 39 10" src="https://user-images.githubusercontent.com/77891754/177061540-971721d8-fd89-4c3e-b8ac-1f79c4fb1c9b.png">
이 수업에서 중점적으로 다룰 문제는
Image Classification입니다.

앞서 ImageNet Challenge 이야기에서
살짝 들어보셨을 것입니다.

Image Classification의 문제 정의를 해보자면
알고리즘이 이미지 한 장을 봅니다.

몇 개의 고정된 카테고리 안에서 정답 하나를 고르는 거죠

이 방법이 다소 한정적이거나 인위적으로 보일 수도
있지만 사실 매우 일반적입니다.

이 문제는 다양한 환경에 적용될 수 있습니다. industry이던
academia이던 말이죠. 다양한 곳에서 적용 가능합니다.

가령 음식, 음식의 칼로리, 미술작품들 등을 인식해야
하는 다양한 제품에 적용할 수 있습니다.

따라서 image classification이라는 간단한 도구가
자체로도 유용할뿐더러 다양한 응용이 될 수도 있습니다.




# There is a number of visual recognition problems that are related to image classification, such as object detection, image captioning

<img width="1132" alt="스크린샷 2022-07-04 오전 8 47 22" src="https://user-images.githubusercontent.com/77891754/177061542-17f5ee18-2736-4790-9e9b-5d988d33952e.png">

수업에서는 다양한 문제들을 다룰 것입니다. 하지만 이 문제들 모두
image classification 기반하에 일궈진 것들입니다.

그리고 object detection과 image captioning도
배워 볼 것입니다.

object detection 문제는
classification과 조금 다릅니다.

이 이미지가 고양이다, 개다, 말이다 이렇게 하는 실제로
어디에 있는지 네모박스를 그릴 수 있어야 합니다.

네모박스를 객체의 위치에 정확히 그려 넣어야 합니다.

image captioning 도 배울 것입니다. 이미지가 입력으로 주어지면
이미지를 묘사하는 적절한 문장을 생성해야 합니다.

이 문제가 어렵고 복잡해 보이고 Image classification과도
별로 관련이 없어 보일 수 있지만

image classification 기술을 이런 문제들에서
충분히 재사용할 수 있습니다.

# Convolutional Neural Networks(CNN) have become an important tool for object recognition

지금까지는 ImageNet Challenge의 맥락에서 말씀드렸습니다만
최근 컴퓨터 비전 분야의 진보를 이끌어낸 주역은 바로

Convolutional neural networks, 즉
CNN입니다. 또는 convnet으로도 불리죠

<img width="1131" alt="스크린샷 2022-07-04 오전 8 47 36" src="https://user-images.githubusercontent.com/77891754/177061543-abfc50ca-c64d-4b93-8a74-64df8a2a9136.png">

지난 몇 해 간 ImageNet Challenge의 우승자들을 살펴봅시다.

2011년에서 Lin et al의 알고리즘은 보시면
여전히 계층적(hierarchical)이죠

여러 단계가 있습니다. 특징들을 뽑고, 지역 불변 특징들을 계산하고,
pooling을 거치고 이렇게 여러 단계를 거쳐서

최종적인 특징 기술자를 Linear SVM에 태웁니다.

핵심은 여전히 "계층적" 이라는 점입니다. edges를 뽑고
"불변 특징" 의 개념도 들어있습니다.

그리고 대부분 이러한 직관들은
CNN에도 영향을 미칩니다.

하지만 2012년 가장 획기적인 순간이었습니다.

당시 Toronto에서 Jeff Hinton 교수님의 연구실의
PHD였던 Alex Krizhevsky와 Ilya Sutskever는

7-Layer Convolutional neural network
을 만들었습니다.

AlexNet 또는 Supervision으로도 알려져 있습니다.
AlexNet은 LSVRC'12 에서 아주 좋은 성과를 달성했습니다.

이후 ImageNet의 우승 트로피는
매년 Neural Network의 몫이었습니다.

그리고 이러한 추세로 CNN은 매년 더 깊어져 갔습니다.

AlexNet은 7(8)-Layer Neural Network입니다.
Layer를 세는 방식에 따라 조금 다릅니다.

2015년에 네트워크가 훨씬 더 깊어졌습니다. Google의
GoogleNet 그리고 Oxford의 VGG가 바로 그 주인공이죠.

2015년에는 정말 대박입니다. MSRA의 Residual Network의
Layer 수는 152개에 육박합니다.

이후 Layer 200개까지 쌓으면 성능이 더 좋아진다고는 하지만
아마도 여러분의 GPU 메모리가 감당할 수 없을 것입니다.

나중에 더 다루기로 하죠

오늘 수업에서 알고 가셔야 할 점은 2012년의 CNN의 시대가 도래했고,
이후 CNN을 개선하고 튜닝하려는 많은 시도들이 있었다는 것입니다.

그리고 이 강의 전반에 걸쳐 CNN 모델들이 어떻게
동작하는지는 심도 깊게 살펴볼 것입니다.



# Convolutional Neural Networks(CNN) were not invented overnight
하지만 한 가지 명심하셔야 할 점은 CNN이 2012년
ImageNet Challenge에서 빛을 본 것은 사실이지만

CNN이 2012년에 발명된 것은 아닙니다.

사실 CNN은 아주 오래전부터 존재했습니다.
<img width="1130" alt="스크린샷 2022-07-04 오전 8 47 50" src="https://user-images.githubusercontent.com/77891754/177061546-ad1a3523-fa61-4c8f-b5ae-31d93f36e24c.png">
CNN의 기초연구라고 한다면 90년도의 Jan LeCun과
Bell Labs와의 공동 과제를 말씀드릴 수 있습니다.

1998년에 그들은 숫자인식을 위해 CNN을 구축했습니다.

이들은 자필 수표 자동 판독과
우편주소 자동인식에 CNN을 적용하고 싶었습니다.

그들은 이미지를 입력으로 받아서 숫자와 문자를
인식할 수 있는 CNN을 만들었습니다.

CNN의 구조만 보자면 2012년의 AlexNet과 유사합니다.

그림처럼, raw pixel을 입력으로 받아 여러
Convolution Layer Layer를 거치고 Sub-Sampling,

Fully Connected Layer를 거치게 됩니다.

이 모든 건 다음 강의 부터 더 자세히 다루도록 하겠습니다.

하지만, 여러분이 이 두 그림을 보고 있자면
둘이 상당히 비슷해 보일 겁니다.

2012년의 CNN 아키텍쳐들은 서로 비슷비슷했습니다.
90년대의 LeNet 아키텍처를 공유하기 때문입니다.

그럼 이런 질문을 할 수 있겠군요
90년대부터 알고리즘이 있었다면

왜 최근에야 갑자기 유명해진 것일까요?

90년대 이래로 아주 큰 혁신들이 있었습니다.

하나는 바로 계산능력입니다. 무어의 법칙 덕분에
컴퓨터의 계산속도가 매년 빨라졌습니다.

완벽한 척도는 아니지만, CPU의 트랜지스터 개수만 세어봐도
90년대보다 몇십 배 이상 발전했음을 알 수 있죠

또한 graphics processing units의 진보도 한몫했습니다.
GPU는 아주 강력한 병렬처리가 가능한데

계산 집약적인 CNN 모델을 고속으로
처리하는 데 안성맞춤입니다.

단지 더 많은 계산이 가능하다는 것만으로도
연구자들이 더 큰 아키텍쳐를 연구해 볼 수 있었고

경우 따라서는 기존의 고전 알고리즘들의 크기만 키웠음에도
훨씬 더 잘 동작하는 경우도 많았습니다.

연산량의 증가는 딥러닝의 역사에서 아주 중요한 요소입니다.

90년대와 지금은 데이터의 차이도 있었습니다.

CNN 알고리즘이 잘 동작하려면 아주 많은
레이블이 매겨진 이미지가 필요합니다.

90년대에는 레이블이 매겨진 이미지 데이터를 구하기가 쉽지 않았습니다.
아주 크고 다양한 데이터셋을 수집하기가 힘든 시기였습니다.

오늘날은 PASCAL이나 ImageNet 같은 규모가 크고
잘 분류된 레이블들을 가진 데이터셋이 많습니다.

90년대에와 비교하면 사용 가능한 데이터셋이 훨씬 많습니다.

큰 데이터셋들을 잘 활용하면
Higher Capacity Model을 만들 수 있습니다.

그렇게 학습시킨 모델들은 실생활 문제에서도 잘 동작했습니다.

하지만 가장 중요한 것은 CNN이 엄청 좋아 보이고 새로워 보이고
몇 해 전에 갑자기 툭 하고 튀어나온 것처럼 보이지만

그렇지 않다는 것입니다.

CNN스러운 알고리즘들은 이미 아주 오래전부터 있었습니다.


# The quest for visual intelligence goes far beyond object recognition
그리고 또 한 가지 중요한 점은 컴퓨터 비전 연구의 목적은
"사람처럼 볼 수 있는" 기계를 만드는 것입니다.

사람들은 시각 체계를 통해 아주 많은 것들을 할 수 있습니다.

여러분은 고양이나 강아지를 찾아서 사각형을
그리는 것 이상의 일들을 할 수 있습니다.

여러분의 시각체계는 컴퓨터 비전보다 훨씬 더 강력합니다.

컴퓨터 비전 분야 이야기를 해드리자면 아직도 우리가 풀어야 할
수많은 도전과제와 미해결 문제가 있습니다.

우리는 더 나은 일을 하고, 더 야심 찬 문제에 도전할 수 있도록
알고리즘을 계속해서 연구해야 합니다.
<img width="1128" alt="스크린샷 2022-07-04 오전 8 48 02" src="https://user-images.githubusercontent.com/77891754/177061547-f353c2f5-3abb-47fd-b063-efa22a82120a.png">
아직 풀지 못한 문제들의 예를 한번 살펴보겠습니다.
사실 예전부터 연구가 활발히 진행됐습니다.

Semantic Segmentation 즉
Perceptual Grouping 같은 문제들이죠

이미지 전체를 레이블링하는 것 대신
모든 픽셀 하나하나를 이해하는 것입니다.

Semantic Segmentation은 다음에 다시 다루도록 하죠

3D understanding은 실세계를 재구성하는 문제입니다.
제 생각에는 여전히 완벽하게 풀지는 못한 문제이죠

여러분도 엄청나게 많은 것들을 상상해 볼 수 있습니다.

행동 인식의 예를 들어볼까요

가령 어떤 사람이 비디오에서 무언가를 하고 있을 때, 그 행동을 인식할
수 있는 가장 좋은 방법은 무엇일까요? 상당히 도전적인 문제입니다.

그리고 증강현실, 가상현실, 또는 새로운 센서 등을 마주하게 되면

그 자체를 한 분야로 다뤄도 될 만큼 아주 새롭고 흥미롭고
도전적인 문제들을 만나게 될 것입니다.



<img width="1130" alt="스크린샷 2022-07-04 오전 8 48 13" src="https://user-images.githubusercontent.com/77891754/177061548-27f73d83-3838-48b4-b812-20d7b58ffc30.png">
지금부터 보여 드릴 것은 제가 연구실에서 진행 중인
프로젝트의 일부인데 Visual Genome이라는 데이터셋입니다.

이 프로젝트에서는 실제 세상에서 복잡한 것들을 일부
포착해 내려고 시도하고 있습니다.

이미지에 박스만 치는 게 아니라 커다란 의미론적 그래프로
표현하는 것이죠. 이 그래프는 객체를 식별하는 것을 넘어

그 장면에서의 객체 간의 관계, 객체의 성격, 행동 등을 나타낼 수 있습니다.

그리고 이런 방식을 이용한다면 실제 세상을
일부는 포착할 수 있지 않을까 예상합니다.

이런 것들은 우리가 단순하게 Classification만 할 때
활용하지 못했던 것들입니다.

이 프로젝트는 현시점으로서는 표준 접근방식은 아니지만

Image Classification만으로는 포착해 낼 수 없는 훨씬 더
다양한 일들이 있다는 사실을 알려 드리고 싶었습니다.





<img width="1131" alt="스크린샷 2022-07-04 오전 8 48 23" src="https://user-images.githubusercontent.com/77891754/177061550-e59d85fe-2a62-4513-932b-4e09eb9c5957.png">

그런 관점에서, 정말 재밌는 연구가 하나 있습니다.

Fei-Fei 교수님의 Cal Tech에서의 박사과정 시절의 연구입니다.

연구를 소개해 드리자면 임의의 사람들을 붙잡아서
이런 사진을 아주 잠시 동안만 보여줬습니다.

사람들에게 아주 짧은 시간 동안만 이미지를 보여준 것입니다.

그런데 사람들은 이미지를 아주 잠깐만 봤음에도 이 같은
아주 긴 문장을 작성할 수 있었습니다.

주목할만한 결과였습니다. 인간은 이미지를 짧은 시간만
보더라도 이렇게 묘사할 수 있었습니다.

"사람들이 어떤 놀이 또는 싸움을 하고 있고, 두 명씩 짝지어져 있고,
왼쪽 사람은 무언가를 던지고 있고 -

잔디밭인 것 같은 느낌이 드니까 밖인 것 같고.." 등등

사람들이 이미지를 조금만 더 오래 볼 수만 있었다면
어땠을지 상상이 가실 것입니다.

이들이 누구이고 왜 저곳에서 게임을 하는지에 대해서
소설을 한 편 쓸 수 있을지 모르겠습니다.

외부 지식과 경험이 가미된다면 아마 끝도 없을 것입니다.

이미지의 내용을 아주 풍부하고 깊게 이해하는 것은
이는 컴퓨터 비전 분야가 진정으로 추구하는 방향입니다.

제 생각에는 컴퓨터 비전이라는 분야에는 많은 진보가
있었지만, 아직 가야 할 길은 멀고도 험난합니다.


<img width="1131" alt="스크린샷 2022-07-04 오전 8 48 33" src="https://user-images.githubusercontent.com/77891754/177061553-af8f23d7-d46d-4657-a1b7-ed47cf99e894.png">
다른 예를 하나 더 들어보겠습니다. Andrej Karpathy의
블로그에서 가져온 이미지입니다. 아주 재미있는 이미지입니다.

많은 사람들의 웃음을 자아냈습니다. 제가 보기에도 상당히
재미있는 이미지입니다. 왜 이 이미지가 웃기죠?

한 남자가 체중계에 서 있습니다. 뭐 보통 사람들이
체중을 재려고 체중계를 사용하곤 합니다.

그런데 어떤 사람이 뒤에서 몰래 체중계를 밟고 있군요.
우리는 체중계가 어떻게 동작할지 짐작할 수 있습니다.

저 남자는 자신의 "부풀려진" 체중을 보게 될 것이란 것도 알 수 있습니다.

하지만 더 많은 정보가 있습니다. 우리는 저 사람이
평범한 사람이 아니란 것을 알고 있습니다.

저 사람은 당시의 미국 대통령 Barack Obama입니다. 우리는
미국의 대통령이라면 존경받는 정치인이어야 한다고 생각합니다.

[웃음]

적어도 동료에게 이런 식의 장난을 치지 말아야 하겠죠

그런데 뒤쪽의 사람들이 이 장면을 보고 웃고 있군요

이를 미루어 우리는 사람들이 이 장면을 어떻게
받아드리는지 이해할 수 있습니다.

그들도 우리와 같은 생각을 한다는 것을 알 수 있죠

이건 정말 놀라운 것입니다.
이 이미지 한 장에 정말 많은 것들이 있습니다.

컴퓨터 비전 알고리즘이 이런 진정한 깊은 이해를
하기까지는 아직 갈 길이 멀다고 생각합니다.

이 분야가 큰 진보를 이루긴 했지만 갈 길은 한참 남았습니다.
연구자로서 저에게는 아주 짜릿한 일입니다.

앞으로 더 진보할 수 있는 흥미진진하고 재미있는 문제들이
우리를 기다리고 있기 때문입니다.




<img width="1134" alt="스크린샷 2022-07-04 오전 8 48 43" src="https://user-images.githubusercontent.com/77891754/177074682-fbde0d6c-996a-4deb-ad7b-76258c59b276.png">

그래서 저는 컴퓨터 비전이 정말 재미있는 분야라는
것을 여러분이 아셨으면 좋겠습니다.

정말 재밌습니다. 그리고 매우 유용합니다.
아주 다양한 방법으로 이 세상에 기여할 수 있습니다.

컴퓨터 비전은 의학 진단, 자율주행, 로보틱스 등
어디든 적용할 수 있습니다.

그리고 인간의 지능을 이해하기 위한 여러 핵심 아이디어들을
집대성하는 일종의 실마리가 될지도 모릅니다.

제 생각에는 컴퓨터 비전은 정말 기상천외하고 재밌는 분야입니다.

저는 여러분과 이 수업을 빌어 그런 알고리즘들이 실제로 어떻게
동작하는지를 심도 깊게 다룰 수 있어서 정말 좋습니다.

여기까지는 컴퓨터 비전의 역사에 대한
저의 개인적인 견해였습니다.
