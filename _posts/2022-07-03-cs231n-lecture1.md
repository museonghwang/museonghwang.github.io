---
layout: post
title: CS231n Lecture1 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html) 강의와 2022년 슬라이드를 바탕으로 작성되었습니다.



# Welcome to CS231n

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182766813-b1676153-eb3c-4563-82ef-97ff91d4af43.png">
</p>

전 세계에서 매일 무수한 센서로부터 엄청나게 많은 시각 데이터가 쏟아져 나오고 있습니다.

CISCO에서 수행한 2015 ~ 2017년도까지의 한 통계자료에 따르면 인터넷 트래픽 중 80%의 지분은 바로 비디오 데이터입니다. 심지어 이 결과는 사진 같은 다른 데이터들을 모두 제외하고 비디오만 추산한 결과인데, 이 통계는 인터넷의 데이터 대부분이 시각 데이터라는 사실을 보여줍니다.

그러므로 시각 데이터들을 잘 활용할 수 있는 알고리즘을 잘 개발하는 것이 무엇보다 중요해졌습니다.

하지만 문제가 있는데, 이런 시각데이터는 해석하기 상당히 까다롭다는 점으로 사실상 이들을 이해하고 해석하는 일은 상당히 어렵습니다. 따라서 시각데이터로 서비스를 하려면 자동으로 시각데이터를 이해하고 분석하는 알고리즘을 개발하는 것이 관건입니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182767243-8717d90a-b458-4d54-8d04-a232b9d0e15c.png">
</p>

* 컴퓨터 비전은 학제적(interdisciplinary)인 분야로, 굉장히 다양한 분야와 맞닿아 있습니다.
    * 물리학 - 광학, 이미지 구성, 이미지의 물리적 형성
    * 생물학, 심리학 - 동물의 뇌가 어떤 방식으로 시각 정보를 보고 처리하는지를 이해
    * 컴퓨터 과학, 수학, 공학 - 컴퓨터 비전 알고리즘을 구현할 컴퓨터 시스템을 구축할 때 필요

<br>





# A brief history of computer Vision

비전(시각)과 컴퓨터 비전이 언제 어디에서 비롯됐고 현재는 어디쯤 왔는지를 살펴보겠습니다.

<br>



## Big Bang

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182766981-00d0b213-11c6-429e-ab85-fa8d0730bbdf.png">
</p>

* Vision의 역사는 5억 4천만년전 시작되었는데, 그 시대의 지구 대부분은 물이었고 대부분 바다를 부유하는 일부 생물들만 존재했으며 눈(eyes)은 존재하지 않았습니다.
* 하지만 5억 4천만 년 전에 천만 년이라는 짧은 시간동안 생물의 종이 폭발적으로 증가한 시기가 있었습니다.
    * 가장 설득력 있는 가설은 Biological Vision의 탄생(by. Andrew Parker)
* 즉, 폭발적인 종 분화의 시기를 촉발시킨 것이며 생물들은 갑자기 볼 수 있게 되어서 능동적이게 되었으며, 일부 포식자들은 먹이를 찾아다니고 먹이들은 포식자로부터 달아나야만 했습니다.
    * 그래서 Vision의 도래로 생물들은 하나의 종으로 살아남으려면 빠르게 진화해야만 했습니다.
    * 이것이 바로 Vision의 태동입니다. 

우리 인간은 대뇌 피질의 50%가량의 뉴런이 시각처리에 관여하는데, Vision은 가장 큰 감각체계이며 우리가 생존하고, 일하고, 움직이고, 어떤 것들을 다루고, 의사소통하고, 오락을 즐기는 등 많은 것들을 가능하게 해줍니다.

<br>



## Hubel and Wiesel, 1959

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182767414-e4d0e297-8244-4e6e-90b5-94613e109817.png">
</p>

생물학자들은 Vision의 매커니즘을 연구하기 시작했는데, 인간과 동물의 Vision의 연구에 가장 영향력 있었을 뿐만 아니라 Computer Vision에도 영감을 준 한 연구가 있었습니다. 1950/60년대 전기생리학을 이용한 Hubel과 Wiesel의 연구입니다.
* 그들이 묻고 싶었던 질문은 바로 "포유류의 시각적 처리 메커니즘은 무엇일까?" 였습니다. 그래서 그들은 고양이의 뇌를 연구하기로 합니다.
* 일차 시각 피질에는 다양한 종류의 세포가 있다는 것을 알았습니다. 그중 가장 중요한 세포가 있었는데 그 세포들은 아주 단순했습니다. 경계(edges)가 움직이면 이에 반응하는 세포들이었습니다.
* 물론 더 복잡한 세포들도 있긴 하지만, 주된 발견은 시각 처리가 처음에는 단순한 구조로 시작되며, 그 정보가 통로를 거치면서 실제 세상을 제대로 인지할 수 있을 때까지 점점 복잡해진다는 것입니다.

<br>



## Larry Roberts, 1963

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182768705-ce98f908-7757-4395-93e8-aa8e56fc1425.png">
</p>

Computer Vision의 역사는 60년대 초반에 태동합니다.
* Larry Roberts의 Block World 연구에서는 우리 눈에 보이는 사물들을 기하학적 모양으로 단순화시켰습니다.
* 이 연구의 목표는 우리 눈에 보이는 세상을 인식하고 그 모양을 재구성하는 일이었습니다.

<br>



## Stages of Visual Representation, David Marr, 1970s

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182767881-c7940fe0-c246-43f4-84fa-1d8e3bee9e9b.png">
</p>

70년대 후기에 David Marr는 아주 유명한 책을 한 권 저술합니다.

이 책은 David Marr이 Vision을 무엇이라 생각하는지, 그리고 어떤 방향으로 Computer Vision이 나아가야 하는지, 그리고 컴퓨터가 Vision을 인식하게 하기 위해 어떤 방향으로 알고리즘을 개발해야 하는지를 다룬 책이었습니다.
* 그의 저서에서, 우리가 눈으로 받아들인 "이미지"를 "최종적인 full 3D 표현"으로 만들려면 몇 단계의 과정을 거쳐야만 한다고 주장했습니다.
    * 첫 단계는, "Primal Sketch" 단계입니다. 이 과정은 주로 경계(edges), 막대(bars), 끝(ends), 가상의 선(virtual lines), 커브(curves), 경계(boundaries)가 표현되는 과정입니다.
    * 이후의 다음 단계는, "2.5-D sketch" 라는 단계이며 이 단계에서는 시각 장면을 구성하는 표면(surfaces) 정보, 깊이 정보, 레이어, 불연속 점과 같은 것들을 종합합니다.
    * 그리고 결국에 그 모든 것을 한데 모아서 surface and volumetric primives의 형태의 계층적으로 조직화된 최종적인 3D 모델을 만들어 냅니다.

이런 방식은 "Vision이 무엇인가"라는 것에 대한 아주 이상적인 사고과정이었으며, 이런 방식의 사고방식은 실제로 수십 년간 Computer Vision 분야를 지배했고, "어떻게 시각정보를 분석할 수 있을까"라는 질문에 직관적인 생각해 볼 수 있는 방법이었습니다.

<br>



##  Recognition via Parts (1970s)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182768815-6063a3c5-9b5d-4722-b82f-134aaec9a1ed.png">
</p>

70년대에 또 다른 연구로 "어떻게 해야 장난감 같은 단순한 블록 세계를 뛰어넘어서 실제 세계를 인식하고 표현할 수 있을까?"라는 질문을 하기 시작했습니다.
* Stanford와 SRI에서 과학자들은 "generalized cylinder"와 "pictorial structure"를 제안했습니다.
* 기본 개념은 "모든 객체는 단순한 기하학적 형태로 표현할 수 있다"라는 것입니다. 가령 사람은 원통 모양을 조합해서 만들 수 있습니다. 또는 "주요 부위"와 "관절"로 표현할 수도 있을 것입니다.

두 방법 모두 단순한 모양과 기하학적인 구성을 이용해서 복잡한 객체를 단순화시키는 방법이며, 이러한 연구들은 수년간 다른 연구에 상당히 많은 영향을 미쳤습니다.

<br>



## Recognition via Edge Detection (1980s)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182768905-fdf35abf-de77-42d4-95b2-05089df0f4ce.png">
</p>

80년대 또 다른 사례로, John Canny와 David Lowe는 어떻게 하면 단순한 구조로
실제 세계를 재구성/인식할 수 있을지 고민했습니다.
* 면도기을 인식하기 위해서 면도기를 선(lines)과 경계(edges) 그리고 직선(straight lines) 그리고 이들의 조합을 이용해서 구성했습니다.
* 즉 John Canny는 1986년 이미지에서 edge를 찾고 edge matching을 통하여 object recognition을 수행하는 방식을 제시했습니다.

<br>



## Recognition via Grouping (1990s)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182771185-e349aa73-c818-4235-8b39-6f91c24c263c.png">
</p>

1990년대에 들어 사람들은 더 복잡한 이미지를 이용하여 연구를 수행하기 시작했는데, 1997년에는 image segmentation을 이미지에 적용해보는 등의 일을 했습니다.

<br>



## Recognition via Matching (2000s)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182771489-45f7b321-0784-411f-b6ac-5dfd32059417.png">
</p>

* 90년대 후반부터 2010년도까지의 시대를 풍미했던 알고리즘은 "특징기반 객체인식 알고리즘" 이었습니다. 아주 유명한 알고리즘이 바로 David Lowe의 SIFT feature입니다.
    * 이 정지 표지판들을 서로 매칭하기는 상당히 어렵습니다. 하지만 객체의 특징 중 일부는 다양한 변화에 조금 더 강인하고 불변하다는 점을 발견했고, 그리하여 객체인식은 객체에서 이와 같은 중요한 특징들을 찾아내고 그 특징들을 다른 객체에 매칭시켰습니다.
    * object의 key point를 저장해서 object의 각도가 바뀐다던가 노이즈가 있는 등의 상태가 바뀌어도 올바르게 object를 detection 할 수 있는 연구가 진행되었습니다.
    * 이미지 전체를 매칭하는 일보다 훨씬 쉬운 일로, 정지표지판 이미지에서 일부 SIFT 특징들을 추출하고 또 다른 정지 표지판에서도 특징을 추출하여 이를 식별하고 매칭합니다.

<br>



## Face Detection (2001)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182772118-91e167b2-339d-417c-a422-5bcab91d0f4e.png">
</p>

그리고 컴퓨터 비전에서 유난히 발전 속도가 빨랐던 분야가 있었는데, 바로 "얼굴인식" 입니다. Paul Viola와 Michael Jones가 실시간 얼굴인식에 성공한 것으로 이 연구는 당시 아주 대단한 성과였습니다.
* Boosted Decision Tree를 사용한 얼굴인식 알고리즘은 실시간과 가깝게(near-real-time) 인식할 수 있었고, 2006년에 Fujifilm은 실시간 얼굴인식을 지원하는 최초의 디지털카메라를 선보였습니다.
* 이는 기초 과학 연구의 성과를 실제 응용 제품으로 가장 빠르게 전달한 사례라고 할 수 있습니다.

<br>



## PASCAL VOC (2006)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/177061534-a637a865-0004-4592-a1bc-3a7779ca9c53.png">
</p>

60/70/80년대를 거치고 하나의 변곡점을 마주하게 됩니다.

사진의 품질이 점점 좋아졌으며, 인터넷과 디지털카메라의 발전은 더더욱 좋은 실험 데이터를 만들어 낼 수 있었습니다. 2000년대 초에 일궈낸 것 중 하나는 바로 컴퓨터 비전이 앞으로 풀어야 할 문제가 무엇인지의 정의를 어느 정도 내렸다는 것입니다.

물론 해결해야 할 다양한 문제가 있겠지만, 이 또한 아주 중요한 문제였습니다. 바로 "Object Recognition" 입니다.

* 객체인식 기술의 어디쯤 왔는지 측정해 보기 위해 Benchmark Dataset를 모으기 시작했고, 그 중 하나는 PASCAL Visual Object Challenge(VOC)입니다.
    * 이 데이터셋에는 20개의 클래스가 있습니다.
    * 있고 보이는 것들과 같이 기차, 비행기, 사람이 있고 소, 병, 고양이등도 있습니다.
    * 2007년부터 2012년도까지의 표를 보면 객체인식 성능은 꾸준히 증가했습니다.

<br>

## ImageNet (2009)


<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/177061535-aead4182-8c3c-4517-8abc-6bde84ebb2e1.png">
</p>

* 그 무렵, 대부분의 기계학습 알고리즘, raphical Model, SVM, AdaBoost 같은 기계학습 알고리즘들이 트레이닝 과정에서 Overfitting 문제가 발생했습니다.
    * 이 문제의 원인 중 하나는 시각 데이터가 너무 복잡하다는 것입니다.
    * 또  학습 데이가 부족해서 Overfiting이 훨씬 더 빠르게 발생했고 일반화 능력이 떨어졌습니다.
* 두 가지 motivation이 있었고, 하나는 이 세상의 모든 것들을 인식하고 싶다는 것이며, 또 하나는 기계학습의 Overfiting 문제를 극복해보자는 이 동기를 바탕으로 ImageNet 프로젝트가 시작되었습니다.
* 그 결과 ImageNet은 대략 15만 장에 달하는 이미지와 22만 가지의 클래스 카테고리를 보유하게 되었습니다. 당시 AI 분야에서 만든 가장 큰 데이터셋 이었으며 ImageNet 덕분에 객체인식은 다른 국면으로 접어들었습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182773870-072489f4-83f2-4688-9d8f-965d34e777a3.png">
</p>

* ImageNet을 Benchmark에 어떻게 활용하는지가 큰 화두였고, 2009년부터 국제 규모의 대회를 주최했습니다. ILSVRC입니다.
* 이 대회를 위해서 1000개의 객체에서 140만 개의 test set 이미지를 엄선했으며, 해당 대회의 목적은 이미지 분류 문제를 푸는 알고리즘들을 테스트하기 위함이었습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182774187-197ad5a1-7a89-4f88-b065-c18e52cc17d2.png">
</p>

Image Classification Challenge의 2010년도부터 2017년도까지의 결과입니다.

위 그래프가 그 대회의 우승자의 정확도를 나타낸 것인데 2012년 이전에는 미미하게 개선 되다가, 2012년에는 오류율이 16%로 거의 10%가량 떨어졌고 2012년도의 감소는 아주 중요합니다.

2012년도에 우승한 알고리즘은 convolutional neural network 모델로 CNN은 그 당시 다른 알고리즘들을 능가하고 ImageNet Challenge에서 우승하였습니다. CNN, Deep learning 모델은 컴퓨터 비전 분야의 진보를 이뤄냄으로써 CNN의 우수성을 입증하였습니다. 





# CS231n Overview



## CS231n focuses on one of the most important problem of visual recognition - image classification

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/177061540-971721d8-fd89-4c3e-b8ac-1f79c4fb1c9b.png">
</p>

* Image Classification는 이미지 한 장을 보고 몇 개의 고정된 카테고리 안에서 정답 하나를 고르는 것입니다.
    * 이 문제는 다양한 환경(industry, academia)에 적용될 수 있습니다. 가령 음식, 음식의 칼로리, 미술작품들 등을 인식해야 하는 다양한 제품에 적용할 수 있습니다.
* 따라서 image classification이라는 간단한 도구가 자체로도 유용할뿐더러 다양한 응용이 될 수도 있습니다.

<br>



## There is a number of visual recognition problems that are related to image classification, such as object detection, image captioning

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/177061542-17f5ee18-2736-4790-9e9b-5d988d33952e.png">
</p>

* 위 문제들 모두 image classification 기반하에 일궈진 것들입니다.
* 하지만 object detection 문제는 classification과 조금 다릅니다. 이 이미지가 고양이다, 개다, 말이다 이렇게 하는 실제로 어디에 있는지 네모박스를 그릴 수 있어야 하며 네모박스를 객체의 위치에 정확히 그려 넣어야 합니다.
* image captioning 은 이미지가 입력으로 주어지면 이미지를 묘사하는 적절한 문장을 생성해야 합니다. 해당 문제가 어렵고 복잡해 보이고 Image classification 과도 별로 관련이 없어 보일 수 있지만 image classification 기술을 이런 문제들에서 충분히 재사용할 수 있습니다.

<br>



## Convolutional Neural Networks(CNN) have become an important tool for object recognition

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/177061543-abfc50ca-c64d-4b93-8a74-64df8a2a9136.png">
</p>

* 2011년에서 Lin et al의 알고리즘은 보시면 여전히 계층적(hierarchical)이며 여러 단계가 있습니다. 핵심은 여전히 "계층적" 이라는 점입니다. edges를 뽑고 "불변 특징" 의 개념도 들어있습니다.
* 하지만 2012년 AlexNet은 ILSVRC12 에서 아주 좋은 성과를 달성했습니다.
    * 이후 ImageNet의 우승 트로피는 매년 Neural Network의 몫 이었고, 이러한 추세로 CNN은 매년 더 깊어져 갔습니다.
    * 2014년에 네트워크가 훨씬 더 깊어졌습니다. Google의 GoogleNet 그리고 Oxford의 VGG가 바로 그 주인공이죠.
    * 2015년에는 MSRA의 Residual Network의 Layer 수는 152개에 육박합니다.

<br>



## Convolutional Neural Networks(CNN) were not invented overnight

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/177061546-ad1a3523-fa61-4c8f-b5ae-31d93f36e24c.png">
</p>

* 하지만 CNN이 2012년 ImageNet Challenge에서 빛을 본 것은 사실이지만 CNN이 2012년에 발명된 것은 아닙니다. 사실 CNN은 아주 오래전부터 존재했습니다.
* 1998년에 Jan LeCun과 Bell Labs와의 공동 과제로 숫자인식을 위해 CNN을 구축했습니다. 이들은 자필 수표 자동 판독과 우편주소 자동인식에 CNN을 적용하고 싶었습니다. 그들은 이미지를 입력으로 받아서 숫자와 문자를 인식할 수 있는 CNN을 만들었습니다.
    * CNN의 구조만 보자면 2012년의 AlexNet과 유사합니다.
    * 그림처럼, raw pixel을 입력으로 받아 여러 Convolution Layer Layer를 거치고 Sub-Sampling, Fully Connected Layer를 거치게 됩니다.

<br>





# The quest for visual intelligence goes far beyond object recognition

Computer Vision 연구의 목적은 "사람처럼 볼 수 있는" 기계를 만드는 것입니다. 사람들은 시각 체계를 통해 아주 많은 것들을 할 수 있으며 인간의 시각체계는 Computer Vision보다 훨씬 더 강력합니다.

이미지의 내용을 아주 풍부하고 깊게 이해하는 것은 Computer Vision 분야가 진정으로 추구하는 방향입니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/177074682-fbde0d6c-996a-4deb-ad7b-76258c59b276.png">
</p>

Computer Vision이 정말 재미있는 분야이며 매우 유용하고, 아주 다양한 방법으로 이 세상에 기여할 수 있습니다. 또한 Computer Vision은 의학 진단, 자율주행, 로보틱스 등 어디든 적용할 수 있습니다.

그리고 인간의 지능을 이해하기 위한 여러 핵심 아이디어들을 집대성하는 일종의 실마리가 될지도 모릅니다. Computer Vision은 정말 기상천외하고 재밌는 분야입니다.




