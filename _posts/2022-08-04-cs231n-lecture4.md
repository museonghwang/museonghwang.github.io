---
layout: post
title: CS231n Lecture4 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html)을 바탕으로 작성되었습니다.




<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183276119-ffad2f06-3106-466b-ba22-f76019b532ef.png">
</p>

<br>





# Lecture3 Recap

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183276154-b3d9989b-66a9-4b49-9406-d2f8f2930892.png">
</p>

함수 $f$ 를 통해 어떻게 classifier를 정의하는지 다뤘습니다.
* weights W를 파라미터로 가집니다.
* 함수 f의 입력(input)은 데이터 $x$,
* 함수 f의 출력(output)은 분류하고자 하는 클래스들에 대한 score vector

또한 loss function을 정의했습니다.
* classifier 얼마나 badness한지 점수로 나타내기위해 Full loss 항(term)을 정의.
* Full loss = data loss + regularization
* regularization은 얼마나 우리의 모델이 단순한지, 더 나은 일반화를 위해 적합한 모델을 가지고 있는지를 표현.

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183276393-4666aff6-9347-4d6e-a876-3c3080cf522a.png">
</p>

또한 최적의 loss를 갖게 하는 파라미터 $W$ 를 찾고 싶어서, loss function의 W에 관한 gradient를 찾았습니다
* Gradient Descent를 통해 optimization을 하여 최적의 $W$ 를 찾습니다.
* gradient가 음수인 방향, 즉 경사가 하강하는 방향을 반복적으로 구해 아래로 나아가 가장 낮은 loss에 찾습니다.

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183276572-3357ae36-6c7a-4ddb-8f7e-f8cd3232eaf3.png">
</p>

또한 gradient를 구하는 다른 방법에 대해서도 이야기했습니다.
* 수학적으로 유한 차분 근사(finite differnce approximation)를 이용해 계산가능.
    * 느리고 또한 근사치이지만 써내려가면서 하기에 가장 쉬운 방법
* analytic gradient를 어떻게 사용하지 그리고 계산하는 것까지 이야기했습니다.
    * analytic gradient를 사용하는 것은 빠르고 정확합니다.

<br>





# Better Idea: Computational graphs + Backpropagation

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183276698-51303f5c-1f0f-42a8-a6d2-5582e192c5f5.png">
</p>

임의의 복잡한 함수를 통해 어떻게 analytic gradient를 계산하는방법
* computational graph framework 사용합니다.
* computational graph를 이용하면 어떤 함수든지 표현할 수 있습니다.


그래프의 각 노드는 연산 단계를 나타냅니다. input이 $x$, $W$ 인 선형 classifier인 경우
* 처음 노드는 곱셈 노드입니다. 파라미터 $W$(weight)와 입력 데이터($x$)의 행렬 곱셈을 의미하고, 그 결과로 $s$(score) vector가 출력됩니다.
* 그 바로 다음 단계의 노드는 전단계에서 계산된 $s$ (score)을 이용해 loss 즉, $L_i$ 를 계산 하는 노드입니다. 이때 계산에 이용되는 손실함수는 hinge loss로 계산이 되어지는 것을 확인 할 수 있습니다.
* 또한 overfitting을 막기 위해 Regularization term을 더해주는 노드를 확인 할 수 있습니다.
* 그리고 최종 loss $L$ 은 regularization term과 data term의 합입니다.


computational graph를 사용해 함수를 표현하게 됨으로써, backpropagation을 사용할 수 있게 됩니다. backpropagation은 gradient를 얻기위해 computational graph 내부의 모든 변수에 대해 chain rule을 재귀적으로 사용합니다.

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183277298-2c075cd2-5c2e-45b7-8792-b2c1f8aef6cd.png">
</p>

computational graph는 매우 복잡한 함수를 이용하여 작업할때 아주 유용하게 사용됩니다. 예를들어 CNN은 가장 윗층(top)에
입력 이미지가 들어가고 아래(bottom)에는 loss가 있습니다. 입력 이미지는 loss function으로 가기까지 많은 layer를 거쳐 변형을 겪게 됩니다.

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183277414-cbeb0dbf-5b5f-40cd-954b-bf7ce716fb78.png">
</p>

심지어 이렇게 복잡한 neural networks도 computational graph를 통해 손쉽게 계산이 가능합니다.

<br>





# Backpropagation using computational graph

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183277459-a4f31244-13e8-48d1-b540-a634cbdc6dab.png">
</p>

backpropagation이 어떻게 동작하는지 간단한 예제를 살펴보겠습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183277606-687e11f2-e96f-420c-970f-2ef01c626cf7.png">
</p>



## Forward Pass

$f(x,\ y,\ z)\ =\ (x\ +\ y)z$ 라는 함수가 있고 덧셈노드, 곱셈노드로 구성된 computational graph가 표현되어 있습니다. 그리고 $x=-2$, $y=5$, $z=-4$ 의 입력값을 넣어 각 연산 과정에서 계산된 값들을 적어두었습니다.

목표는 function $f$ 의 출력에 대한 어떤 변수의 gradient를 찾기 원합니다. 그렇기에 처음에는 항상 함수 $f$ 를 이용해서 computational graph로 나타내는 것입니다. 그리고 이 네트워크에 우리가 가지고 있는 값을 전달합니다.

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183277609-1b7ce737-b2a1-4c5e-934f-97a6d04e6580.png">
</p>

* 우리가 찾기 원하는것은 $x$, $y$, $z$ 각각에 대한 $f$ 의 gradient입니다.
* 다음과 같이 정의한다면, $f(x,\ y,\ z)\ =\ (x\ +\ y)z\ =\ qz$ 라고 표현 가능합니다.
    * $x\ +\ y$ 덧셈 노드를 $q$ 라고 정의
    * $q\ *\ z$ 곱셈 노드를 $f$ 라고 정의
* 위 두 식에 각 변수에대해 편미분을 실행하면 다음과 같습니다.
    * $x$ 변화량에 따른 $q$ 변화량, $\frac{∂q}{∂x}\ =\ 1$
    * $y$ 변화량에 따른 $q$ 변화량, $\frac{∂q}{∂y}\ =\ 1$
    * $q$ 변화량에 따른 $f$ 변화량, $\frac{∂f}{∂q}\ =\ z$
    * $z$ 변화량에 따른 $f$ 변화량, $\frac{∂f}{∂z}\ =\ q$

<br>



## Backward Pass

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183278047-8bf0dfee-0868-4830-8b1e-32810498e766.png">
</p>

* backpropagation은 chain rule의 재귀적인 응용입니다.
    * chiain rule에 의해 computational graph의 가장 끝, 뒤에서부터 gradient를 계산합니다.
    * 우선 제일 뒷부분, 즉 출력 $f$ 에 대한 gradient를 계산하길 원합니다.
    * 제일 뒷부분의 gradient는 $\frac{∂f}{∂f}\ =\ 1$ 입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183278075-98094829-e98d-4bc5-8679-34c3e3676b63.png">
</p>

* 다음으로, 뒤로 이동해서 $z$ 에 대한 $f$ 의 gradient를 계산합니다.
    * $z$ 에 대한 $f$ 의 gradient, 즉 $\frac{∂f}{∂z}$ 를 계산합니다.
    * 해당 값은 이미 Forward Pass 과정에서 이미 $\frac{∂f}{∂z}\ =\ q$ 라고 계산해두었습니다.
    * 따라서 $z$ 에 대한 $f$ 의 미분값은 $\frac{∂f}{∂z}\ =\ q\ =\ x\ +\ y\ =\ −2\ +\ 5\ =\ 3$ 가 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183278078-79547ae5-9387-4967-890c-28dfb886572d.png">
</p>

* 다음으로, 뒤로 이동해서 $q$ 에 대한 $f$ 의 gradient를 계산합니다.
    * $q$ 에 대한 $f$ 의 gradient, 즉 $\frac{∂f}{∂q}$ 를 계산합니다.
    * 해당 값은 이미 Forward Pass 과정에서 이미 $\frac{∂f}{∂q}\ =\ z$ 라고 계산해두었습니다.
    * 따라서 $z$ 에 대한 $f$ 의 미분값은 $\frac{∂f}{∂q}\ =\ z\ =\ -4$ 가 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183278084-e4811a8c-d121-415c-9271-8b8c637646bc.png">
</p>

* 이제, 계산그래프 뒤로 이동해서 $y$ 에 대한 $f$ 의 gradient를 계산합니다.
    * $y$ 에 대한 $f$ 의 gradient, 즉 $\frac{∂f}{∂y}$ 를 계산합니다.
    * 하지만 여기의 $y$ 는 $f$ 와 바로 연결되어 있지 않습니다.
    * 이때 이 값을 구할 때 chain rule 을 사용합니다.
    * 즉, 해당 값을 $\frac{∂f}{∂y}\ =\ \frac{∂f}{∂q} \frac{∂q}{∂y}$ 로 나타낼 수 있으며, 이 것은 Forward Pass 에서 계산한 $\frac{∂f}{∂q}\ =\ z$, $\frac{∂q}{∂y}\ =\ 1$ 를 이용해 계산할 수 있습니다.
    * 따라서 $y$ 에 대한 $f$ 의 미분값은 $\frac{∂f}{∂y}\ =\ \frac{∂f}{∂q} \frac{∂q}{∂y}\ =\ z\ ∗\ 1\ =\ −4$ 가 됩니다.
* 직관적으로 $y$ 가 $f$ 에 미치는 영향을 구하기 위한 것을 알 수 있습니다.
    * 즉, $y$ 를 조금 바꿨을때 그것이 $q$ 에 대해 1만큼의 영향력을 미치는 것을 의미합니다.
    * 또한 $f$ 에 대한 $q$ 의 영향력은 정확하게 -4 입니다.
    * 결과적으로 이것들을 곱해보면 $f$ 에 대한 $y$ 의 영향력으로 $1\ *\ -4\ =\ -4$ 를 얻습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183278089-3f696dfb-9f71-494d-8141-4892cd0382ae.png">
</p>

* 이제, 계산그래프 뒤로 이동해서 $x$ 에 대한 $f$ 의 gradient를 계산합니다.
    * $x$ 에 대한 $f$ 의 gradient, 즉 $\frac{∂f}{∂x}$ 를 계산합니다.
    * 하지만 여기의 $x$ 는 $f$ 와 바로 연결되어 있지 않습니다.
    * 이때 이 값을 구할 때 chain rule을 사용합니다.
    * 즉, 해당 값을 $\frac{∂f}{∂x}\ =\ \frac{∂f}{∂q} \frac{∂q}{∂x}$ 로 나타낼 수 있으며, 이 것은 Forward Pass 에서 계산한 $\frac{∂f}{∂q}\ =\ z$, $\frac{∂q}{∂x}\ =\ 1$ 를 이용해 계산할 수 있습니다.
    * 따라서 $x$ 에 대한 $f$ 의 미분값은 $\frac{∂f}{∂x}\ =\ \frac{∂f}{∂q} \frac{∂q}{∂x}\ =\ z\ ∗\ 1\ =\ −4$ 가 됩니다.
* 직관적으로 $x$ 가 $f$ 에 미치는 영향을 구하기 위한 것을 알 수 있습니다.
    * 즉, $x$ 를 조금 바꿨을때 그것이 $q$ 에 대해 1만큼의 영향력을 미치는 것을 의미합니다.
    * 또한 $f$ 에 대한 $q$ 의 영향력은 정확하게 -4 입니다.
    * 결과적으로 이것들을 곱해보면 $f$ 에 대한 $x$ 의 영향력으로 $1\ *\ -4\ =\ -4$ 를 얻습니다.

<br>





# Local gradient와 Upstream gradient

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183279143-276eeaec-ea36-49f0-ad91-7b111f9c31dd.png">
</p>

* 지금까지 Backpropagation을 통해 모든 지점에서의 gradient를 구해보았습니다.
* 각 단계에서의 과정을 살펴보면, 일정한 규칙이 있다는 것을 알 수 있습니다.
    * Computational graph의 모든 연산 단계에는 노드가 있습니다.
    * $x$, $y$ 는 local input 입니다.
    * 현재 지점에서의 노드의 출력에 대한 gradient를 local gradient 입니다.
        * local input 인 $x$, $y$ 변화량에 따른 $z$ 의 변화량, 각각 $\frac{∂z}{∂x}$, $\frac{∂z}{∂y}$
    * 다음 node에서 내려오는 gradient를 upstream gradient 입니다.
        * 위 그림에서는 $\frac{∂L}{∂z}$ 
    * Upstream gradient와 local gradient를 곱하면, chain rule에 따라 우리가 원하는 최종 출력에 대한 현재 지점에서의 gradient를 구할 수 있습니다.
        * $\frac{∂L}{∂x}\ =\ \frac{∂L}{∂z} \frac{∂z}{∂x}$, $\frac{∂L}{∂y}\ =\ \frac{∂L}{∂y} \frac{∂z}{∂y}$ 가 됩니다. 
    * 또한, 이 gradient는 다시 이전 노드의 Upstream gradient가 됩니다.
* 따라서, Backpropagation의 이러한 계산은 recursive하게 구현이 가능하기에, 우리가 원하는 모든 변수에 대한 gradient 를 미적분계산 없이 효율적으로 구할 수 있습니다.
* 정리하면, 각 노드는 우리가 계산한 local gradient를 가지고 있으며, backpropagation을 통해 gradient을 얻습니다. 우리는 이것을 받아서 local gradient와 곱하기만 하면됩니다. 노드와 연결된, 노드 이외의 다른 어떤 값에 대하여 신경쓰지 않아도 됩니다.

<br>





# Backpropagation using computational graph: Sigmoid

또 다른 예제로 $w$, $x$에 대한 함수 $f$ 는 $\frac{1}{1\ +\ e^{-(w0x0 + w1x1 + w2)}}$ 와 같습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183283089-127b561d-c4fe-41ac-be98-fd693e4b5df4.png">
</p>

* 이전 예제와 마찬가지로, Computational graph를 그린 후, 차례대로 출력을 계산합니다.
* Sigmoid함수의 Forward Pass입니다. 위 슬라이드를 바탕으로 Backward Pass를 진행하겠습니다.

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183283483-0c25b652-ddc0-454c-b8fd-bc1caf6513ee.png">
</p>

* 우선 최종 출력에서의 gradient를 구합니다.
    * $\frac{∂f}{∂f}\ =\ 1$

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183283417-b494e608-fa31-4885-94f9-0ffe9991e5e2.png">
</p>

* 노드들은 recursive하므로, 지금부터 upstream gradient $\frac{∂L}{∂f}$ 와 local gradient $\frac{∂f}{∂x}$ 로 생각하겠습니다.
* 앞에서 구한 gradient는 현재 지점에서의 upstream gradient가 되므로, 다음과 같습니다.
    * $\frac{∂L}{∂f}\ =\ 1$
* 그리고 local gradient는 위 슬라이드의 미분식에 따라 다음과 같습니다.
    * $\frac{∂f}{∂x}\ =\ \frac{−1}{x^2}\ =\ \frac{−1}{1.37^2}$
* 따라서, 최종 출력에 대한 현재 지점에서의 gradient는 다음과 같습니다.
    * $\frac{∂L}{∂f} \frac{∂f}{∂x}\ =\ \frac{∂L}{∂x}\ =\ (1)(\frac{−1}{1.37^2})\ =\ −0.53$

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183283779-0fb72455-f68e-4fbb-b85c-b8031b38a11c.png">
</p>

* 앞에서 구한 gradient는 현재 지점에서의 upstream gradient가 되므로, 다음과 같습니다.
    * $\frac{∂L}{∂f}\ =\ −0.53$
* 그리고 local gradient는 위 슬라이드의 미분식에 따라 다음과 같습니다.
    * $\frac{∂f}{∂x}\ =\ 1$
* 따라서, 최종 출력에 대한 현재 지점에서의 gradient는 다음과 같습니다.
    * $\frac{∂L}{∂f} \frac{∂f}{∂x}\ =\ \frac{∂L}{∂x}\ =\ (-0.53)(1)\ =\ −0.53$

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183283996-f8891099-5b11-4581-9db2-1e97e528c97c.png">
</p>

* 앞에서 구한 gradient는 현재 지점에서의 upstream gradient가 되므로, 다음과 같습니다.
    * $\frac{∂L}{∂f}\ =\ −0.53$
* 그리고 local gradient는 위 슬라이드의 미분식에 따라 다음과 같습니다.
    * $\frac{∂f}{∂x}\ =\ e^x\ =\ e^{-1}$
* 따라서, 최종 출력에 대한 현재 지점에서의 gradient는 다음과 같습니다.
    * $\frac{∂L}{∂f} \frac{∂f}{∂x}\ =\ \frac{∂L}{∂x}\ =\ (-0.53)(e^{-1})\ =\ −0.20$

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183284507-87e7f3dd-fa64-4ec1-8bbb-ec86d3859d1c.png">
</p>

* 앞에서 구한 gradient는 현재 지점에서의 upstream gradient가 되므로, 다음과 같습니다.
    * $\frac{∂L}{∂f}\ =\ −0.20$
* 그리고 local gradient는 위 슬라이드의 미분식에 따라 다음과 같습니다.
    * $\frac{∂f}{∂x}\ =\ a\ =\ -1$
* 따라서, 최종 출력에 대한 현재 지점에서의 gradient는 다음과 같습니다.
    * $\frac{∂L}{∂f} \frac{∂f}{∂x}\ =\ \frac{∂L}{∂x}\ =\ (-0.20)(-1)\ =\ 0.20$

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183284584-53290619-33a0-4bef-85af-8df6f656a0e0.png">
</p>

* 이제 덧셈 노드에 도달했습니다. 이번에는 위 아래로 나뉘어지는데, 각각에 대해 지금까지와 동일하게 수행하면 됩니다.
* 앞에서 구한 gradient는 현재 지점에서의 upstream gradient가 되므로, 다음과 같습니다.
    * $\frac{∂L}{∂f}\ =\ 0.20$
* 그리고 local gradient는 $+$ 연산자이므로, 다음과 같습니다.
    * 윗쪽 노드
        * $\frac{∂f}{∂x}\ =\ 1$
    * 아래쪽 노드($w_2$)
        * $\frac{∂f}{∂w_2}\ =\ 1$
* 그러므로 최종 출력에 대한 각 지점에서의 gradient는 다음과 같습니다.
    * 윗쪽 노드
        * $\frac{∂L}{∂f} \frac{∂f}{∂x}\ =\ \frac{∂L}{∂x}\ =\ (0.20)(1)\ =\ 0.20$
    * 아래쪽 노드($w_2$)
        * $\frac{∂L}{∂f} \frac{∂f}{∂w_2}\ =\ \frac{∂L}{∂w_2}\ =\ (0.20)(1)\ =\ 0.20$

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183284907-2d3255ab-3516-4409-85d0-42e18f30fac5.png">
</p>

* 이제 곱셈 노드에 도달했습니다. 이번에는 위 아래로 나뉘어지는데, 각각에 대해 지금까지와 동일하게 수행하면 됩니다.
* 앞에서 구한 gradient는 현재 지점에서의 upstream gradient가 되므로, 다음과 같습니다.
    * $\frac{∂L}{∂f}\ =\ 0.20$
* 그리고 local gradient는 $*$ 연산자이므로, 다음과 같습니다.
    * 윗쪽 노드($w_0$)
        * $\frac{∂f}{∂w_0}\ =\ x_0\ =\ -1$
    * 아래쪽 노드($x_0$)
        * $\frac{∂f}{∂x_0}\ =\ w_0\ =\ 2$
* 그러므로 최종 출력에 대한 각 지점에서의 gradient는 다음과 같습니다.
    * 윗쪽 노드($w_0$)
        * $\frac{∂L}{∂f} \frac{∂f}{∂w_0}\ =\ \frac{∂L}{∂w_0}\ =\ (0.20)(x_0)\ =\ (0.20)(-1)\ =\ -0.20$
    * 아래쪽 노드($x_0$)
        * $\frac{∂L}{∂f} \frac{∂f}{∂x_0}\ =\ \frac{∂L}{∂x_0}\ =\ (0.20)(w_0)\ =\ (0.20)(2)\ =\ 0.40$

위와 마찬가지로, $w1$ 과 $x1$ 도 구해보겠습니다.
* local gradient는 $*$ 연산자이므로, 다음과 같습니다.
    * 윗쪽 노드($w_1$)
        * $\frac{∂f}{∂w_0}\ =\ x_1\ =\ -2$
    * 아래쪽 노드($x_1$)
        * $\frac{∂f}{∂x_0}\ =\ w_1\ =\ -3$
* 그러므로 최종 출력에 대한 각 지점에서의 gradient는 다음과 같습니다.
    * 윗쪽 노드($w_1$)
        * $\frac{∂L}{∂f} \frac{∂f}{∂w_1}\ =\ \frac{∂L}{∂w_1}\ =\ (0.20)(x_1)\ =\ (0.20)(-2)\ =\ -0.40$
    * 아래쪽 노드($x_1$)
        * $\frac{∂L}{∂f} \frac{∂f}{∂x_1}\ =\ \frac{∂L}{∂x_1}\ =\ (0.20)(w_1)\ =\ (0.20)(-3)\ =\ -0.60$

<br>

지금까지의 과정을 정리하면 다음과 같습니다.
1. 먼저, 식을 computational graph로 표현합니다.
2. forward pass 계산합니다. (초록색 숫자들)
3. backward gradient 계산합니다. (빨간색 숫자들)
    * local gradient와 upstream gradient의 곱을 계속해서 구하면 됩니다.
    * 즉 chain rule을 사용했습니다.
    * 이때, local gradient는 각 연산에 따른 미분을 통해 구합니다.

<br>





# Computational Graph can be easily expressed

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183286311-833104b5-8650-40a0-86de-ffcef65e3288.png">
</p>

* 위 슬라이드의 파란색 박스 부분 연산들은 사실 Sigmoid 함수를 풀어서 나타낸 것입니다.
* 따라서, Computational Graph로 생각하는 것의 장점은 Sigmoid와 같은 복잡한 함수도 computational graph를 작성하고 simple한 연산들의 조합으로 (local gradient를 구하는 과정을 반복해서) gradient를 구할 수 있습니다.
    * 즉 computational graph를 만들 때, computational 노드에 대해 우리가 원하는 세분화된 정의를 할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183298972-f6926976-62e2-4ceb-851f-28f45b4ed0be.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183298990-93ff51f4-4c10-4304-9820-7e2434edb4d8.png">
</p>

* 다시 예를 들어 sigmoid 함수를 볼 때 gradient를 계산할 수 있습니다.
    * 분석적으로 이를 살펴본다면 sigmoid의 gradient는 $(1 - sigma(x)) * sigma(x)$ 와 같습니다.
* 하나의 big 노드, sigmoid로 바꿀 수 있습니다.
    * 입력이 1(초록색) 출력은 0.73을 갖습니다.
    * gradient를 얻기 위해 하나의 완전한 sigmoid 노드를 이용한다면, (1-sigmoid(x)) * sigmoid(x)를 이용할 수 있습니다.
    * x는 0.73이고 이 값을 위의 식에 대입하면 gradient로 0.2를 얻게 됩니다.
    * upstream gradient인 1과 곱하면 sigmoid gate로 바꾸기 이전의 작은 노드들로 계산된 값과 정확히 같은 값을 얻을 수 있습니다.
* 만약, 아주 복잡한 식에 backpropagation을 적용하기 위해 gradient를 구해야 한다면, Computational graph를 작성하고 하나하나의 local gradient를 구한 후, chain rule을 적용하면 빠르게 문제해결이 가능합니다.

<br>





# Patterns in gradient flow

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183314077-2290dfc6-020c-4910-a1bc-ab617734ca81.png">
</p>

* Backpropagation에서 backward pass 과정을 하다보면 일관된 패턴을 발견할 수 있습니다.
    * add gate: upstream gradient가 연결된 브랜치에 똑같이 같은 값으로 분배됩니다.(gradient distributor)
    * max gate: 큰 값의 local gradient는 1, 작은 값의 local gradient는 0이 되므로 여러 개 중에 가장 큰 값에만 gradient를 전달합니다. 즉, gradient가 한쪽으로만 흐릅니다.(gradient router)
    * mul gate: local gradient가 다른 반대 편 변수의 값이므로, upstream gradient를 받아 다른 변수 값으로 scaling 합니다. 즉, gradient가 switch되며, swithing + scaling의 역할을 합니다.(gradient switcher)
    * copy gate: gradient가 단순히 더해집니다.(gradient adder)

<br>





# Backprop Implementation: “Flat” code

위 과정들을 실제 Flat한 code 형태로 진행과정을 살펴보겠습니다.

```py
def f(w0, x0, w1, x1, w2):
    # Forward pass: Compute output
    s0 = w0 * x0
    s1 = w1 * x1
    s2 = s0 * s1
    s3 = s2 * w2
    L = sigmoid(s3)

    #Backward pass: Compute grads
    grad_L = 1.0
    grad_s3 = grad_L * (1 - L) * L
    grad_w2 = grad_s3
    grad_s2 = grad_s3
    grad_s0 = grad_s2
    grad_s1 = grad_s2
    grad_w1 = grad_s1 * x1
    grad_x1 = grad_s1 * w1
    grad_w0 = grad_s0 * x0
    grad_x0 = grad_s0 * w0
```

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183314741-c4511fdb-cae0-464c-93f7-e8f3498b3663.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183314760-a220866f-d94d-4df0-abd7-c0bae485cfe4.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183314782-ba39d83c-1965-4baa-9f1a-4db40558b5cb.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183314809-59d3b475-e935-4d82-aac4-27be263a5adf.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183314831-e3b13711-2083-4b03-9920-293a9204e3ab.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183314848-6c834ff7-c767-450b-9a97-036316b1a3f0.png">
</p>

<br>





# Vector derivatives

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183320179-13b861e7-7b44-49b2-9cc3-fdccca0c551f.png">
</p>

vector to vector의 경우에 derivative는 Jacobian입니다.

<br>





# Gradients for vectorized code

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183316251-2dddb19a-a8d0-4b43-82b6-e947772b1f9c.png">
</p>

* 위 슬라이드는 Upstream gradient, local gradient, Downstream gradient의 관계를 보여줍니다.
* x, y, z는 scalar 대신에 vector를 가지는것이 일반적이며, vector인 경우에는 local gradient가 Jacobian Matrix가 됩니다.
    * Loss L은 여전히 scalar입니다.
    * Gradients of variables wrt loss have same dims as the original variable
    * gradient는 Jacobian 행렬이 되며, 각 요소의 미분을 포함하는 행렬이 됩니다.(예를들어 x의 각 원소에 대해 z에 대한 미분을 포함하는)
* 즉 지금까지는 scalar값에 대해 backpropagation을 진행해 gradient를 구하는 방법에 대해 살펴봤지만, 실제로 입력값으로 넣어주는 파라미터 $W$ 는 matrix로 되어 있기 때문에 입력값이 vector로 들어가는 경우가 많습니다.
* 입력이 vector로 들어가게 되어도 전체적인 흐름은 같고 gradient의 형태만 달라지게 됩니다.
* 즉 gradient는 스칼라 값이 아니라, Jacobian matrix(자코비안 행렬, 다변수 벡터 함수의 도함수 행렬)로 표현됩니다.

<br>



## What is the Jacobian matrix?

The Jacobian matrix is a matrix composed of the first-order partial derivatives of a multivariable function.

The formula for the Jacobian matrix is the following:

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183317916-1d0cb84b-261a-416d-978e-3f0c105fecc2.png">
</p>

<br>

### Example of Jacobian matrix

Having seen the meaning of the Jacobian matrix, we are going to see step by step how to compute the Jacobian matrix of a multivariable function.
* Find the Jacobian matrix at the point (1,2) of the following function:
    * $f(x,\ y) = (x^4\ +\ 3y^2x,\ 5y^2\ -\ 2xy\ +\ 1)$
* First of all, we calculate all the first-order partial derivatives of the function:
    * $\frac{∂f_1}{∂x}\ =\ 4x^3\ +\ 3y^2$
    * $\frac{∂f_1}{∂y}\ =\ 6yx$
    * $\frac{∂f_2}{∂x}\ =\ -2y$
    * $\frac{∂f_2}{∂y}\ =\ 10y\ -\ 2x$

정리하면 다음과 같이 나타낼 수 있습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183318359-546cfaaf-4753-4b4c-a489-3a327116c830.png">
</p>

Once we have found the expression of the Jacobian matrix, we evaluate it at the point (1,2):

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183318452-716aa3e4-de28-4bc3-8506-a8768b88a94c.png">
</p>

<br>





# Vectorized operations

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183316829-ddf41991-d5ea-44f7-83d1-d84da3c6a64c.png">
</p>

$4096×1$ 크기를 가진 vector가 입력값(input)으로 들어간다고 하고 $4096×1$ 크기의 vector가 결과값(output)으로 나온다고 할때, 요소별로(elementwise) 최대값을 취하는 예제입니다.
* Jacobian matrix의 사이즈는 얼마인가요?
    * $4096\ ∗\ 4096$ size가 됩니다.
* 만약 100개의 input을 동시에 입력으로 갖는 배치를 이용해 작업할 수 있습니다.
    * 노드를 더욱 효율적으로 만들지만 100배 커지게 만듭니다.
    * 즉 Jacobian은 $4096000\ *\ 4096000$ 입니다.
    * 정리하면 데이터의 차원이 커질때, Jacobian matrix의 크기가 제곱으로 커지게 됩니다. 이는 실용적이지 않으며, 실제로는 이렇게 큰 Jacobian을 계산할 필요가 없습니다.

<br>





# Backprop with Vectors

또한 Jacobian matrix의 형태(구조)를 살펴볼 수 있습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183317330-5a6af6a2-159c-4a11-8822-9e0ad1bab19a.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183317342-6d5c18ef-5250-47bf-bda9-be483e1e26d8.png">
</p>

* Jacobian matrix의 형태를 보면 대각성분을 제외한 나머지 성분들이 0이 되는 diagonal matrix가 되므로, Jacobian matrix를 모두 작성하지 않고 diagonal 성분만을 사용해서 gradient를 구할 수 있습니다.
    * input의 n번째 차원의 element는 output의 n번째 차원의 element에만 영향을 미칩니다.
    * 따라서, 각 차원의 element끼리만 영향을 주므로, Jacobian matrix에서 대각성분만 모두 남게되는 것 입니다.
    * 즉 Jacobian행렬은 대각행렬이 됩니다.
    * 출력에 대한 x의 영향에 대해서, 그리고 이 값을 사용하는 것에 대해서만 알면 됩니다.

<br>





# Backprop with Matrices(or Tensors)

다음은 텐서로 확장하였습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183318633-745f5e86-ac52-4429-aa6e-e049c6364e94.png">
</p>

* Jacobians가 확장되었습니다.
    * $\frac{dy}{dx}: [(N×D)×(N×M)]$
    * $\frac{dy}{dw}: [(D×M)×(N×M)]$

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183319072-54f9261a-b063-439e-8851-a449d6590451.png">
</p>
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183319097-fd34a30d-a209-477c-9e71-c0465b452d24.png">
</p>

<br>





# A vectorized example

computational graph보다 구체적인 벡터화 된 예제를 살펴보겠습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183321640-66efe133-5f50-46eb-980a-4e5cf0d42ab3.png">
</p>

* Matrix에서 Backpropagation을 수행합니다.
* 출력에 대한 upstream gradient는 이전과 동일하게 1입니다.
    * $\frac{∂f}{∂f}\ =\ 1$
* local gradient는 L2 연산에서 제곱을 하므로 q2의 미분인 2q입니다.
    * $W⋅x$ 연산을 $q$ 라고 한다면, $f(q)\ =\ q^2_1\ +\ q^2_2\ +\ ...\ +\ q^2_n$ 으로 나타낼 수 있고, 각 $q_i$ 에 대해 편미분하면 $\frac{∂f}{∂q}\ =\ 2q_i$ 로 나타낼 수 있습니다.
    * 즉, gradient 는 $∇_qf\ =\ 2q$ 입니다.
    * $\frac{∂f}{∂q}\ =\ 2q\ =\ 2 \begin{bmatrix}0.22 \\ 0.26\end{bmatrix}\ =\ \begin{bmatrix}0.44 \\ 0.52\end{bmatrix}$
* 따라서, 최종 출력에 대한 q에서의 gradient는 다음과 같습니다.
    * $\frac{∂f}{∂f} \frac{∂f}{∂q}\ =\ \frac{∂f}{∂q}\ =\ 2q\ =\ \begin{bmatrix}0.44 \\ 0.52\end{bmatrix}$

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183321683-24e33e87-d248-46d4-bb98-23a3ea5cbe72.png">
</p>

* 다음으로, 곱셈 노드의 $W$ 브랜치를 계산하겠습니다.
* $W$ 변화량에 따른 $f$ 의 변화량, 즉 $\frac{∂f}{∂W}$ 을 구해야합니다.
* chain rule 적용 위해 $W$ 변화량에 따른 $q$ 의 변화량, $\frac{∂q}{∂W}$ 을 알아야합니다.
* mul gate 는 swithching 역할을 하며, 각 행을 계산할때 다른 행은 관여하면 안되기때문에 $k$ 가 $i$ 일때는 1 을 곱해주고, 아닐때에는 0을 곱해줍니다.
    * $\frac{∂q_k}{∂W_{i,j}}\ =\ 1_{k=i}x_j$
* 이것을 chain rule을 이용해서 $W$ 변화량에 따른 $f$ 의 변화량, $\frac{∂f}{∂W}$ 을 구할 수 있습니다.
    * $\frac{∂f}{∂W_{i,j}}\ =\ \sum_k \frac{∂f}{∂q_k} \frac{∂q_k}{∂W_{i,j}}\ =\ \sum_k(2q_k)(1_{k=i}x_j)\ =\ 2q_ix_j$
* 그러므로 gradient는 $∇_Wf=2q⋅x^T$ 입니다.

즉 정리하면 다음과 같습니다.

$$\frac{∂f}{∂W}\ =\ \frac{∂f}{∂q}x^T\ =\ (2q)x^T\ =\ \begin{bmatrix}0.44 \\ 0.52\end{bmatrix} [0.2\ 0.4]\ =\ \begin{bmatrix}0.088\ 0.176 \\ 0.104\ 0.208\end{bmatrix}$$

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183321719-29136de1-c7af-4b64-8a37-4749f809e66e.png">
</p>

* 다음으로, 곱셈 노드의 $x$ 브랜치를 계산하겠습니다.
* $x$ 변화량에 따른 $f$ 의 변화량, 즉 $\frac{∂f}{∂x}$ 을 구해야합니다.
* chain rule 적용 위해 $x$ 변화량에 따른 $q$ 의 변화량, $\frac{∂q}{∂x}$ 을 알아야합니다.
    * $\frac{∂q_k}{∂x_i}\ =\ W_{k,i}$
* 이것을 chain rule을 이용해서 $x$ 변화량에 따른 $f$ 의 변화량, $\frac{∂f}{∂x}$ 을 구할 수 있습니다.
    * $\frac{∂f}{∂x_i}\ =\ \sum_k \frac{∂f}{∂q_k} \frac{∂q_k}{∂x_i}\ =\ \sum_k(2q_k)(W_{k,i})\ =\ 2q_ix_j$
* 그러므로 gradient는 $∇_xf=W^T⋅q$ 입니다.

즉 정리하면 다음과 같습니다.

$$\frac{∂f}{∂X}\ =\ W^T\frac{∂f}{∂q}\ =\ W^T(2q) = \begin{bmatrix}0.1\ -0.3 \\ 0.5\ 0.8\end{bmatrix} \begin{bmatrix}0.44 \\ 0.52\end{bmatrix}\ =\ \begin{bmatrix}-0.112 \\ 0.636\end{bmatrix}$$

지금까지의 중요한 점은 다음과 같습니다.
* 벡터의 gradient는 항상 원본 벡터의 사이즈와 같습니다.
* 항상 gradient의 shape을 확인해야하며, gradient의 값을 계산했을 때 shape가 다른 결과가 나올 수 있기에 shape은 중요합니다.
* gradient의 각 요소는 함수의 최종 출력에 얼마나 특별한 영향을 미치는지 의미합니다.

<br>





#  Modularized implementation: forward / backward API

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183334510-acd102e1-0c23-46df-98c3-f4d570a5d754.png">
</p>

* computational graph에서의 과정을 실제 코드로 모듈화 된 구현이 가능합니다.
    * 각 노드를 local하게 보았고 upstream gradient와 함께 chain rule을 이용해서 local gradient를 계산했습니다.
    * 이것을 forward pass, 그리고 backward pass의 API로 생각할 수 있습니다.
    * forward pass에서는 노드의 출력을 계산하는 함수를 구현하고, backward pass에서는 gradient를 계산합니다.
* 또한 위치적으로 정렬 된 순서로 수행하기를 원합니다.
    * 노드를 처리하기 전에 이 전에 노드로 들어오는 모든 입력을 처리합니다.
    * 그리고 backward로 보면, 역순서로 모든 게이트를 통과한 다음에 게이트 각각을 거꾸로 호출합니다.

<br>



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183334485-ea636cf5-d9d0-401e-8afe-4680695ee16c.png">
</p>

* 또한 특정 게이트, 곱셈 게이트를 살펴보면 forward()와 backward()로 구성되어있습니다.
* forward pass
    * x, y를 입력으로 받고 z를 리턴합니다.
    * 중요한 것은 값을 저장(cache)해야합니다. backward pass에서 더 많이 사용하기 때문입니다.
* backward pass
    * 입력으로 upstream gradient인 dz를 받고 출력으로 입력인 x와 y에 gradient를 전달합니다.
    * 여기서 출력은 dx와 dy입니다.
    * chain rule을 사용하여 upstream gradient 값을 이용해 다른 브랜치의 값과 곱합니다.

<br>





# Summary so far...

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183342473-cb989bab-39d9-4ab4-b528-e30089522063.png">
</p>

* 신경망은 정말로 크고 복잡합니다.
* 그렇기에 모든 파라미터에 대해 gradient를 손으로 구하고 써내려가는 것은 비현실적입니다.
* 그래서 gradient를 계산하기 위해서 backpropagation을 사용하는 방법에 대해서 언급했고, 신경망의 핵심 기술입니다.
* 그리고 computational graph에서 chain rule을 재귀적으로 적용한 것입니다.
* 또한 각각의 노드에 대한 그래프 구조에 대해 어떻게 forward, backward API 구현으로 나타내는지에 대해서 이야기 했었습니다.
* forward pass에서는 나중에 gradient를 계산할 때 backward pass에서 chain rule에서 사용하기 위해 연산 결과를 계산하고 결과를 저장합니다.
* backward pass에서는 upstream gradient와 저장한 값들을 곱해 각 노드의 input에 대한 gradient를 구하고, 연결된 이전 노드로 통과시킵니다.




# Neural Networks

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/183335475-fb6f8d6a-5cc2-4281-b834-3d99d2d475ae.png">
</p>






좋아요, 이제 마지막으로 신경망에 대해서 이야기 해 봅시다. 사람들은
신경망과 뇌 사이에서 많은 유추와 여러 종류의 생물학적 영감을 이끌어냅니다.

우리는 그것에 대해 조금 배우겠지만, 일단 그것에 대해 이야기해봅시다.

우리는 아시다시피 뇌 기능이 없는, 그저 함수 클래스로서 그것들을 보았었습니다.






<p align="center">
<img alt="image" src="">
</p>






우리가 지금까지 이야기한것은 우리는 다양한 선형 score 함수를 이용했었습니다.
f = W*x, 우리는 이것을 최적화할 때의 실행 예제로 사용했었습니다.
그러면 single 변환을 사용하는 대신에 아주 간단한 형태의 신경망을 사용해봅시다.

위에 있는 것은 선형 변환식이고 다른 하나는
2층짜리 신경망의 레이어입니다. 그렇죠?






<p align="center">
<img alt="image" src="">
</p>






이것이 어떻게 생겼나요, 첫번째로 알다시피

W1과 x의 행렬 곱입니다.

그리고 우리는 이것의 중간값을 얻고, 우리가
가진 max(0, W)의 비선형 함수를 이용해

선형 레이어 출력의 max를 얻습니다.

그리고 이 비선형 변환은 매우 중요합니다.

나중에 이것에 대해 더 이야기하겠습니다. 만약 당신이 선형
레이어들을 쌓는다면 그것들은 결국 선형 함수가 될 것입니다.

자 우리는 첫번째 선형 레이어를 같고 있습니다.
그 다음 우리는 비선형 레이어를 가지고 있습니다.

그리고 이것의 위에 선형 레이어를 추가할 것입니다.

여기에서 최종적으로 우리는 score 함수를 얻을 수 있습니다.

그래서 기본적으로 광범위하게 말하면 신경망은 함수들의 집합(class)입니다.
비선형의 복잡한 함수를 만들기 위해서 간단한 함수들을 계층적으로 여러개 쌓아올린.

그리고 이 아이디어는 여러 단계의 계층적
계산으로 이루어져 있습니다. 맞죠?

여러분이 알다시피

이것은 우리가 하는 주된 방법중 하나입니다.

행렬 곱, 중간에 비선형 함수와 함께 선형 레이어를 여러개 쌓아 계산하는 것입니다.






<p align="center">
<img alt="image" src="">
</p>






그리고 이것이 이것들을 이해하는데 도움이 되는 한 가지는 이전에
선형 score 함수에 대해서 이야기 했던 것을 생각해보세요

우리의 가중치 행렬 W의 각 행이 각각의 템플릿과 같았는지를 논의했던 것을 기억해보세요.

그것은 일종의 표현된 템플릿이었습니다.

우리가 구체적인 클래스에 대한 입력을 보면

예를들면 자동차 템플릿은 이런
종류의 빨간 차처럼 보입니다.

입력에 대한 car 클래스 스코어를 계산합니다.

우리는 하나의 문제에 대해서만 이야기했습니다. 이것은
오직 하나의 자동차 템플릿만 가지고 있습니다. 그렇죠?

여기에는 빨간 차가 있습니다. 우리는 실제로 여러 모드가 있습니다. 우리는 빨간
차가 있는지 노란 차가 있는지, 다른 여러 종류의 차를 찾기 원할 수도 있습니다.

다중 레이어 네트워크는 그것을 가능하게 합니다.

중간 변수 h와 W1은 이러한 종류의 템플릿일 수 있습니다.

하지만 h에서 이 템플릿들에 대한 모든 점수를 가지고 있습니다. 그리고
우리는 이것을 결합하는 또 다른 레이어를 가질 수 있습니다. 그렇죠?

그래서 실제로 차 클래스가 연결되어야 한다고 말할 수 있습니다.
우리는 빨간 차와 노란 차 모두를 찾기 원하므로.

h의 벡터에 대한 가중치인 W2 행렬을 가지고 있기 때문에

좋아, 이것에 대해 질문이 있으십니까?

그래?

[학생이 마이크로 말하는 중]

네, 많은 방법이 있습니다. 당신이 선택할 수
있는 다른 비선형 함수가 많이 있습니다.

당신이 사용할 수 있는 여러 비선형들에 대해서 추후 강의에 이야기 할 것입니다.

[학생] 슬라이드의 그림에서

[학생] 아래에는 W1 가중치에 대한 그림이 있습니다.

[학생] W2에 대한 그림이있습니까?

W1은 입력과 직접적으로 연결되어 있죠 그래서 이
템플릿을 공식화 할 수 있기 때문에 해석가능한 것입니다.

W2는 h의 스코어가 될 것입니다.

얼마나 많은 점수를 얻었는지를 알 수 있습니다. W2는 당신이 가지고
있는 것처럼 나올 것입니다. 나는 그것을 예측할 수 없습니다.

[학생] W1이 10인 경우 말고, 만약 오른쪽을 향하고 있는
말과 왼쪽을 향하고 있는 말을 동시에 표현할 수 있습니까?

네, 정확히 말하자면 W1이 왼쪽으로 향해있는 말과

오른쪽으로 향해있는 말을 모두 가질 수 있는지인데요, 그렇습니다.
W1은 많은 다른 종류의 템플릿이 될 수 있습니까?

안됩니다. 그리고 W2는 모든 템플릿의 가중치를 합한 것입니다.

그것은 특정 클래스에 대한 최종 스코어를 얻기 위해

여러 템플릿의 가중치를 합할 수 있게 해줍니다.

[학생] 만약 실제로 왼쪽으로 향하고 있는 말의 이미지를 이용해 작업을
하면 왼쪽을 향하는 말의 템플릿에서 높은 점수를 얻고

[학생] 오른쪽을 향하고 있는
말에서는 낮은 점수를 얻겠죠

[학생]그러면 이것의 최대값을 구합니까?

질문은 만약 우리의 이미지 x가 왼쪽을 향하고 있는
말이고 그리고 W1이 왼쪽을 향하고 있는 말의 템플릿과

오른쪽을 향하고 있는 말의 템플릿을 가질 때,
이때 어떤일이 일어나는지에 대해서였습니다.

무슨일이 일어날까요?

h에서 왼쪽을 향하고 있는 말은 점수가 높게 나올 것이고,

오른쪽을 향하고 말에 대해서는 낮은 점수를 얻을 것입니다.

그리고 W2는 가중치들의 합입니다. 따라서 최대값이 아닙니다.
이것은 템플릿들의 가중치의 합계입니다.

하지만 이 템플릿중 하나에 대해서 정말로 높은 점수를 얻었거나 두
템플릿 모두에 대해서 낮거나 중간의 점수를 얻었다고 해봅시다.

이것들의 조합은 둘다 높은 점수를 얻을 것입니다.
그렇죠?

결국 당신이 얻고 싶은 것은 특정한
종류의 말을 가지고 있을 때

일반적으로 높은 점수를 얻는 것입니다. 어떤 종류의 말이라도 가지고
있을 때, 앞을 보고 있는 말을 가지고 있다고 해봅시다.

당신은 아마 왼쪽, 오른쪽을 보는 말의 템플릿에
대해서 두개 다 중간정도의 점수를 가질 것입니다.

질문있나요?

[학생] W2가 가중치를 쓰고 있습니까? 아니면 H가 가중치를 쓰고 있습니까?

W2가 가중치를 쓰고 있습니다. 질문은 즉 "W-two가 가중치를 부여합니까,
아니면 h가 가중치를 부여하고 있습니까?" 이거죠? h는 값입니다.

h는 W1에서 가지고 있는 템플릿에 대한 스코어 값입니다.

그래서 h는 스코어 함수입니다. 아시겠습니까?

그것은 W1의 각 템플릿이 얼마나 많은지를 나타내주는 것이고

W2는 이들 모두에 가중치를 부여하고 모든 중간
점수를 더해 클래스에 대한 최종 점수를 얻습니다.

[학생] 그것은 비선형입니까?

비선형이냐는 질문이 있었는데요
h 이전에 비선형성이 발생하므로

h는 비선형 값입니다. 우리는
이것에 대해 이야기하고 있습니다.

이 예제에서 직관적으로 W1은 같은 템플릿을 찾는 것이고

그리고 W2는 이것들에 대한 가중치입니다.

실제로는 정확히 이렇지는 않습니다. 왜냐하면 말했던
것처럼 비선형적인 것들이 나오기 때문입니다.

이것들은 대략적인 해석을 가지고 있습니다.

[학생] h는 W1*X 인가요?

네네, 질문은 h는 W1*X인가 였는데요

h는 딱 W1과 X의 곱입니다. max function을 위에 두고 있는.






<p align="center">
<img alt="image" src="">
</p>






우리는 이 두 레이어로 구성된
신경망에 대해서 이야기 했습니다.

그리고 우리는 더 많은 레이어를 쌓아 임의의 깊은 신경망을 구성할 수 있습니다.

우리는 이것을 한번 더 곱해봅시다.

다른 비선형 행렬 W3로 곱할 수 있습니다. 이제 우리는 3-레이어 신경망을 가지고 있습니다.
그렇죠? 이런것으로부터 깊은 신경망이라는 용어가 나오게 됩니다.

복잡한 네트워크의 경우 이러한 계층을
여러 개 쌓는 아이디어에서 나옵니다.






<p align="center">
<img alt="image" src="">
</p>






과제를 하다보면 쓰기(writing) 연습을 하게 될 것입니다.

신경망 중 하나를 학습하다보면, 과제 2에서

기본적으로 forward pass, backward pass를 완벽하게 구현하게 됩니다.

gradient를 구하기 위해 앞서 보았던 chain rule을 사용하게 됩니다.
2-레이어 네트워크의 전체 구현은 실제로 매우 단순합니다.

이것은 20줄로 할 수 있습니다.






<p align="center">
<img alt="image" src="">
</p>






그리고 당신은 이 과제 2에서 이 파트들을 쓰는 연습을 하게 될 것입니다.






<p align="center">
<img alt="image" src="">
</p>






우리는 신경망이 무엇인지를 함수로써 살펴보았습니다.

알다시피, 우리는 신경망에 대한 생물학적 영감에 대해

많은 이야기를 하는것을 듣게 됩니다.

이것을 강조하는것은 중요합니다.

비록 분석이 실제로 loose 하지만

이것은 정말로 loose한 관계입니다. 하지만 이러한 연결과
영감이 어디에서 왔는지 이해하는데 있어 여전히 흥미롭습니다.

이것에 대해서 간략하게 이야기해봅시다.






<p align="center">
<img alt="image" src="">
</p>






뉴런에 대해서 생각해보면 그것은 단순합니다.

이것은 뉴런의 다이어그램입니다.

우리는 각 뉴런을 따라 전달되는 신호를 가지고 있습니다. 그리고 우리는 서로 연결된
많은 뉴런을 가지고 있습니다. 그리고 각 뉴런은 수상돌기를 가지고 있습니다.

그것들은 뉴런에 들어온 신호를 받습니다.

그리고 우리는 세포체(cell
body)를 가지고 있습니다.

이것은 기본적으로 들어오는 신호를 종합합니다.

이것들을 받아  합친 후에 모든 신호는

하류 뉴런과 연결된 다른 세포체로 이동합니다.

이것은 축삭(axon)을 통해 운반됩니다.






<p align="center">
<img alt="image" src="">
</p>






지금까지 우리가 해왔던 것을 보았을 때 각 computational
node는 실제 동작과 비슷한 방식으로 볼 수 있습니다.

노드가 computational
graph에서 서로 연결되어 있고

입력 또는 신호는 x입니다. 뉴런에 들어가는 것처럼

모든 x0, x1, x2, 모든 x는

우리의 가중치 W와 결합되어 합쳐집니다.

우리는 일종의 계산을합니다. 우리가 해왔던 일부
계산에서는 W를 x와 곱하고 b를 더했습니다.

모든것을 통합하였고 그리고 활성
함수를 꼭대기에 적용했습니다.

출력 값을 얻었고

아래로 연결된 뉴런에 전달했습니다.






<p align="center">
<img alt="image" src="">
</p>






이것을 보면 당신은 매우 비슷한 방식이라고 생각할 수 있습니다. 그렇죠?

알다시피 이것들은 들어오는 신호가
시냅스에서 연결되는 것들입니다. 그렇죠?

여러개의 뉴런과 연결된 시냅스, 그리고 수상돌기는

세포체로 이 모든 정보를 합칩니다.

그리고 나서 우리는 출력을 실었습니다.

이것은 표현할 수 있는 일종의 비유입니다.

그리고 당신은 이 활성함수를 볼 수 있습니다. 그렇죠?

이것은 입력을 받아 나중에 출력이 될 하나의 숫자를 보여주는 것입니다.

우리는 sigmoid 및 여러 종류의
비선형과 관련해서 이야기 했습니다.

그리고 이것은 loose한 종류입니다.

당신은 비선형성으로 뉴런의 firing이나 spiking에 대해서 표현할 수 있습니다.

우리의 뉴런이 이산 spike 종류를
사용해서 신호를 전송합니다. 그렇죠?

알다시피 생각해볼 수 있습니다.

그것들이 매우 빠르게 spiking 하는 경우 전달되는 강한 신호가 있습니다.
그래서 우리는 활성함수를 통과한 다음에 이 값을 생각할 수 있습니다.

알다시피 실제로, 이것을 연구하고 있는 신경 과학자들은

실제 방식과 가장 유사한 비선형성의 종류가

ReLU라고 말합니다.

나중에 더 자세히 볼꺼지만 이것은 함수입니다.

모든 음수 입력값에 대해서 0이고,
양수 입력에 대해서는 선형 함수인.

알다시피 우리는 활성함수에대해
나중에 더 이야기 할 것입니다.






<p align="center">
<img alt="image" src="">
</p>






실제로 이것들은 뉴런들의 행동과 비슷하지만,

실제 생물학적 뉴런은 이보다 훨씬 복잡하기 때문에

이러한 종류의 신경을 만드는 것은 매우 신중해야 합니다.

생물학적 뉴런에는 여러 종류가 있습니다. 수상돌기는
실제로 복잡한 비선형 계산을 수행할 수 있습니다.

우리의 시냅스, 이전에 비유적으로 그렸던 W0은

우리가 가진 것처럼 단일 가중치가 아니며,
실제로 엄청 복잡한 비선형 시스템입니다.

우리의 활성함수를 일종의 rate
code나 fire rate로 해석하는

이러한 아이디어는 실제로는 불충분합니다.

이러한 종류의 firing rate는
충분한 모델이 아닐 것입니다.

뉴런이 어떻게 하부의 뉴런과 통신하는지에 대해서.

아주 간단한 방법처럼 뉴런은 가변
fire rate로 발사할 것이고,

이 가변성에 대해서 고려되어야 할 것입니다.

우리가 다루는 것보다 훨씬 복잡한 모든 것이 있습니다.

수상돌기의 계산에 예제에 관한 레퍼런스가 있습니다.

이 주제에 관심이 있다면 살펴볼 수 잇습니다.

하지만 실제로 알다시피, 우리는 높은 수준에서
많은 뉴런을 닮은 것들을 볼 수 있습니다.

하지만 실제로 그것들은 더 복잡합니다.






<p align="center">
<img alt="image" src="">
</p>






우리는 사용될 수 있는 많은 다른 종류의 활성함수에 대해서 이야기 했습니다.

거기에는 앞서 언급한 ReLu도 있었습니다. 그리고 당신이
사용하고 싶어할 수 있는 다른 종류의 활성함수에 대해서

나중에 자세히 다룰 것입니다.






<p align="center">
<img alt="image" src="">
</p>






그리고 우리는 다른 종류의 신경망에
대해서도 이야기 할 것입니다.

우리는 완전히 연결된 신경망 예제를 보았습니다.

각 레이어는 행렬곱을 했고,

우리가 2-레이어 신경망이라고 불렀던 것은

우리가 선형 레이어를 2개 가지고 있다는 사실에 입각했었습니다.

그것은 행렬 곱셈을 하는 완전히 연결된 두 층입니다.

이것을 하나의 히든 레이어 네트워크로 부를 수 있습니다.
행렬 곱의 수를 세는 대신에 히든 레이어를 셀 수 있습니다.

제 생각에는 두 가지 중 하나를 사용할 수 있습니다.

아마 2-레이어 신경망이 조금 더 일반적으로 사용될 것 같습니다.
또한 여기에서 우리의 3-레이어 신경망은

2-히든 레이어 신경망으로 부를 수 있습니다.






<p align="center">
<img alt="image" src="">
</p>






feed forward를 할 때 우리는 보았습니다.

네트워크 안의 각 노드에 forward
pass를 수행하는 것은

이전에 보여주었던 뉴런의 행동과 같습니다. 맞나요?

그리고 실제 동작에서 생각해볼 수 있는 것은 기본적으로
각 히든 레이어는 완전한 벡터인 것입니다.

이러한 행렬 곱셈을 통해

우리의 뉴런 값을 계산하는 방식으로 쓰면

우리는 뉴런의 전체 레이어를 효율적으로 평가할 수 있습니다.

따라서 하나의 행렬 곱셈을 사용하면 10,50 또는 100개의
뉴런을 의미하는 레이어의 출력값을 얻을 수 있습니다.






<p align="center">
<img alt="image" src="">
</p>






좋아요, 그리고 다시 살펴보면

우리가 가지고 있는 벡터 행렬 형태의 출력은 비선형성을 가집니다.

여기에서 사용한 F는 sigmoid 함수입니다.

그리고 우리의 데이터는 x로 받습니다.

첫 번째 행렬 곱 W1은 가장 윗줄에 있습니다.

비선형성을 적용한 다음, 두 번째 히든 레이어
h2를 얻기 위한 두 번째 행렬곱을 합니다.

그리고 최종 출력을 얻습니다. 아시겠습니까?

그리고 아시다시피 앞에서 보았던 backward pass는
신경망을 쓰기(write) 위해 기본적으로 필요할 것입니다.

그리고 모든 것을 계산하기 위해
backpropagation을 쓰기만 하면 됩니다.

그리고 신경망이 무엇인지에 대한 주요 아이디어가 있습니다.








<p align="center">
<img alt="image" src="">
</p>






요약하자면, 어떻게 뉴런을
선형 레이어와 fully-connected로 재배열하는지에 대해서 이야기했습니다.

이 레이어의 추상화는 이 모든
것을 계산하는데 매우 효율적인

벡터화된 코드를 사용할 수 있게 하는 좋은 속성을 가지고 있습니다.
우리는 또한신경망이 생물학적 비유와 loose한 영감을 가지고 있음을

명심하는 것이 얼마나 중요한지에 대해 이야기 했습니다.
하지만 실제로는 신경이 아닙니다.

제 말은 즉 우리가 만들고 있는 것은
꽤나 loose한 유추라는 것입니다.
