---
layout: post
title: confusion matrix 및 정밀도(Precision)와 재현율(Recall)
category: Machine Learning
tag: Machine-Learning
---

 


# 오차 행렬(confusion matrix, 혼동행렬)

이진 분류에서 성능 지표로 잘 활용되는 **<span style="color:red">오차행렬(confusion matrix, 혼동행렬)</span>** 은 **학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고(confused) 있는지를 함께 보여주는 지표** 입니다. **<u>즉, 이진 분류의 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표</u>** 입니다.

오차 행렬은 다음과 같이 4분면 행렬에서 **실제 레이블 클래스 값과 예측 레이블 클래스 값이 어떠한 유형을 가지고 매핑** 되는지를 나타내며, 예측 클래스와 실제 클래스의 값 유형에 따라 결정되는 **TN**, **FP**, **FN**, **TP** 형태로 오차 행렬의 4분면을 채울 수 있습니다. **<u>TN, FP, FN, TP 값을 다양하게 결합해 분류 모델 예측 성능의 오류가 어떠한 모습으로 발생하는지 알 수 있습니다.</u>**

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/235571416-747992c7-779b-4a57-9c64-3c8d442397a5.png">
</p>

<br>



**TN**, **FP**, **FN**, **TP** 는 **<u>예측 클래스와 실제 클래스의 Positive 결정값(값 1)과 Negative 결정 값(값 0)의 결합에 따라 결정</u>** 됩니다. **<span style="color:red">앞 문자 True/False 는 예측값과 실제값이 '같은가/틀린가'를 의미</span>** 하고, **<span style="color:red">뒤 문자 **Negative/Positive** 는 예측 결과 값이 부정(0)/긍정(1)을 의미</span>** 합니다.

* **TN** 는 예측값을 **Negative** 값 0으로 예측했고, **실제 값** 역시 Negative 값 0
* **FP** 는 예측값을 **Positive** 값 1로 예측했는데, **실제 값은** Negative 값 0
* **FN** 은 예측값을 **Negative** 값 0으로 예측했는데, **실제 값은** Positive 값 1
* **TP** 는 예측값을 **Positive** 값 1로 예측했고, **실제 값** 역시 Positive 값 1

<br>

다음 그림은 **TN**, **FP**, **FN**, **TP** 구분을 재미있게 표현한 그림입니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/235572566-2102397e-c8ab-4216-92d9-416a5cb1d590.png">
</p>

<br>




사이킷런은 오차 행렬을 구하기 위해 **confusion_matrix() API** 를 제공합니다. 

**MNIST** 데이터의 10%만 True, 나머지 90%는 False인 **불균형한 데이터 세트** 와 **MyFakeClassifier** 를 생성하고, **MyFakeClassifier** 의 예측 성능 지표를 **confusion_matrix()** 를 이용해 오차 행렬로 표현하겠습니다.
```py
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd

class MyFakeClassifier(BaseEstimator):
    def fit(self, X, y):
        pass
    
    # 입력값으로 들어오는 X 데이터 셋의 크기만큼 모두 0값으로 만들어서 반환
    def predict(self, X):
        return np.zeros((len(X), 1), dtype=bool)

# 사이킷런의 내장 데이터 셋인 load_digits()를 이용하여 MNIST 데이터 로딩
digits = load_digits()

# digits번호가 7번이면 True이고 이를 astype(int)로 1로 변환, 7번이 아니면 False이고 0으로 변환. 
y = (digits.target == 7).astype(int)
X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=11)

# 불균형한 레이블 데이터 분포도 확인. 
print('레이블 테스트 세트 크기 :', y_test.shape)
print('테스트 세트 레이블 0 과 1의 분포도')
print(pd.Series(y_test).value_counts())

# Dummy Classifier로 학습/예측/정확도 평가
fakeclf = MyFakeClassifier()
fakeclf.fit(X_train, y_train)
fakepred = fakeclf.predict(X_test)
print('\n모든 예측을 0으로 하여도 정확도는 : {:.3f}'.format(accuracy_score(y_test, fakepred)))
```
```
[output]
레이블 테스트 세트 크기 : (450,)
테스트 세트 레이블 0 과 1의 분포도
0    405
1     45
dtype: int64

모든 예측을 0으로 하여도 정확도는 : 0.900
```


<br>


```py
from sklearn.metrics import confusion_matrix

# 앞절의 예측 결과인 fakepred와 실제 결과인 y_test의 Confusion Matrix출력
confusion_matrix(y_test, fakepred)
```
```
[output]
array([[405,   0],
       [ 45,   0]])
```


<br>



이진 분류의 **TN**, **FP**, **FN**, **TP** 는 상단 도표와 동일한 위치를 가지며, array에서 가져올 수 있습니다. 즉, **TN** 은 **array[0,0]** 로 405, **FP** 는 **array[0,1]** 로 0, **FN** 은 **array[1,0]** 로 45, **TP** 는 **array[11]** 로 0에 해당합니다.

<br>



**MyFakeClassifie** 는 **load_digits()** 에 **target==7** 인지 아닌지에 따라 클래스 값을 **True/False 이진 분류** 로 변경한 데이터 세트를 사용해 무조건 **Negative** 로 예측하는 **Classifier** 였고, 테스트 데이터 세트의 클래스 값 분포는 0이 405건, 1이 45건입니다.


따라서 **TN** 은 전체 450건 데이터 중 무조건 Negative 0으로 예측해서 True가 된 결과 405건, **FP** 는 Positive 1로 예측한 건수가 없으므로 0건, **FN** 은 Positive 1인 건수 45건을 Negative로 예측해서 False가 된 결과 45건, **TP** 는 Positive 1로 예측한 건수가 없으므로 0건입니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/235574101-4c278719-26b4-4b4a-8f58-9794451f210f.png">
</p>


---


**TN**, **FP**, **FN**, **TP** 값은 **Classifier 성능의 여러 면모를 판단할 수 있는 기반 정보를 제공** 합니다. 이 값을 조합해 **Classifier**의 성능을 측정할 수 있는 주요 지표인 **<span style="color:red">정확도(Accuracy)</span>**, **<span style="color:red">정밀도(Precision)</span>**, **<span style="color:red">재현율(Recall)</span>** 값을 알 수 있습니다.


**<span style="color:red">정확도</span>** 는 **예측값과 실제 값이 얼마나 동일한가에 대한 비율만으로 결정** 됩니다. **<span style="color:red">즉, 오차 행렬에서 True에 해당하는 값인 TN과 TP에 좌우</span>** 됩니다. 정확도는 오차 행렬상에서 다음과 같이 재정의될 수 있습니다.

$$
정확도 = \frac{예측\ 결과와\ 실제\ 값이\ 동일한\ 건수}{전체\ 데이터\ 수}=\frac{FN + TP}{TN + FP + FN + TP}
$$



일반적으로 **불균형한 레이블 클래스를 가지는 이진 분류 모델** 에서는 많은 데이터 중에서 중점적으로 찾아야 하는 매우 적은 수의 결과값에 **Positive** 를 설정해 1 값을 부여하고, 그렇지 않은 경우는 **Negative** 로 0 값을 부여하는 경우가 많습니다. 예를 들어 암 검진 예측 모델에서는 암이 양성일 경우 **Positive** 양성으로 1, 암이 음성일 경우 **Negative** 음성으로 값이 할당되는 경우가 일반적입니다.


<br>



**<u>불균형한 이진 분류 데이터 세트에서는 Positive 데이터 건수가 매우 작기 때문에</u>** 데이터에 기반한 **<span style="color:red">ML 알고리즘은 Positive보다는 Negative로 예측 정확도가 높아지는 경향이 발생</span>** 합니다.

10,000건의 데이터 세트에서 9,900건이 Negative이고 100건이 Positive라면 **<u>Negative로 예측하는 경향이 더 강해져</u>**서 **<span style="color:red">TN은 매우 커지고</span>**, **<span style="color:red">TP는 매우 작아지게</span>** 됩니다. 또한 **<u>Negative로 예측할 때 정확도가 높기 때문에</u>** **<span style="color:red">FN(Negative로 예측할 때 틀린 데이터 수)이 매우 작고</span>**, **<u>Positive로 예측하는 경우가 작기 때문</u>**에 **<span style="color:red">FP 역시 매우 작아집니다.</span>**

<br>



결과적으로 **<span style="color:red">정확도 지표는 비대칭한 데이터 세트에서 Positive에 대한 예측 정확도를 판단하지 못한 채 Negative에 대한 예측 정확도만으로도 분류의 정확도가 매우 높게 나타나는 수치적인 판단 오류</span>** 를 일으키게 됩니다. 불균형한 데이터 세트에서 정확도만으로는 모델 신뢰도가 떨어질 수 있는 사례를 확인했습니다.

<br>






# 정밀도와 재현율