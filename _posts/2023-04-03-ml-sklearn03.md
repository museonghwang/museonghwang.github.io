---
layout: post
title: Scikit-Learn Model Selection 모듈
category: Machine Learning
tag: Machine-Learning
---

 

<br>
<p align="center">
<img alt="image" src="">
</p>

<br>

**<span style="color:red"></span>**
**<u></u>** 입니다.

사이킷런의 **model_selection** 모듈은 학습 데이터와 테스트 데이터 세트를 분리하거나 교차 검증 분할 및 평가, 그리고 **Estimator** 의 하이퍼 파라미터를 튜닝하기 위한 다양한 함수와 클래스를 제공합니다.

<br>


# 학습/테스트 데이터 세트 분리 - train_test_split()

먼저 테스트 데이터 세트를 이용하지 않고 학습 데이터 세트로만 학습하고 예측하면 무엇이 문제인지 살펴보겠습니다. 다음 예제는 학습과 예측을 동일한 데이터 세트로 수행한 결과입니다.
```py
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
dt_clf = DecisionTreeClassifier()
train_data = iris.data
train_label = iris.target
dt_clf.fit(train_data, train_label)

# 학습 데이터 셋으로 예측 수행
pred = dt_clf.predict(train_data)
print('예측 정확도:', accuracy_score(train_label, pred))
```
```
[output]
예측 정확도: 1.0
```

예측 결과가 100% 인 이유는 이미 학습한 학습 데이터 세트를 기반으로 예측했기 때문입니다. **<u>즉, 모의고사를 이미 한 번 보고 답을 알고 있는 상태에서 모의고사 문제와 똑같은 본고사 문제가 출제됐기 때문</u>** 입니다. **<span style="color:red">따라서 예측을 수행하는 데이터 세트는 학습을 수행한 학습용 데이터 세트가 아닌 전용의 테스트 데이터 세트여야</span>** 합니다.



사이킷런의 **train_test_split()** 를 통해 원본 데이터 세트에서 학습 및 테스트 데이터 세트를 쉽게 분리할 수 있습니다. **sklearn.model_selection** 모듈에서 **train_test_split** 을 로드합니다.

- **train_test_split()**
    - **X**: 피처 데이터 세트
    - **y**: 레이블 데이터 세트
    - **test_size**: 전체 데이터에서 테스트 데이터 세트 크기를 얼마나 샘플링할 것인가를 결정.
    - **train_size**: 전체 데이터에서 학습용 데이터 세트 크기를 얼마나 샘플링할 것인가를 결정.
    - **shuffle**: 데이터를 분리하기 전에 데이터를 미리 섞을지를 결정.
    - **random_state**: 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위한 난수 값.
    - **반환값**: **튜플** 형태. 순차적으로 **학습용 피처 데이터 세트**, **테스트용 피처 데이터 세트**, **학습용 레이블 데이터 세트**, **테스트용 레이블 데이터 세트** 가 반환.

붓꽃 데이터 세트를 **train_test_split()** 를 이용해 테스트 데이터 세트를 전체의 30%로, 학습 데이터 세트를 70%로 분리하겠습니다.
```py
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

dt_clf = DecisionTreeClassifier()
iris_data = load_iris()

X_train, X_test, y_train, y_test = train_test_split(
    iris_data.data,
    iris_data.target,
    test_size=0.3,
    random_state=121
)
```

<br>


학습 데이터를 기반으로 **DecisionTreeClassifier** 를 학습하고 해당 모델을 이용해 예측 정확도를 측정해보겠습니다.
```py
dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))
```
```
[output]
예측 정확도: 0.9556
```

<br>

붓꽃 데이터는 **데이터 양이 크지 않으므로**(30% 정도인 테스트 데이터는 45개) 이를 통해 **알고리즘의 예측 성능을 판단하기에는 그리 적절하지 않습니다.**

학습을 위한 데이터의 양을 일정 수준이상으로 보장하는 것도 중요하지만, **<span style="color:red">학습된 모델에 대해 다양한 데이터를 기반으로 예측 성능을 평가해보는 것도 매우 중요</span>** 합니다.

<br>





# 교차 검증

**<span style="color:red">과적합(Overfitting)</span>** 은 **모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 것** 을 말합니다. 하지만 고정된 학습 데이터와 테스트 데이터로 평가를 하다 보면 **<span style="color:red">테스트 데이터에만 최적의 성능을 발휘할 수 있도록 편향되게 모델을 유도하는 경향</span>** 이 생기게 됩니다.


**<span style="color:red">결국 테스트 데이터에만 과적합</span>** 되는 학습 모델이 만들어져 다른 테스트용 데이터가 들어올 경우에는 **<span style="color:red">성능이 저하</span>** 됩니다.

<br>

ML은 데이터에 기반하며, 데이터는 이상치, 분포도, 다양한 속성값, 피처 중요도 등 여러 가지 ML에 영향을 미치는 요소를 가지고 있습니다. 특정 ML 알고리즘에서 최적으로 동작할 수 있도록 데이터를 선별해 학습한다면 실제 데이터와는 많은 차이가 있을 것이고 결국 성능 저하로 이어질 것입니다.

<br>

이러한 문제점을 개선하기 위해 교차 검증을 수행합니다. **<span style="color:red">교차 검증</span>** 은 **<u>데이터 편중을 막기 위해서 별도의 여러 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행하는 것</u>** 입니다. 그리고 각 세트에서 수행한 평가 결과에 따라 하이퍼 파라미터 튜닝 등의 모델 최적화를 더욱 손쉽게 할 수 있습니다.


대부분의 ML 모델의 성능 평가는 교차 검증 기반으로 1차 평가를 한 뒤에 최종적으로 테스트 데이터 세트에 적용해 평가하는 프로세스입니다. 테스트 데이터 세트 외에 별도의 검증 데이터 세트를 둬서 최종 평가 이전에 학습된 모델을 다양하게 평가하는 데 사용합니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/235067292-621dcdd4-685f-485a-bae4-794fd2f48e06.png">
</p>

<br>





## K 폴드 교차 검증

**<span style="color:red">K 폴드 교차 검증</span>** 은 가장 보편적으로 사용되는 교차 검증 기법으로, **<u>K개의 데이터 폴드 세트를 만들어서(K등분) K번만큼 각 폴트 세트에 학습과 검증 평가를 반복적으로 수행하는 방법</u>** 입니다.


다음 그림은 5 폴드 교차 검증을 수행합니다(즉, K가 5). 5개의 폴드된 데이터 세트를 학습과 검증을 위한 데이터 세트로 변경하면서 5번 평가를 수행한 뒤, 이 **5개의 평가를 평균한 결과를 가지고 예측 성능을 평가** 합니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/235067202-5b5a2f64-36c0-43dc-a38b-0b1a2fd8d88f.png">
</p>

<br>


이렇게 **학습 데이터 세트와 검증 데이터 세트를 점진적으로 변경하면서 마지막 5번째(K번째)까지 학습과 검증을 수행하는 것** 이 바로 **<span style="color:red">K 폴드 교차 검증</span>** 입니다. 5개(K개)의 예측 평가를 구했으면 이를 **평균해서 K 폴드 평가 결과로 반영** 하면 됩니다.

사이킷런에서는 **K 폴드 교차 검증 프로세스** 를 구현하기 위해 **<span style="color:red">KFold</span>** 와 **<span style="color:red">StratifiedKFold</span>** 클래스를 제공합니다.

<br>

먼저 **KFold** 클래스를 이용해 5개의 폴드 세트로 분리하는 KFold 객체를 생성하고, 붓꽃 데이터 세트를 교차 검증하고 예측 정확도를 구하겠습니다.
```py
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np

iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.
kfold = KFold(n_splits=5)
cv_accuracy = []
print('붓꽃 데이터 세트 크기:', features.shape[0])
```
```
[output]
붓꽃 데이터 세트 크기: 150
```

생성된 **KFold** 객체의 **split()** 을 호출해 전체 붓꽃 데이터를 5개의 폴드 데이터 세트로 분리합니다. 전체 붓꽃 데이터는 모두 150개이므로, 학습용 데이터 세트는 4/5인 120개, 검증 테스트 데이터 세트는 1/5인 30개로 분할됩니다.

**KFold** 객체는 **split()** 을 호출하면 **<u>학습용/검증용 데이터로 분할할 수 있는 인덱스를 반환</u>** 합니다. 실제로 학습용/검증용 데이터 추출은 반환된 인덱스를 기반으로 개발 코드에서 직접 수행해야 합니다. 교차 검증 수행을 통해 학습과 검증을 반복해 예측 정확도를 측정해보겠습니다.
```py
n_iter = 0

# KFold객체의 split() 호출하면 폴드별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  
for train_index, test_index in kfold.split(features):
    # kfold.split()으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    
    #학습 및 예측 
    dt_clf.fit(X_train, y_train)    
    pred = dt_clf.predict(X_test)
    n_iter += 1
    
    # 반복 시 마다 정확도 측정 
    accuracy = np.round(accuracy_score(y_test, pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    cv_accuracy.append(accuracy)
    print('#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}\n'.format(n_iter, test_index))
    
# 개별 iteration별 정확도를 합하여 평균 정확도 계산 
print('### 평균 검증 정확도:', np.mean(cv_accuracy)) 
```
```
[output]
#1 교차 검증 정확도 :1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30
#1 검증 세트 인덱스:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]

#2 교차 검증 정확도 :0.9667, 학습 데이터 크기: 120, 검증 데이터 크기: 30
#2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
 54 55 56 57 58 59]

#3 교차 검증 정확도 :0.8667, 학습 데이터 크기: 120, 검증 데이터 크기: 30
#3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83
 84 85 86 87 88 89]

#4 교차 검증 정확도 :0.9333, 학습 데이터 크기: 120, 검증 데이터 크기: 30
#4 검증 세트 인덱스:[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119]

#5 교차 검증 정확도 :0.7333, 학습 데이터 크기: 120, 검증 데이터 크기: 30
#5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149]

### 평균 검증 정확도: 0.9
```

<br>




## Stratified K 폴드

**<span style="color:red">Stratified K 폴드</span>** 는 **<u>불균형한(imbalanced) 분포도를 가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식</u>** 입니다.

**불균형한 분포도** 를 가진 레이블 데이터 집합은 **특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우치는 것** 을 말합니다.


<br>

가령 대출 사기 데이터를 예측한다고 가정해 보겠습니다. 이 데이터 세트(대출 사기: 1, 정상 대출: 0)는 1억 건이고, 대출 사기가 약 1000건이 있다고 한다면 전체의 0.0001%의 아주 작은 확률로 대출 사기 레이블이 존재합니다.

이렇게 작은 비율로 1 레이블 값이 있다면 K 폴드로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라도 **레이블 값인 0과 1의 비율을 제대로 반영하지 못하는 경우가 쉽게 발생** 합니다.

대출 사기 레이블이 1인 레코드는 비록 건수는 작지만 알고리즘이 대출 사기를 예측하기 위한 중요한 피처 값을 가지고 있기 때문에 매우 중요한 데이터 세트입니다. **<span style="color:red">따라서 원본 데이터와 유사한 대출 사기 레이블 값의 분포를 학습/테스트 세트에도 유지하는 게 매우 중요합니다.</span>**

<br>

**<span style="color:red">Stratified K 폴드</span>** 는 **<u>원본 데이터의 레이블 분포를 먼저 고려한 뒤 이 분포와 동일하게 학습과 검증 데이터 세트를 분배</u>** 하여, K 폴드가 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결합니다.

먼저 K 폴드의 문제점을 확인해 보고 이를 사이킷런의 **StratifiedKFold 클래스** 를 이용해 개선해 보겠습니다.
```py
import pandas as pd

iris = load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label'] = iris.target
iris_df['label'].value_counts()
```
```
[output]
0    50
1    50
2    50
Name: label, dtype: int64
```

<br>


레이블 값은 0(Setosa), 1(Versicolor), 2(Virginica) 값 모두 50개로 동일합니다. 이슈가 발생하는 현상을 도출하기 위해 3개의 폴드 세트를 KFold로 생성하고, 각 교차 검증 시마다 생성되는 학습/검증 레이블 데이터 값의 분포도를 확인해 보겠습니다.
```py
kfold = KFold(n_splits=3)
# kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환.
n_iter = 0
for train_index, test_index in kfold.split(iris_df):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    print('\n## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n{0}'.format(label_train.value_counts()))
    print('검증 레이블 데이터 분포:\n{0}'.format(label_test.value_counts()))
```
```
[output]
## 교차 검증: 1
학습 레이블 데이터 분포:
1    50
2    50
Name: label, dtype: int64
검증 레이블 데이터 분포:
0    50
Name: label, dtype: int64

## 교차 검증: 2
학습 레이블 데이터 분포:
0    50
2    50
Name: label, dtype: int64
검증 레이블 데이터 분포:
1    50
Name: label, dtype: int64

## 교차 검증: 3
학습 레이블 데이터 분포:
0    50
1    50
Name: label, dtype: int64
검증 레이블 데이터 분포:
2    50
Name: label, dtype: int64
```

<br>

교차 검증 시마다 3개의 폴드 세트로 만들어지는 학습 레이블과 검증 레이블이 완전히 다른 값으로 추출되었습니다. 예를 들어 첫 번째 교차 검증에서는 학습 레이블의 1, 2 값이 각각 50개가 추출되었고, 검증 레이블의 0값이 50개 추출되었습니다. 학습 레이블은 1, 2밖에 없으므로 0의 경우는 전혀 학습하지 못합니다. 반대로 검증 레이블은 0밖에 없으므로 학습 모델은 절대 0을 예측하지 못합니다. **이런 유형으로 교차 검증 데이터 세트를 분할하면 검증 예측 정확도는 0이 될 수밖에 없습니다.**


**StratifiedKFold** 는 이렇게 **KFold로 분할된 레이블 데이터 세트가 전체 레이블 값의 분포도를 반영하지 못하는 문제를 해결해 줍니다.** 이번에는 동일한 데이터 분할을 **StratifiedKFold** 로 수행하고 학습/검증 레이블 데이터의 분포도를 확인해 보겠습니다.StratifiedKFold는 레이블 데이터 분포도에 따라 학습/검증 데이터를 나누기 때문에 split() 메서드에 인자로 피처 데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 필요하
```py
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3)
n_iter = 0

# StratifiedKFold는 레이블 데이터 분포도에 따라 학습/검증 데이터를 나누기 때문에,
# split() 메서드에 인자로 피처 데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 필요.
for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    print('\n## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n{0}'.format(label_train.value_counts()))
    print('검증 레이블 데이터 분포:\n{0}'.format(label_test.value_counts()))
```
```
## 교차 검증: 1
학습 레이블 데이터 분포:
2    34
0    33
1    33
Name: label, dtype: int64
검증 레이블 데이터 분포:
0    17
1    17
2    16
Name: label, dtype: int64

## 교차 검증: 2
학습 레이블 데이터 분포:
1    34
0    33
2    33
Name: label, dtype: int64
검증 레이블 데이터 분포:
0    17
2    17
1    16
Name: label, dtype: int64

## 교차 검증: 3
학습 레이블 데이터 분포:
0    34
1    33
2    33
Name: label, dtype: int64
검증 레이블 데이터 분포:
1    17
2    17
0    16
Name: label, dtype: int64
```

<br>



출력 결과를 보면 학습 레이블과 검증 레이블 데이터 값의 분포도가 거의 동일하게 할당됐음을 알 수 있습니다. **이렇게 분할이 되어야 레이블 값 0, 1, 2 를 모두 학습할 수 있고, 이에 기반해 검증을 수행할 수 있습니다.** StratifiedKFold를 이용해 붓꽃 데이터를 교차 검증해 보겠습니다.
```py
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold
import numpy as np

iris = load_iris()
features = iris.data
label = iris.target

dt_clf = DecisionTreeClassifier(random_state=156)
skfold = StratifiedKFold(n_splits=3)
n_iter=0
cv_accuracy=[]

# StratifiedKFold의 split() 호출시 반드시 레이블 데이터 셋도 추가 입력 필요  
for train_index, test_index in skfold.split(features, label):
    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    
    #학습 및 예측 
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)

    # 반복 시 마다 정확도 측정 
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test, pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    cv_accuracy.append(accuracy)
    print('#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}\n'.format(n_iter, test_index))
    
# 교차 검증별 정확도 및 평균 정확도 계산 
print('### 교차 검증별 정확도:', np.round(cv_accuracy, 4))
print('### 평균 검증 정확도:', np.round(np.mean(cv_accuracy), 4))
```
```
[output]
#1 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50
#1 검증 세트 인덱스:[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50
  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101
 102 103 104 105 106 107 108 109 110 111 112 113 114 115]

#2 교차 검증 정확도 :0.94, 학습 데이터 크기: 100, 검증 데이터 크기: 50
#2 검증 세트 인덱스:[ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67
  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118
 119 120 121 122 123 124 125 126 127 128 129 130 131 132]

#3 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50
#3 검증 세트 인덱스:[ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84
  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149]

### 교차 검증별 정확도: [0.98 0.94 0.98]
### 평균 검증 정확도: 0.9667
```

<br>


Stratified K 폴드의 경우 원본 데이터의 레이블 분포도 특성을 반영한 학습 및 검증 데이터 세트를 만들 수 있으므로 **<span style="color:red">왜곡된 레이블 데이터 세트에서는 반드시 Stratified K 폴드를 이용해 교차 검증해야 합니다.</span>**

일반적으로 **분류(Classification)** 에서의 교차 검증은 Stratified K 폴드로 분할돼야 합니다. **회귀(Regression)** 에서는 Stratified K 폴드가 지원되지 않습니다. 이유는 간단합니다. **<u>회귀의 결정값은 이산값 형태의 레이블이 아니라 연속된 숫자값이기 때문에 결정값별로 분포를 정하는 의미가 없기 때문</u>** 입니다.

<br>



## cross_val_score() - 교차 검증을 보다 간편하게

사이킷런은 교차 검증을 좀 더 편리하게 수행할 수 있게 해주는 API를 제공합니다. 대표적으로 **<span style="color:red">cross_val_score()</span>** 입니다. **classifier** 가 입력되면 **Stratified K 폴드** 방식으로 레이블값의 분포에 따라 학습/테스트 세트를 분할하며(회귀는 K 폴드 방식으로 분할), **수행 후 반환 값은 scoring 파라미터로 지정된 성능 지표 측정값을 배열 형태로 반환** 합니다.

* **cross_val_score()**
    * **estimator**: 알고리즘 클래스
    * **X**: 피처 데이터 세트
    * **y**: 레이블 데이터 세트
    * **scoring**: 예측 성능 평가 지표
    * **cv**: 교차 검증 폴드 수

교차 검증 폴드 수는 3, 성능 평가 지표는 정확도인 accuracy로 하겠습니다.
```py
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score , cross_validate
from sklearn.datasets import load_iris

iris_data = load_iris()
dt_clf = DecisionTreeClassifier(random_state=156)

data = iris_data.data
label = iris_data.target

# 성능 지표는 정확도(accuracy) , 교차 검증 세트는 3개 
scores = cross_val_score(
    dt_clf,
    data,
    label,
    scoring='accuracy',
    cv=3
)

print('교차 검증별 정확도:', np.round(scores, 4))
print('평균 검증 정확도:', np.round(np.mean(scores), 4))
```
```
[output]
교차 검증별 정확도: [0.98 0.94 0.98]
평균 검증 정확도: 0.9667
```

**cross_val_score()** 는 cv로 지정된 횟수만큼 **scoring** 파라미터로 **<u>지정된 평가 지표로 평가 결값을 배열로 반환</u>** 합니다. 해당 API는 내부에서 **Estimator** 를 학습(fit), 예측(predict), 평가(evaluation)시켜주므로 간단하게 교차검증을 수행할 수 있습니다.

또한 내부적으로 StratifiedKFold를 이용하기 때문에 StratifiedKFold의 수행 결과와 비교했을때 각 교차 검증별 정확도와 평균 검증 정확도가 모두 동일함을 알 수 있습니다.

<br>

비슷한 API로 **cross_validate()** 가 있습니다. **cross_val_score()** 는 단 하나의 평가 지표만 가능하지만 **cross_validate()**는 **<u>여러 개의 평가 지표를 반환할 수 있으며, 학습 데이터에 대한 성능 평가 지표와 수행 시간도 같이 제공</u>** 합니다.

<br>



# GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에

**<span style="color:red">하이퍼 파라미터</span>** 는 **머신러닝 알고리즘을 구성하는 주요 구성 요소이며, 이 값을 조정해 알고리즘의 예측성능을 개선할 수 있습니다.** 사이킷런은 **<span style="color:red">GridSearchCV API</span>** 를 이용해 **<u>하이퍼 파라미터를 순차적으로 입력하면서, 교차 검증을 기반으로 편리하게 최적의 하이퍼 파라미터의 최적 값을 찾게 해줍니다.</u>**

예를 들어 결정 트리 알고리즘의 여러 하이퍼 파라미터를 순차적으로 변경하면서 최고 성능을 가지는 파라미터 조합을 찾고자 한다면 다음과 같이 파라미터의 집합을 만들고 이를 순차적으로 적용하면서 최적화를 수행할 수 있습니다.
```
grid_parameters = {
    'max_depth': [1, 2, 3],
    'min_samples_split': [2, 3]
}
```

<br>


하이퍼 파라미터는 다음과 같이 순차적으로 적용되며, 총 6회에 걸쳐 파라미터를 순차적으로 바꿔 실행하면서 최적의 파라미터와 수행 결과를 도출할 수 있습니다.

| 순번 | max_depth | min_samples_split |
| --- | --------- | ----------------- |
| 1 | 1 | 2 |
| 2 | 1 | 3 |
| 3 | 2 | 2 |
| 4 | 2 | 3 |
| 5 | 3 | 2 |
| 6 | 3 | 3 |

<br>

**GridSearchCV** 는 **데이터 세트를 cross-validation을 위한 학습/테스트 세트로 자동으로 분할한 뒤에 하이퍼 파라미터 그리드에 기술된 모든 파라미터를 순차적으로 적용해 최적의 파라미터를 찾을 수 있게 해줍니다.**

위의 경우 순차적으로 6회에 걸쳐 하이퍼 파라미터를 변경하면서 교차 검증 데이터 세트에 수행 성능을 측정합니다. CV가 3회라면 개별 파라미터 조합마다 3개의 폴딩 세트를 3회에 걸쳐 학습/평가해 평균값으로 성능을 측정합니다. 6개의 파라미터 조합이라면 총 CV 3회 X 6개 파라미터 조합 = 18회의 학습/평가가 이뤄집니다.

<br>

**GridSearchCV** 는 사용자가 튜닝하고자 하는 여러 종류의 하이퍼 파라미터를 다양하게 테스트하면서 최적의 파라미터를 편리하게 찾게 해주지만 동시에 순차적으로 파라미터를 테스트하므로 수행시간이 상대적으로 오래 걸리는 것에 유념해야 합니다.

* **GridSearchCV()**
    * **estimator**: classifier, regressor, pipeline이 사용될 수 있음.
    * **param_grid**: key+리스트 값을 가지는 딕셔너리를 입력. estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값을 지정.
    * **scoring**: 예측 성능을 측정할 평가 방법을 지정.
    * **cv**: 교차 검증을 위해 분할되는 학습/테스트 세트의 개수를 지정.
    * **refit**: True로 생성 시 가장 최적의 하이퍼 파라미터를 찾은 뒤, 입력된 estimator 객체를 해당 하이퍼파라미터로 재학습.

<br>


결정 트리 알고리즘의 여러 가지 최적화 파라미터를 순차적으로 적용해 붓꽃 데이터를 예측 분석을 위해 GridSearchCV를 이용하겠습니다. **테스트할 하이퍼 파라미터 세트는 딕셔너리 형태** 로 **하이퍼 파라미터의 명칭은 문자열 Key 값** 으로, **하이퍼 파라미터의 값은 리스트** 로 설정합니다.
```py
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# 데이터를 로딩하고 학습데이타와 테스트 데이터 분리
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris_data.data,
    iris_data.target,
    test_size=0.2,
    random_state=121
)
dtree = DecisionTreeClassifier()

# parameter 들을 dictionary 형태로 설정
parameters = {
    'max_depth': [1, 2, 3],
    'min_samples_split': [2, 3]
}
```

<br>



학습 데이터 세트를 **GridSearchCV** 객체의 **fit(학습 데이터 세트)** 메서드에 인자로 입력하고 **fit(학습 데이터 세트)** 메서드를 수행하면, 학습 데이터를 **cv** 에 기술된 폴딩 세트로 분할해 **param_grid** 에 기술된 하이퍼 파라미터를 순차적으로 변경하면서 학습/평가를 수행하고 그 결과를 **<span style="color:red">cv_results_</span>** 속성에 기록합니다.

**cv_results_** 는 **gridsearchcv** 의 결과 세트로서 딕셔너리 형태로 key 값과 리스트 형태의 value 값을 가집니다. **cv_results_** 를 DataFrame으로 변환하면 내용을 좀 더 쉽게 볼 수 있으며, 이 중 주요 칼럼만 발췌해서 어떻게 **GridSearchCV** 가 동작하는지 좀 더 자세히 알아보겠습니다.
```py
import pandas as pd

# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  
# refit=True가 default임. True이면 가장 좋은 파라미터 설정으로 재 학습시킴.  
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가.
grid_dtree.fit(X_train, y_train)

# GridSearchCV 결과 추출하여 DataFrame으로 변환
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']]
```

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/235097993-fee47aa6-9fc3-4e68-937c-f3558318b8b4.png">
</p>

<br>



위의 결과에서 총 6개의 결과를 볼 수 있으며, 이는 하이퍼 파라미터를 순차적으로 총 6번 변경하면서 학습 및 평가를 수행했음을 나타냅니다.

* **params**: 수행할 때마다 적용된 개별 하이퍼 파라미터값
* **rank_test_socre**: 하이퍼 파라미터별로 성능이 좋은 score 순위. 1이 가장 뛰어난 순위이며 이때의 파라미터가 최적의 하이퍼 파라미터.
* **mean_test_score**: 개별 하이퍼 파라미터별로 CV의 폴딩 테스트 세트에 대해 총 수행한 평가 평균값.

<br>

**GridSearchCV** 객체의 **fit()** 을 수행하면 최고 성능을 나타낸 하이퍼 파라미터의 값과 그때의 평가 결과 값이 각각 **<span style="color:red">best_params_</span>**, **<span style="color:red">best_score_</span>** 속성에 기록됩니다. 이 속성을 이용해 최적 하이퍼 파라미터의 값과 그때의 정확도를 알아보겠습니다.
```py
print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))
```
```
[output]
GridSearchCV 최적 파라미터: {'max_depth': 3, 'min_samples_split': 2}
GridSearchCV 최고 정확도: 0.9750
```

<br>


**max_depth** 가 3, **min_samples_split** 가 2일 때 검증용 폴드 세트에서 평균 최고 정확도가 97.50%로 측정됐습니다. GridSearchCV 객체의 생성 파라미터로 **refit=True** 가 디폴트입니다. **refit=True** 이면 **GridSearchCV** 가 최적 성능을 나타내는 하이퍼 파라미터로 **Estimator** 를 학습해 **<span style="color:red">best_estimator_</span>** 로 저장합니다.

이미 학습된 **best_estimator_** 를 이용해 앞에서 **train_test_split()** 으로 분리한 테스트 데이터 세트에 대해 예측하고 성능을 평가해 보겠습니다.
```py
# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환
estimator = grid_dtree.best_estimator_

# GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨
pred = estimator.predict(X_test)
print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))
```
```
[output]
테스트 데이터 세트 정확도: 0.9667
```

<br>

일반적으로 학습 데이터를 **GridSearchCV** 를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신러닝 모델 적용 방법입니다.




