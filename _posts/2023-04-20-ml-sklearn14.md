---
layout: post
title: 데이터 전처리(Data Preprocessing)
category: Machine Learning
tag: Machine-Learning
---

 





















# 단순 선형 회귀



**<span style="color:red">단순 선형 회귀</span>** 는 **<span style="background-color: #fff5b1">독립변수도 하나</span>**, **<span style="background-color: #fff5b1">종속변수도 하나</span>** 인 선형 회귀입니다. 예를 들어, 주택 가격($\hat{Y}$)이 주택의 크기($X$)로만 결정된다고 할 때, 일반적으로 주택의 크기가 크면 가격이 높아지는 경향이 있기 때문에 주택 가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현할 수 있습니다. 즉, **<u>특정 기울기와 절편을 가진 1차 함수식으로 모델링(독립변수가 1개인 단순 선형 회귀 모델)</u>** 할 수 있습니다.(**<span style="background-color: #fff5b1">$\hat{Y}=w_0+w_1*X$</span>**)


<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/fb44aa9b-f02f-4a58-81c4-fa38c14f7853">
</p>

<br>



**실제 값($Y$)은** 예측 값($\hat{Y}$)에서 실제 값만큼의 오류 값을 뺀(또는 더한) 값이 됩니다.(**<span style="background-color: #fff5b1">$\hat{Y}=w_0+w_1*X+오류값$</span>**) 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, 즉 **<span style="color:red">잔차(오류 값, Error)</span>** 라고 지칭합니다.


**<span style="color:red">최적의 회귀모델을 만든다는 것</span>** 은 바로 **<span style="background-color: #fff5b1">전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델</span>** 을 만든다는 의미입니다. 동시에 **<span style="background-color: #fff5b1">오류 값 합이 최소가 될 수 있는 최적의 회귀 계수</span>** 를 찾는다는 의미도 됩니다.


<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/05e2fea8-41ad-41ab-afe5-f19abb6930ab">
</p>

<br>



오류 값은 $+$ 나 $-$ 가 될 수 있기에, **<u>전체 데이터의 오류 합을 구하기 위해 단순히 더했다가는 뜻하지 않게 오류의 합이 크게 줄어들 수 있습니다.</u>** 따라서 보통 오류 합을 계산할 때는 오류 값의 제곱을 구해서 더하는 방식(**<span style="color:red">$RSS$</span>**, Residual Sum of Square)을 취합니다. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 $RSS$ 방식으로 오류 합을 구합니다. 즉, **<span style="background-color: #fff5b1">$Error^2 = RSS$</span>**


<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/e89c146b-2aa2-4b66-9504-cdcb4a3bbe2c">
</p>

<br>





$RSS$ 는 이제 변수가 $w_0, w_1$ 인 식으로 표현할 수 있으며, 이 **<span style="color:red">$RSS$ 를 최소로 하는 $w_0, w_1$, 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항</span>** 입니다. $RSS$ 는 회귀식의 독립변수 $X$, 종속변수 $Y$ 가 중심 변수가 아니라, **<span style="background-color: #fff5b1">$w$ 변수(회귀 계수)가 중심 변수임을 인지하는 것이 매우 중요</span>** 합니다(학습 데이터로 입력되는 독립변수와 종속변수는 $RSS$ 에서 모두 상수로 간주). 일반적으로 $RSS$ 는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.

<br>

$$
RSS(w_0, W_1) = \frac{1}{N}	\sum_{k=1}^N (y_i - (w_0 + w_1 * x_i))^2
$$

<br>

회귀에서 이 **<span style="color:red">$RSS$</span>** 는 **<span style="background-color: #fff5b1">비용(Cost)</span>** 이며 $w$ 변수(회귀 계수)로 구성되는 $RSS$ 를 **<span style="background-color: #fff5b1">비용함수</span>** 또는 **<span style="background-color: #fff5b1">손실함수(loss function)</span>** 라고 합니다.

**<span style="color:red">머신러닝 회귀 알고리즘</span>** 은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류 값)을 지속해서 감소시키고, **<span style="background-color: #fff5b1">최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것</span>** 입니다.






