---
layout: post
title: CS231n Lecture2 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html)을 바탕으로 작성되었습니다.





<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/181658691-de8b4117-b1cf-4324-89b5-7780e4f3ccb4.png">
</p>





# Image Classification

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/181661581-27c16863-6d43-4ab3-b990-f004be2a9126.png">
</p>

* Image Classification은 컴퓨터비전 분야에서 Core Task 에 속합니다.
* Image Classification을 한다고 할때, 미리 정해놓은 카테고리 집합(discrete labels)이 있는 시스템에 이미지를 입력하여, 컴퓨터가 이미지를 보고 어떤 카테고리에 속하는지 고르는 것입니다.
* 하지만 사람의 시각체계는 Visual Recognition task에 고도화 되어 있기 때문에 쉬워보이지만, 기계의 입장에서는 어려운 일 입니다.



## The Problem: Semantic Gap

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182022436-6e84c5c6-7d5b-483a-9bac-de28d236ac73.png">
</p>

* Semantic Gap(의미론적 차이) : 사람 눈으로 보는 이미지, 실제 컴퓨터가 보는 픽셀 값과의 차이
* 위 그림은 컴퓨터가 이미지를 바라보는 관점
* 이미지는 0부터 255까지의 숫자로 픽셀이 표현되며, Width(너비) x Height(높이) x Channel(채널)의 크기의 3차원 배열. 각 채널은 red, green, blue를 의미
* 우리가 보기에는 고양이 이미지이지만, 컴퓨터에게 이미지는 그저 아주 큰 격자 모양의 숫자 집합

위 사진과 같이 기계는 고양이 사진을 입력받으면, RGB(Red, Blue, Green) 값을 기준으로 격자 모양의 숫자들을 나열하여 인식합니다. 하지만 기계는 카메라 각도나 밝기, 객채의 행동 혹은 가려짐 등 여러차이로 인해 이미지의 픽셀 값이 달리 읽어 사물을 다르게 인식하는데, 이러한 기계를 잘 인식할 수있도록 알고리즘 개발을 시도 했으나, 다양한 객체들에게 유연하고 확장성 있는 알고리즘을 개발하는데 한계가 있었습니다.



## Viewpoint variation(카메라의 위치 변화)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023890-2e54b6a5-249c-4551-8e35-848ac4eefc42.png" style="zoom:80%;">
</p>

* 가령 이미지에 아주 미묘한 변화만 주더라도 픽셀 값들은 모조리 변하게 될 것.
* 고양이 이미지를 예로 들었을때, 고양이 한 마리가 얌전히 앉아만 있으며 아무 일도 일어나지 않겠지만, 카메라를 아주 조금만 옆으로 옮겨도 모든 픽셀 값들이 모조리 달라질 것입니다. 하지만 픽셀 값이 달라진다해도 고양이라는 사실은 변하지 않기 때문에 Classification 알고리즘은 robust해야 합니다.



## Illumination(조명에 의한 변화)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023326-9ffd08c3-802a-457c-9a46-73b290c7ee3a.png" style="zoom:80%;">
</p>

* 바라보는 방향 뿐만 아니라 조명 또한 문제가 될 수 있습니다. 어떤 장면이냐에 따라 조명은 각양각생일 것입니다.
* 고양이가 어두운 곳에 있던 밝은 곳에 있던 고양이는 고양이 이므로, 알고리즘은 robust해야 합니다.



## Deformation(객체 변형에 의한 변화)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023335-d103a595-9f35-48a3-aa16-413cba93fe0e.png" style="zoom:80%;">
</p>

* 객체 자체에 변형이 있을 수 있습니다.
* 고양이는 다양한 자세를 취할 수 있는 동물 중 하나인데, deformation에 대해서도 알고리즘은 robust해야 합니다.



## Occlusion(객체 가려짐에 의한 변화)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023353-384225c0-7726-4c25-8b72-67b23e8ec202.png" style="zoom:80%;">
</p>

* 가려짐(occlusion)도 문제가 될 수 있습니다. 가령 고양이의 일부밖에 볼 수 없는 상황이 있을 수도 있습니다.
* 고양이의 얼굴밖에 볼 수 없다던가, 극단적인 경우에는 소파에 숨어들어간 고양이의 꼬리밖에 볼 수 없을지도 모르지만, 사람이라면 고양이라는 사실을 단번에 알아챌 수 있습니다.
* 즉 이 이미지는 고양이 이미지라는 것을 알 수 있기 때문에, 알고리즘은 robust해야 합니다.



## Background Clutter(배경과 유사한 색의 객체)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023367-00065595-9459-43e3-bc10-56b619a9d268.png" style="zoom:80%;">
</p>

* Background clutter(배경과 비슷한 경우)라는 문제도 존재.
* 고양이가 배경과 거의 비슷하게 생겼을 수도 있기때문에, 알고리즘은 robust해야 합니다.



## IntraClass variation(한 클래스에 여러 종류)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023377-577d42d6-3b4b-476c-88af-21f622373443.png" style="zoom:80%;">
</p>

* 하나의 클래스 내에도 다양성이 존재. 즉 "고양이"라는 하나의 개념으로 모든 고양이의 다양한 모습들을 전부 소화해 내야 합니다.
* 고양이에 따라 생김새, 크기, 색, 나이가 각양 각색일 것.

위와 같은 이유들 때문에 Image Classification 문제는 어렵습니다. 사물 인식의 경우, 가령 고양이를 인식해야 하는 상황이라면 객체를 인식하는 직관적이고 명시적인 알고리즘은 존재하지 않습니다.





# Image Classification Algorithm - 기존의 시도들

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182024327-c36554c9-c690-4812-87c5-29790e8a235c.png">
</p>

* 기존에는 Edges and corners를 찾는, 즉 Image의 Feature(특징)을 찾고 Feature(특징)을 이용하여 명시적인 규칙을 만드는 방법으로 접근하였다.
* 하지만 아래의 이유로 잘 동작하지 않았다.
    * 앞에서 살펴본 조건들에서 여전히 robust하지 못하다.
    * 특정 class에 동작하도록 구현된 알고리즘은 다른 class에 적용하지 못한다.

즉 알고리즘이 robust하지 못 할 뿐더러, 각 class에 대해 새로 다시 짜야하므로, 확장성이 전혀 없는 방법입니다. 따라서 이러한 문제를 해결하기위해, 이 세상에 존재하는 다양한 객체들에 대해 적용이 가능한 방식이 필요합니다.





# Image Classification Algorithm - Data-Driven Approach
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182024392-e8108c72-528c-42fa-aab5-9dc759a2d330.png">
</p>

* 기존에는 이미지를 인식시킬 때, 각 객체에 대한 규칙을 하나하나 정하였다. 그러나 실제 생활에서는 수많은 객체들이 존재하기에 객체마다 규칙을 정해주는 것은 한계를 가지기 때문에, 다양한 객체들에 대해 적용하기 위한 Insight로 데이터 중심 접근방법(Data-Driven Approcach)을 사용합니다.
* Data-Driven Approcach은 객체의 특징을 규정하지 않고, 다양한 사진들과 label을 수집하고, 이를 이용해 Machine Learning Clssifier 모델을 학습하고, 새로운 이미지를 테스트해 이미지를 새롭게 분류하는 방식입니다.

Data-Driven Approcach은 Machine Learning의 key insight이며, Deep Learning 뿐만 아니라 아주 일반적인 개념입니다.






# Example Dataset: CIFAR10

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182025355-48224f09-1a41-4188-abc4-d23794ed31e0.png">
</p>

* Cifar-10은 Machine Learning에서 자주 쓰는 연습용(테스트용) 데이터셋.
* CIFAR-10에는 10가지 클래스가 존재(비행기, 자동차, 새, 고양이 등)
* 총 50,000여개의 학습용 이미지와, 10,000여개의 테스트 이미지가 존재.
* 32 x 32 이미지

CIFAR-10 데이터셋을 이용해서 Nearest Neighbor(NN) 예제를 살펴보겠습니다. 우선 오른쪽 칸의 맨 왼쪽 열은 CIFAR-10 테스트 이미지이며, 오른쪽 방향으로는 학습 이미지 중 테스트 이미지와 유사한 순으로 정렬했습니다. 테스트 이미지와 학습 이미지를 비교해 보면, 눈으로 보기에는 상당히 비슷해 보입니다.

두 번째 행의 이미지는 "개" 이며, 가장 가까운 이미지(1등)도 "개" 입니다. 하지만 2등, 3등을 살펴보면 "사슴"이나 "말"같아 보이는 이미지들도 있습니다. "개"는 아니지만 눈으로 보기에는 아주 비슷해 보입니다.

가장 간단하고 기본적인 분류방법인 Nearest Neighbor(NN)는 "가장 가까운 이웃 찾기" 알고리즘으로, 직관적인데 새로운 이미지와 이미 알고 있던 이미지를 비교하여 가장 비슷하게 생긴 것을 찾아내는 알고리즘을 말합니다. NN 알고리즘이 잘 동작하지 않을 것 같아 보이지만 그럼에도 해 볼만한 아주 좋은 예제입니다.





# Nearest Neighbor(NN)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182025828-626fb75d-0091-4abf-b136-cbdd65c09757.png">
</p>

* Nearest Neighbor(NN) : 입력받은 데이터를 저장한 다음 새로운 입력 데이터가 들어오면, 기존 데이터에서 비교하여 가장 유사한 이미지 데이터의 라벨을 예측하는 알고리즘입니다.

즉, 최근접 이웃 분류기는 테스트 이미지를 위해 모든 학습 이미지와 비교를 하고 라벨 값을 예상합니다.

위 코드를 보면 다음과 같이 동작합니다.
* Train함수 - Train Step에서는 단지 모든 학습 데이터를 기억합니다. (입력은 이미지와 레이블이고, 출력은 우리의 모델)
* Predict 함수 - Pridict Step에서는 새로운 이미지가 들어오면 새로운 이미지와 기존의 학습 데이터를 비교해서 가장 유사한 이미지로 레이블링을 예측합니다. (입력이 모델이고, 출력은 이미지의 예측값)



## Distance Metric to compare images

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182026350-75133d32-ee4d-4f09-b5df-f06faceec22f.png">
</p>

* 여기에서 중요한 점은 이미지 쌍이 있을 때 얼마나 유사한지를 어떻게 비교를 할 것인지가 관건입니다.
* 테스트 이미지 하나를 모든 학습 이미지들과 비교할 때 여러가지 비교 방법들이 있습니다.
* 위 그림에서 $I_1$, $I_2$ 벡터로 나타냈을 때, 벡터 간의 L1 Distance(Manhattan distance)를 사용하여 계산.
* 결과는 모든 픽셀값 차이의 합

이미지를 Pixel-wise로 비교합니다. 가령 4x4 테스트 이미지가 있다고 가정할 때, training/test image의 같은 자리의 픽셀을 서로 빼고 절댓값을 취합니다. 이렇게 픽셀 간의 차이 값을 계산하고 모든 픽셀의 수행 결과를 모두 더합니다.

"두 이미지간의 차이를 어떻게 측정 할 것인가?"에 대해 구체적인 방법을 제시합니다. 지금 예제의 경우에는 두 이미지간에 "456" 만큼 차이가 납니다.



## NN Classifier - python code
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182026636-55c3485a-d058-4cf4-bc62-a924a76ba0f3.png">
</p>

* Train 함수
    * NN의 경우 Train 함수는 단지 학습 데이터를 기억하는 것입니다.
    * N개의 이미지가 있는 X와 N개의 라벨이 있는 y, 훈련데이터를 Xtr과 ytr에 모두 저장합니다.
* Test 함수
    * 이미지를 입력으로 받고 L1 Distance로 비교합니다. 즉 학습 데이터들 중 테스트 이미지와 가장 유사한 이미지들을 찾아냅니다.
    * 모든 훈련데이터의 이미지가 저장된 Xtr과 비교하고자하는 X의 행을 Xtr과 dimension을 맞춰준다음 빼고 abs를 취해줍니다.
    * axis=1의 의미는 y축을 기준으로 더한 값들로 저장된 배열을 distances에 저장해줍니다.
    * 그중 제일 작은 값을 argmin으로 찾아내서 그 위치를 min_index에 저장하고
    * ytr에 그 인덱스값을 넣어 예측값으로 보냅니다.


여기서 Simple Classifier인 NN알고리즘에 대해 생각할 점이 있습니다.

1. Trainset의 이미지가 총 N개라면, Train/Test 함수의 Train time은 데이터를 기억만 하면 되기 때문에 상수시간 O(1)입니다.
2. 하지만 Test time에서는 N개의 학습 데이터 전부를 테스트 이미지와 비교해야만 합니다. 즉 (Train time < Test time) 데이터 학습은 빠르지만, 새로운 데이터를 판단하는데 있어서 걸리는 시간이 많이 필요하므로, Test시 Test time은 Test data 가 많아지면 Test 시간이 늘어납니다.

실제로 Train Time은 조금 느려도 되지만 Test Time에서는 빠르게 동작하길 원합니다. Classifier의 좋은 성능을 보장하기 위해서 Train Time에 많은 시간을 쏟을 수도 있기 때문입니다. 하지만 NN Classifier의 "Test Time" 을 생각해보면, 일반적으로 모델들은 핸드폰, 브라우저 등 Low Power Device에서 동작해야 되기 때문에 test time이 빨라야 합니다. 하지만 Nearest Neighbor 모델은 느립니다.



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182027154-de192791-72c7-4f2d-91be-f4289fcbd6af.png" style="zoom:80%;">
</p>

* NN 알고리즘으로 decision regions를 그려본 이미지입니다.
* 2차원 평면 상의 각 점은 학습데이터, 점의 색은 클래스 라벨(카테고리)입니다.
* 2차원 평면 내의 모든 좌표에서 각 좌표가 어떤 학습 데이터와 가장 가까운지 계산하고, 각 좌표를 해당 클래스로 칠했습니다.

NN 알고리즘은 "가장 가까운 이웃" 만을 보기 때문에, 녹색 한 가운데 노란색 영역, 초록색 영역에서 파란색 영역 침범하는 구간 등등 decision boundary가 Robust 하지 않음을 볼 수 있습니다. 해당 점들은 잡음(noise)이거나 가짜(spurious)일 가능성이 높습니다. 이러한 단점들로 NN 알고리즘은 잘 사용하지 않습니다.





# K-Nearest Neighbors(K-NN)

* NN의 일반화된 방법인 K-NN은 K개의 가장 가까운 지점의 데이터들의 Majority vote를 통해 예측하는 모델입니다.
* 단순하게 가장 가까운 이웃만 찾기보다는 Distance metric을 이용해서 가까운 이웃을 K개의 만큼 찾고, 이웃끼리 투표를 하는 방법입니다. 그리고 가장 많은 특표수를 획득한 레이블로 예측합니다.
* K-Nearest Neighbor Algorithm 을 사용할때, 결정해야하는 두 가지 parameter가 있습니다.
    * K값
    * Distance Metric(거리척도)



## K-Nearest Neighbors(K-NN) - K

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182027762-fb620d6c-454f-4982-88f1-811821328d8c.png">
</p>

위 그림은 동일한 데이터를 사용한 k-nn 분류기들로, 각각 K=1/3/5 에서의 결과입니다.
* K=3 의 경우, 앞서 초록색 영역에 자리 잡았던 노란색 점 때문에 생긴 노란 지역이 깔끔하게 사라졌습니다. 중앙은 초록색이 깔끔하게 점령했습니다. 그리고 왼쪽의 빨강/파랑 사이의 뾰족한 경계들도 다수결에 의해 점차 부드러워지고 있습니다.
* K=5의 경우, 파란/빨간 영역의 경계가 이제는 아주 부드럽고 좋아졌습니다.
* 흰색 영역은 K-NN이 "대다수"를 결정할 수 없는 지역으로, 어떤 식으로든 추론을 해보거나, 임의로 정할 수도 있습니다.
* KNN은 위 슬라이드와 같이, K가 커질수록 decision boundary가 더 smooth해지는 경향이 있습니다.


이러한 방식을 이용하면 좀 더 일반화 된 결정 경계를 찾을 수 있습니다. 여기서 K값의 증가함에 따라서 부드러워 지지만, 흰색 영역이 증가 하는 것을 볼 수 있습니다. 이 흰색 영역 은 어느 쪽에도 분류 할지 알 수 없는 영역입니다. 이러한 부분에서 K값이 증가한다고 항상 좋은 것이 아니라. 데이터나 상황에 따라서 알맞은 K값을 찾아야합니다.



## K-Nearest Neighbors(K-NN) - Distance Metric

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182028810-f0bbc1a0-3a1a-4624-8bdf-8df3bc358607.png">
</p>

* K-NN을 사용할 때 결정해야 할 한 가지 사항으로, 서로 다른 점들을 어떻게 비교할 것인지 입니다. 즉 데이터 간의 거리를 잴 때 사용하는 기준으로, 학습한 이미지 중 어떤 이미지와 비슷한지 비교하는 척도라고 생각하면 됩니다.
* L1 Manhattan Distance: "픽셀 간 차이 절대값의 합"
* L2 Euclidean distance: "제곱 합의 제곱근"을 거리로 이용
* 어떤 거리 척도(distance metric)를 선택할지는 아주 흥미로운 주제입니다.
* 왜냐하면 서로 다른 척도에서는 해당 공간의 근본적인 기하학적 구조 자체가 서로 다르기 때문입니다.

일반적으로 K-NN에서의 Distance Metric은 L1 distance 또는 L2 distance를 사용할 수 있습니다.
* L1, L2 distance가 원점으로부터 1인 경우를 나타낸 것이라 가정했을때
    * L1 distance
        * 왼쪽에 보이는 사각형은 L1 Distance의 관점에서는 원
        * 학습된 이미지와 테스트 이미지의 픽셀 간의 차이 값을 계산하고 모두 더 하는 방식
        * 같은 거리지만 좌표축 방향에서 가장 크게 뻗어나가는 형태를 보인다.
        * 좌표축에 따라 거리가 달라지기 때문에 특정 벡터가 개별적인 의미(ex. 키, 몸무게)를 가지고 있을때 사용합니다.
        * 따라서, L1을 사용하는 경우에는 특정 feature의 영향이 강하게 적용될 수 있다는 것으로 이해할 수 있다.
    * L2 distance
        * 오른쪽에 보이는 원은 L2 Distance의 관점에서는 원
        습된 이미지와 테스트 이미지의 픽셀 간의 차이의 제곱 합의 제곱근을 거리로 이용하는 방식
        * 같은 거리를 가지는 경우에는 모든 방향으로 균일하게 뻗어나가는 형태를 보인다.
        * 특징 벡터가 일반적인 벡터이고, 요소들간의 실질적인 의미를 잘 모르는 경우에 사용합니다.
        * 따라서, L2를 사용하는 경우에는 모든 feature의 영향이 골고루 적용된다는 것으로 이해할 수 있다.



K-NN는 거리척도에 따라 다양한 문제를 해결할 수 있는데, 벡터나 이미지 외에 문장도 분류가 가능합니다. 즉 거리 척도만 정해주면 어떤 종류의 데이터도 다룰 수 있습니다. 어떤 거리 척도를 사용하는지에 따라서 실제 기하학적으로 어떻게 변하는지 살펴보겠습니다.


<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182048419-c3b08bc8-b745-46f6-82ac-c50ca8f618b3.png">
</p>

* 양 쪽 모두 동일한 데이터로, 왼쪽은 L1 Distance를 오른쪽은 L2 Distance를 사용했습니다.
* 결과를 보면 거리 척도에 따라서 결정 경계의 모양 자체가 달라짐을 알 수 있습니다.
* 두 경우를 비교해보면, L2 distance에서 decision boundary가 더 smooth해지는 경향이 있습니다.
    * L1은 좌표 시스템의 영향을 받기 때문에 결정 경계가 좌표축 방향으로의 영향을 더 크게 받지만, L2는 모든 방향으로의 영향을 골고루 받기 때문에 조금 더 자연스럽습니다.



## Hyperparameters

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182048758-43096492-66ea-46a6-b3db-fc0ca0f652f6.png">
</p>

* Hyperparameter는 (학습 이전에) 학습할 모델에 대해 설정하는 파라미터들을 의미한다.
    * Ex) K-Nearest Neighbors에서 K와 Distance(L1/L2...)
* 즉, HyperParameter는 학습을 하는데 영향을 미치는 parameter 이고 학습을 하기 전 선택하는 parameter 입니다.
* Train time에 학습하는 것이 아니므로 데이터로 직접 학습시킬 방법이 없습니다. 그러므로 학습 전 사전에 반드시 선택해야만 합니다.
* "해당 문제"와 "데이터"에 맞는 모델의 Hyperparameter를 설정하는 방법은 다음과 같습니다.
    * 문제에 따라 다르므로(Problem dependent), 가장 잘 동작하는 값을 사용한다.
    * 여러번의 학습을 통해 성능을 구하고, 그래프를 그려서 가장 좋은 hyperparameter 조합으로 설정한다.



## Setting Hyperparameters 

다양한 방법으로 실험을 하여 최적의 hyperparameter 값을 찾는 일은 아주 중요합니다. 하이퍼파라미터 값들을 실험해 보는 작업도 다양합니다.



### Idea #1: 모든 데이터를 Train 으로 사용하는 방법

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182049136-47491a80-c343-4186-89bf-43700c0dda09.png">
</p>

* 첫번째로 생각할 수 있는 방법은 전체 데이터셋을 이용해 학습했을때 가장 좋은 성능을 보인 hyperparameter를 사용하는 것입니다.
* terrible한 방법. 절대 이렇게 하면 안됩니다.

전체 데이터셋의 정확도를 올리는 전략대로 한다면 K-NN 분류기의 경우 K=1 일 때 학습 데이터를 가장 완벽하게 분류합니다. 하지만 앞선 예제에서도 보았듯이, 실제로는 K를 더 큰 값으로 선택하는 것이 학습 데이터에서는 몇 개 잘못 분류할 수는 있지만 학습 데이터에 없던 데이터에 대해서는 더 좋은 성능을 보일 수 있습니다.

궁극적으로 기계학습에서는 학습 데이터를 얼마나 잘 맞추는지는 중요한게 아니라, 우리가 학습시킨 분류기가 한번도 보지 못한 데이터를 얼마나 잘 예측하는지가 중요하므로, 학습 데이터에만 신경쓰는 것은 최악입니다.



### Idea #2: 데이터를 Train과 Test 로 나누는 방법

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182049139-a59d8790-3fcf-4bb0-a742-1605d6812b66.png">
</p>

* 두번째로, 전체 데이터셋을 train data와 test data로 나누어 train data 로 학습을 시킨뒤 test data 에서 가장 성능이 높게 나오는 hyperparameter를 사용하는 것입니다.
* 해당 방법이 조금 더 합리적인 것 같지만, Test 데이터에 대해서만 좋은 결과 값을 가질 수도 있기에 이 방법 또한 아주 끔찍한 방법입니다. 절대 하면 안됩니다.

다시한번 기계학습의 궁극적인 목적은 한번도 보지 못한 데이터에서 잘 동작해야 합니다.



### Idea #3: 데이터를 Train, Validation(Dev), Test 로 나누는 방법

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182049144-b44da069-b5db-4294-8aeb-0a34a3b3eff4.png">
</p>

* 데이터의 대부분은 training set으로 나누고, 일부는 validation set, 그리고 나머지는 test set으로 나눕니다.
* trainning set, validation set을 통해 hyperparmeter 에 대해 실험해보고 마지막으로 새로운 데이터인 test set을 통해 평가하는 방법입니다.


최종적으로 개발/디버깅 등 모든 일들을 다 마친 후에 validation set에서 가장 좋았던 분류기를 가지고, test set에서는  "오로지 한번만" 수행합니다. 이 숫자가 알고리즘이 한번도 보지 못한 데이터에 얼마나 잘 동작해 주는지를 실질적으로 말해줄 수 있는 것입니다. 즉, 이 데이터를 최대한 "Unseen Data"로 활용하는 방법 입니다.



### Idea #4: Cross-Validation

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182050315-5fde59a1-eb00-4c95-8f81-6b708f1b61c9.png">
</p>

* Cross-Validation 은 아래 그림과 같이 Dataset를 Fold 단위로 자르고, 이 fold 중 하나를 validation으로 선택하고 나머지를 train 데이터로 사용합니다.
* 위 방법으로 Validation으로 사용할 fold를 바꿔가면서 반복 하고, 이중에 가장 좋은 성능을 가지는 Hyperparameter를 찾아내는 방법입니다.
* 이 방법은 validation의 데이터가 편향되는 현상을 방지할 수 있습니다.
* 기존에 방식보다 많은 학습시간을 요구하며 이 방법은 딥러닝에서 거의 사용되지 않고 데이터가 적은 상황에서 유용한 장점을 가집니다.

그림에서 데이터를 training/validation으로 딱 나눠 놓는 대신, training data를 여러 부분으로 나눠줍니다. 이런 식으로 번갈아가면서 validation set을 지정해 줍니다.

해당 예제에서는 5-Fold Cross Validation을 사용하고 있습니다. 처음 4개의 fold에서 하이퍼 파라미터를 학습시키고, 남은 한 fold에서 알고리즘을 평가합니다. 그리고 1,2,3,5 fold에서 다시 학습시키고 4 fold로 평가합니다. 이런식으로 계속 순환하여 최적의 하이퍼파라미터를 확인할 수 있을 것입니다. 이런 방식은 거의 표준이긴 하지만 실제로는 딥러닝같은 큰 모델을 학습시킬 때는 학습 자체가 계산량이 많기 때문에 실제로는 잘 쓰지 않습니다.


<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182050743-c92461ad-c93b-43d7-ad23-ab9568c1c078.png">
</p>


Cross Validation을 수행하고 나면 위와 같은 그래프를 보실 수 있습니다.

* X축은 K-NN의 K입니다. Y축은 분류 정확도입니다.
* 그래프는 각 K마다 5번의 Cross Validation을 통해 알고리즘이 얼마나 잘 동작하는지를 알려줍니다.
* K-NN 의 K 값을 5-fold cross-validation 을 통해 나타내면 위 그래프와 같이 나타낼 수 있고, 이 경우에는 k 가 약 7일때 최적의 hyperparameter 가 됨을 알 수 있습니다.

그리고 Cross Validation을 이용하여, 여러 validation folds 별 성능의 분산(variance)을 고려하여 "테스트셋이 알고리즘 성능 향상에 미치는 영향" 를 알아볼 수 있습니다. 분산을 같이 계산하게 되면, 어떤 하이퍼파라미터가 가장 좋은지 뿐만 아니라, 그 성능의 분산도 알 수 있습니다.

그러므로 하이퍼파라미터에 따라 모델의 정확도와 성능을 평가할 수 있으며, Validation에서 가장 좋은 성능을 내는 하이퍼파라미터를 선택하는 것이 좋은 전략입니다.



## K-Nearest Neighbor on images never used.

하지만 실제로 image classification task 에서는 다음과 같은 이유들 때문에 K-Nearest Neighbor Algorithm을 잘 사용하지 않습니다.

<p align="center">
<img alt="image" src="ttps://user-images.githubusercontent.com/77891754/182051009-ddfaa2d1-bb62-4469-8c45-79cc960ef747.png">
</p>

* 우선 한 가지 문제점은 k-nn이 너무 느리다는 것입니다.
* 또 하나의 문제는 L1/L2 Distance가 이미지간의 거리를 측정하기에 적절하지 않다는 점입니다. 즉 벡터간의 거리 측정 관련 함수들은(L1/L2) 이미지들 간의 "지각적 유사성"을 측정하는 척도로는 적절하지 않습니다.

위 사진은 가장 왼쪽에 원본 이미지와 변형된 3개의 이미지를 보여줍니다. 여기서 재미있는 부분은 원본사진과 각각의 사진에 거리가 모두 같은 사진입니다. 즉 원본 이미지에서 약간의 변형을 가한 세 개의 이미지는 모두 같은 L2 distance를 가집니다. 이러한 관점에서 이미지의 Distance의 값은 그렇게 의미 있는 값이 아닙니다. L1 distance 도 마찬가지입니다.



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182051015-d6ad00d8-93d1-4e8f-b25d-22e1fe854032.png">
</p>

* 차원의 저주(Curse of dimensionality) : 차원이 늘어날수록 필요한 train data가 기하급수적으로 증가함
* k-NN 알고리즘은 training data를 이용해서 공간을 분할하여 분류를 진행했습니다. 이게 잘 동작하려면 전체 공간을 densely 하게 채울 수 있을만한 데이터가 필요한데, 차원이 커지면 필요한 training data 수가 기하급수적으로 증가합니다. 기하급수적인 증가는 언제나 옳지 못합니다.
    * 1차원 에서는 4개의 데이터가 필요 했다면,
    * 2차원 에서는 4 * 4 = 16개의 데이터가,
    * 3차원 에서는 4 * 4 * 4 = 64개의 데이터가 필요합니다.

고차원의 이미지라면 모든 공간을 조밀하게 메울만큼의 데이터를 모으는 일은 현실적으로 불가능합니다. 또한 Nearest한 data point가 실제로는 아주 멀리 떨어진 데이터일 수도 있다.(즉, 아주 밀집된 경우에서만 잘 동작한다는 의미) 그러므로 K-NN을 사용할 시 항상 이 점을 염두해야 합니다.





# K-Nearest Neighbors: Summary

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182051736-e2590299-0ddd-4ee1-b442-b7c76df188a4.png">
</p>

* 요약을 해보자면 이미지 분류가 무엇인지 설명하기 위해 K-NN 예제를 들었습니다.
* "이미지"와 "정답 레이블"이 있는 트레이닝 셋이 있었고 테스트 셋을 예측하는데 이용하였습니다.





# Linear Classification

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182052171-80a3311f-85ee-4fa0-a56d-800dc2824284.png">
</p>

* Linear Classification(선형 분류)은 단순하지만 이후에 배우게 되는 Neural Network와 CNN의 기반이 되는 알고리즘입니다. 즉 아래 그림과 같이 기본 블럭이 되는 것입니다.
* 앞으로 보게될 다양한 종류의 딥러닝 알고리즘들의 가장 기본이 되는 building block중 하나가 바로 Linear classifier입니다.
* Linear classification이 어떻게 동작하는지를 정확히 이해하는것은 아주 중요합니다.





# Parametric Approach: Linear Classification

Linear classification에서는 K-NN과는 조금은 다른 접근 방법을 이용합니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182052500-76fd73b5-801e-42a6-97c5-d388673e8cc1.png">
</p>

위 그림에서 ‘W’에 train data의 요약된 정보가 들어있습니다. 딥러닝은 f(x,W)로 된 가설 함수를 적절하게 잘 설계하는 일입니다. 입력데이터 x 와 가중치(weight) W 를 조합하여 관계식을 만들때 가장 기초적인 방법이 이 둘을 곱하는 것이고 그것이 Linear Classification 입니다.

* Linear classifier는 Parametric Approach를 사용하며 "parametric model"의 가장 단순한 형태입니다.
    * Parametric approach는 모델의 파라미터(가중치)를 학습하는 방법입니다
    * K-NN에서는 파라미터 없이 전체 데이터를 저장하고 비교하기 때문에 prediction에서 느렸지만, linear classifier는 training data의 정보를 요약하여 요약된 정보를 파라미터 W에 모아줍니다. 즉 가중치를 학습하므로 prediction을 빠르게 수행할 수 있습니다.
    * 그러므로 딥러닝은 바로 이 함수 $F$의 구조를 적절하게 잘 설계하는 일이라고 할 수 있습니다.

* Linear classifier의 출력(class score)은 (데이터와 가중치 행렬의 inner product) + bias로 계산한다.
    * $f(x,W)=Wx+b$
        * $f(x,W)$ : class score를 반환
        * $W$ : 모델의 가중치 파라미터
        * $x$ : data
        * $b$ : bias

* 여기서, bias는 학습과는 무관한 데이터의 일부 클래스에 대한 선호도를 의미합니다. 주로 dataset이 unbalance할 때 사용합니다.(data independent scaling offset)
    * Ex) 개와 고양이의 분류(unbalance)
        * 개의 데이터가 고양이보다 많은 경우, bias는 고양이에서 개보다 높게 됨

* Ex) CIFAR-10에서의 Linear Classifier 10개의 카테고리 중 하나를 분류
    * Image를 펼침 (입력 이미지 32X32X3 = 3072개의 원소로 구성된 1차원 벡터)
    * $f(x,W)=Wx+b$
        * $f(x,W)$ : 10 x 1
            * 10개의 class이므로, 10개의 숫자로 이루어진 class score를 반환
        * $W$ : 10 x 3072
        * $x$ : 3072 x 1
        * $b$ : 10 x 1

입력이미지는 행렬의 형태로 W(weight)와 곱(내적)해지게 되고 거기에 bias를 더해 각 class에 대한 점수가 나옵니다. 즉, 여기서 곱해지는 w 의 각각의 행(row)이 각 클래스의 평균적인 템플릿이라고 할 수 있습니다.



함수가 어떻게 동작하는지 그림으로 살펴보겠습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182053874-1b43d307-dbd9-4409-811d-8dcbe7f47181.png">
</p>

이 그림을 보면 왼쪽에 2x2 입력 이미지가 있고 전체 4개의 픽셀입니다. Linear classifier는 2x2 이미지를 입력으로 받고 이미지를 4-dim 열 벡터로 쭉 폅니다. 이 예제에서는 고양이, 개, 배 이렇게 세가지 클래스만 분류하는 가정을 하겠습니다.

* 가중치 행렬 W는 4x3 행렬이 됩니다.
* 입력은 픽셀 4개고 클래스는 총 3개 입니다.
* 추가적으로 3-dim bias 벡터가 있습니다. bias는 데이터와 독립적으로 각 카테고리에 scailing offsets을 더해주어 연결됩니다.
* "고양이 스코어" 는 입력 이미지의 픽셀 값들과 가중치 행렬을 내적한 값에 bias term을 더한 것입니다.

이러한 관점에서 Linear classification은 템플릿 매칭과 거의 유사합니다. 가중치 행렬 W의 각 행은 각 이미지에 대한 템플릿으로 볼 수 있고, 가중치 행 벡터와 이미지의 열벡터 간의 내적을 계산하는데, 여기에서 내적이란 결국 클래스 간 탬플릿의 유사도를 측정하는 것과 유사함을 알 수 있습니다.





# Interpreting a Linear Classifier

## Templete of classes weight
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182054230-f58e8222-0fc4-4e9f-9426-4d2d2b01a25a.png">
</p>

* Linear Classifier에서 가중치 행렬의 각 행은 각 class에 대한 템플릿이라고 해석할 수 있습니다.
    * 각 행에서의 위치 값들은 해당 위치의 픽셀이 해당 클래스에 얼마나 영향을 미치는지를 알려줍니다.
    * 따라서, 가중치 행렬의 각 행을 이미지로 시각화하면, linear classifier가 데이터를 어떻게 바라보는지 알 수 있다.
* 위 슬라이드 하단의 희미한 그림들은 실제 가중치 행렬이 어떻게 학습되는지 볼 수 있는데, CIFAR-10의 plane, car, bird 등 각 10개의 카테고리에 해당하는 가중치 행렬의 각 행을 시각화 한 것입니다.

Linear classifier 의 문제점은 한 class 내에 다양한 특징들이 존재할 수 있지만, 모든 것들을 평균화시킨다는 점이 있습니다. 그래서 다양한 모습들이 있더라도 각 카테고리를 인식하기위해 단 하나의 템플릿만을 학습하다는 것입니다.

말(馬)을 분류하는 템플릿을 살펴보면 바닥은 푸르스름해 보이며, 보통 말이 풀밭에 서 있으니 템플릿이 바닥을 푸르스름하게 학습한 것입니다. 그런데 유심히 살펴보면 말의 머리가 두 개로 각 사이드 마다 하나씩 달려 있습니다. 머리 두개 달린 말은 존재하지 않습니다. 하지만 Linear classifier가 클래스 당 하나의 템플릿밖에 허용하지 않으므로 이 방법이 최선입니다.

하지만 클래스 당 하나의 템플릿만 학습 할 수 있다는 것과 같은 제약조건이 없는 Neural Network같은 복잡한 모델이라면 조금 더 정확도 높은 결과를 볼 수 있을 것입니다.



## 이미지를 고차원 공간의 한 점으로

Linear classifier 을 또 다른 관점으로 해석하면 이미지를 고차원의 한 점으로 볼 수도 있습니다.

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182055530-a936de4b-b63d-47c5-a2ba-555b2719eefd.png">
</p>

* 각 이미지을 고차원 공간의 한 점이라고 생각했을때, Linear classifier는 각 클래스를 구분시켜주는 선형 결정 경계를 그어주는 역할을 합니다. 
* 가령 왼쪽 상단에 비행기를 예로, Linear classifier는 파란색 선을 학습해서 비행기와 다른 클래스를 구분할 수 있습니다.



## Hard cases for a linear classifier
 
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182055854-66f54bac-e157-4647-9df3-7c01fda3e9da.png">
</p>

* 하지만 이미지가 고차원 공간의 하나의 점 이라는 관점으로 해석하면 다음과 같은 데이터들은 Linear Classifier 를 통해 분류하기 어렵다는 문제점이 있습니다.
    * linear한 bounary만 그릴 수 있다.
    * boundary가 linear하지 않은 경우, 잘 동작하지 않는다.
    * 데이터가 몇개의 점처럼 나타나는 경우에 잘 동작하지 않는다.




<p align="center">
<img alt="image" src="">
</p>


Linear classifier는 단순히 행렬과 벡터 곱의 형태라는 것을 알았고, 템플릿 매칭과 관련이 있고, 이 관점에서 해석해 보면 각 카테고리에 대해 하나의 템플릿을 학습한다는 것을 배웠습니다. 그리고 가중치 행렬 W를 학습시키고 나면 새로운 학습 데이터에도 스코어를 매길 수 있습니다.

Linear classifier가 어떻게 생겼고, 어떻게 동작하는지만 가볍게 알아보았습니다. 다음 시간에는 적절한 가중치 행렬 W를 고르는 법과 다양한 알고리즘들에 대해서 다뤄보도록 하며, 더 나아가 Loss function/optimization/ConvNets에 대해서 배울 것입니다.




