---
layout: post
title: CS231n Lecture2 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html)을 바탕으로 작성되었습니다.





<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/181658691-de8b4117-b1cf-4324-89b5-7780e4f3ccb4.png">
</p>





# Image Classification

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/181661581-27c16863-6d43-4ab3-b990-f004be2a9126.png">
</p>

* Image Classification은 컴퓨터비전 분야에서 Core Task 에 속합니다.
* Image Classification을 한다고 할때, 미리 정해놓은 카테고리 집합(discrete labels)이 있는 시스템에 이미지를 입력하여, 컴퓨터가 이미지를 보고 어떤 카테고리에 속하는지 고르는 것입니다.
* 하지만 사람의 시각체계는 Visual Recognition task에 고도화 되어 있기 때문에 쉬워보이지만, 기계의 입장에서는 어려운 일 입니다.



## The Problem: Semantic Gap

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182022436-6e84c5c6-7d5b-483a-9bac-de28d236ac73.png">
</p>

* Semantic Gap(의미론적 차이) : 사람 눈으로 보는 이미지, 실제 컴퓨터가 보는 픽셀 값과의 차이
* 위 그림은 컴퓨터가 이미지를 바라보는 관점
* 이미지는 0부터 255까지의 숫자로 픽셀이 표현되며, Width(너비) x Height(높이) x Channel(채널)의 크기의 3차원 배열. 각 채널은 red, green, blue를 의미
* 우리가 보기에는 고양이 이미지이지만, 컴퓨터에게 이미지는 그저 아주 큰 격자 모양의 숫자 집합

위 사진과 같이 기계는 고양이 사진을 입력받으면, RGB(Red, Blue, Green) 값을 기준으로 격자 모양의 숫자들을 나열하여 인식합니다. 하지만 기계는 카메라 각도나 밝기, 객채의 행동 혹은 가려짐 등 여러차이로 인해 이미지의 픽셀 값이 달리 읽어 사물을 다르게 인식하는데, 이러한 기계를 잘 인식할 수있도록 알고리즘 개발을 시도 했으나, 다양한 객체들에게 유연하고 확장성 있는 알고리즘을 개발하는데 한계가 있었습니다.



## Viewpoint variation(카메라의 위치 변화)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023890-2e54b6a5-249c-4551-8e35-848ac4eefc42.png" style="zoom:80%;">
</p>

* 가령 이미지에 아주 미묘한 변화만 주더라도 픽셀 값들은 모조리 변하게 될 것.
* 고양이 이미지를 예로 들었을때, 고양이 한 마리가 얌전히 앉아만 있으며 아무 일도 일어나지 않겠지만, 카메라를 아주 조금만 옆으로 옮겨도 모든 픽셀 값들이 모조리 달라질 것입니다. 하지만 픽셀 값이 달라진다해도 고양이라는 사실은 변하지 않기 때문에 Classification 알고리즘은 robust해야 합니다.



## Illumination(조명에 의한 변화)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023326-9ffd08c3-802a-457c-9a46-73b290c7ee3a.png" style="zoom:80%;">
</p>

* 바라보는 방향 뿐만 아니라 조명 또한 문제가 될 수 있습니다. 어떤 장면이냐에 따라 조명은 각양각생일 것입니다.
* 고양이가 어두운 곳에 있던 밝은 곳에 있던 고양이는 고양이 이므로, 알고리즘은 robust해야 합니다.



## Deformation(객체 변형에 의한 변화)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023335-d103a595-9f35-48a3-aa16-413cba93fe0e.png" style="zoom:80%;">
</p>

* 객체 자체에 변형이 있을 수 있습니다.
* 고양이는 다양한 자세를 취할 수 있는 동물 중 하나인데, deformation에 대해서도 알고리즘은 robust해야 합니다.



## Occlusion(객체 가려짐에 의한 변화)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023353-384225c0-7726-4c25-8b72-67b23e8ec202.png" style="zoom:80%;">
</p>

* 가려짐(occlusion)도 문제가 될 수 있습니다. 가령 고양이의 일부밖에 볼 수 없는 상황이 있을 수도 있습니다.
* 고양이의 얼굴밖에 볼 수 없다던가, 극단적인 경우에는 소파에 숨어들어간 고양이의 꼬리밖에 볼 수 없을지도 모르지만, 사람이라면 고양이라는 사실을 단번에 알아챌 수 있습니다.
* 즉 이 이미지는 고양이 이미지라는 것을 알 수 있기 때문에, 알고리즘은 robust해야 합니다.



## Background Clutter(배경과 유사한 색의 객체)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023367-00065595-9459-43e3-bc10-56b619a9d268.png" style="zoom:80%;">
</p>

* Background clutter(배경과 비슷한 경우)라는 문제도 존재.
* 고양이가 배경과 거의 비슷하게 생겼을 수도 있기때문에, 알고리즘은 robust해야 합니다.



## IntraClass variation(한 클래스에 여러 종류)
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182023377-577d42d6-3b4b-476c-88af-21f622373443.png" style="zoom:80%;">
</p>

* 하나의 클래스 내에도 다양성이 존재. 즉 "고양이"라는 하나의 개념으로 모든 고양이의 다양한 모습들을 전부 소화해 내야 합니다.
* 고양이에 따라 생김새, 크기, 색, 나이가 각양 각색일 것.

위와 같은 이유들 때문에 Image Classification 문제는 어렵습니다. 사물 인식의 경우, 가령 고양이를 인식해야 하는 상황이라면 객체를 인식하는 직관적이고 명시적인 알고리즘은 존재하지 않습니다.





# Image Classification Algorithm - 기존의 시도들

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182024327-c36554c9-c690-4812-87c5-29790e8a235c.png">
</p>

* 기존에는 Edges and corners를 찾는, 즉 Image의 Feature(특징)을 찾고 Feature(특징)을 이용하여 명시적인 규칙을 만드는 방법으로 접근하였다.
* 하지만 아래의 이유로 잘 동작하지 않았다.
    * 앞에서 살펴본 조건들에서 여전히 robust하지 못하다.
    * 특정 class에 동작하도록 구현된 알고리즘은 다른 class에 적용하지 못한다.

즉 알고리즘이 robust하지 못 할 뿐더러, 각 class에 대해 새로 다시 짜야하므로, 확장성이 전혀 없는 방법입니다. 따라서 이러한 문제를 해결하기위해, 이 세상에 존재하는 다양한 객체들에 대해 적용이 가능한 방식이 필요합니다.





# Image Classification Algorithm - Data-Driven Approach
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182024392-e8108c72-528c-42fa-aab5-9dc759a2d330.png">
</p>

* 기존에는 이미지를 인식시킬 때, 각 객체에 대한 규칙을 하나하나 정하였다. 그러나 실제 생활에서는 수많은 객체들이 존재하기에 객체마다 규칙을 정해주는 것은 한계를 가지기 때문에, 다양한 객체들에 대해 적용하기 위한 Insight로 데이터 중심 접근방법(Data-Driven Approcach)을 사용합니다.
* Data-Driven Approcach은 객체의 특징을 규정하지 않고, 다양한 사진들과 label을 수집하고, 이를 이용해 Machine Learning Clssifier 모델을 학습하고, 새로운 이미지를 테스트해 이미지를 새롭게 분류하는 방식입니다.

Data-Driven Approcach은 Machine Learning의 key insight이며, Deep Learning 뿐만 아니라 아주 일반적인 개념입니다.






# Example Dataset: CIFAR10

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182025355-48224f09-1a41-4188-abc4-d23794ed31e0.png">
</p>

* Cifar-10은 Machine Learning에서 자주 쓰는 연습용(테스트용) 데이터셋.
* CIFAR-10에는 10가지 클래스가 존재(비행기, 자동차, 새, 고양이 등)
* 총 50,000여개의 학습용 이미지와, 10,000여개의 테스트 이미지가 존재.
* 32 x 32 이미지

CIFAR-10 데이터셋을 이용해서 Nearest Neighbor(NN) 예제를 살펴보겠습니다. 우선 오른쪽 칸의 맨 왼쪽 열은 CIFAR-10 테스트 이미지이며, 오른쪽 방향으로는 학습 이미지 중 테스트 이미지와 유사한 순으로 정렬했습니다. 테스트 이미지와 학습 이미지를 비교해 보면, 눈으로 보기에는 상당히 비슷해 보입니다.

두 번째 행의 이미지는 "개" 이며, 가장 가까운 이미지(1등)도 "개" 입니다. 하지만 2등, 3등을 살펴보면 "사슴"이나 "말"같아 보이는 이미지들도 있습니다. "개"는 아니지만 눈으로 보기에는 아주 비슷해 보입니다.

가장 간단하고 기본적인 분류방법인 Nearest Neighbor(NN)는 "가장 가까운 이웃 찾기" 알고리즘으로, 직관적인데 새로운 이미지와 이미 알고 있던 이미지를 비교하여 가장 비슷하게 생긴 것을 찾아내는 알고리즘을 말합니다. NN 알고리즘이 잘 동작하지 않을 것 같아 보이지만 그럼에도 해 볼만한 아주 좋은 예제입니다.





# Nearest Neighbor(NN)

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182025828-626fb75d-0091-4abf-b136-cbdd65c09757.png">
</p>

* Nearest Neighbor(NN) : 입력받은 데이터를 저장한 다음 새로운 입력 데이터가 들어오면, 기존 데이터에서 비교하여 가장 유사한 이미지 데이터의 라벨을 예측하는 알고리즘입니다.

즉, 최근접 이웃 분류기는 테스트 이미지를 위해 모든 학습 이미지와 비교를 하고 라벨 값을 예상합니다.

위 코드를 보면 다음과 같이 동작합니다.
* Train함수 - Train Step에서는 단지 모든 학습 데이터를 기억합니다. (입력은 이미지와 레이블이고, 출력은 우리의 모델)
* Predict 함수 - Pridict Step에서는 새로운 이미지가 들어오면 새로운 이미지와 기존의 학습 데이터를 비교해서 가장 유사한 이미지로 레이블링을 예측합니다. (입력이 모델이고, 출력은 이미지의 예측값)



## Distance Metric to compare images

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182026350-75133d32-ee4d-4f09-b5df-f06faceec22f.png">
</p>

* 여기에서 중요한 점은 이미지 쌍이 있을 때 얼마나 유사한지를 어떻게 비교를 할 것인지가 관건입니다.
* 테스트 이미지 하나를 모든 학습 이미지들과 비교할 때 여러가지 비교 방법들이 있습니다.
* 위 그림에서 $I_1$, $I_2$ 벡터로 나타냈을 때, 벡터 간의 L1 Distance(Manhattan distance)를 사용하여 계산.
* 결과는 모든 픽셀값 차이의 합

이미지를 Pixel-wise로 비교합니다. 가령 4x4 테스트 이미지가 있다고 가정할 때, training/test image의 같은 자리의 픽셀을 서로 빼고 절댓값을 취합니다. 이렇게 픽셀 간의 차이 값을 계산하고 모든 픽셀의 수행 결과를 모두 더합니다.

"두 이미지간의 차이를 어떻게 측정 할 것인가?"에 대해 구체적인 방법을 제시합니다. 지금 예제의 경우에는 두 이미지간에 "456" 만큼 차이가 납니다.



## NN Classifier - python code
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182026636-55c3485a-d058-4cf4-bc62-a924a76ba0f3.png">
</p>

* Train 함수
    * NN의 경우 Train 함수는 단지 학습 데이터를 기억하는 것입니다.
    * N개의 이미지가 있는 X와 N개의 라벨이 있는 y, 훈련데이터를 Xtr과 ytr에 모두 저장합니다.
* Test 함수
    * 이미지를 입력으로 받고 L1 Distance로 비교합니다. 즉 학습 데이터들 중 테스트 이미지와 가장 유사한 이미지들을 찾아냅니다.
    * 모든 훈련데이터의 이미지가 저장된 Xtr과 비교하고자하는 X의 행을 Xtr과 dimension을 맞춰준다음 빼고 abs를 취해줍니다.
    * axis=1의 의미는 y축을 기준으로 더한 값들로 저장된 배열을 distances에 저장해줍니다.
    * 그중 제일 작은 값을 argmin으로 찾아내서 그 위치를 min_index에 저장하고
    * ytr에 그 인덱스값을 넣어 예측값으로 보냅니다.


여기서 Simple Classifier인 NN알고리즘에 대해 생각할 점이 있습니다.

1. Trainset의 이미지가 총 N개라면, Train/Test 함수의 Train time은 데이터를 기억만 하면 되기 때문에 상수시간 O(1)입니다.
2. 하지만 Test time에서는 N개의 학습 데이터 전부를 테스트 이미지와 비교해야만 합니다. 즉 (Train time < Test time) 데이터 학습은 빠르지만, 새로운 데이터를 판단하는데 있어서 걸리는 시간이 많이 필요하므로, Test시 Test time은 Test data 가 많아지면 Test 시간이 늘어납니다.

실제로 Train Time은 조금 느려도 되지만 Test Time에서는 빠르게 동작하길 원합니다. Classifier의 좋은 성능을 보장하기 위해서 Train Time에 많은 시간을 쏟을 수도 있기 때문입니다. 하지만 NN Classifier의 "Test Time" 을 생각해보면, 일반적으로 모델들은 핸드폰, 브라우저 등 Low Power Device에서 동작해야 되기 때문에 test time이 빨라야 합니다. 하지만 Nearest Neighbor 모델은 느립니다.



<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182027154-de192791-72c7-4f2d-91be-f4289fcbd6af.png" style="zoom:80%;">
</p>

* NN 알고리즘으로 decision regions를 그려본 이미지입니다.
* 2차원 평면 상의 각 점은 학습데이터, 점의 색은 클래스 라벨(카테고리)입니다.
* 2차원 평면 내의 모든 좌표에서 각 좌표가 어떤 학습 데이터와 가장 가까운지 계산하고, 각 좌표를 해당 클래스로 칠했습니다.

NN 알고리즘은 "가장 가까운 이웃" 만을 보기 때문에, 녹색 한 가운데 노란색 영역, 초록색 영역에서 파란색 영역 침범하는 구간 등등 decision boundary가 Robust 하지 않음을 볼 수 있습니다. 해당 점들은 잡음(noise)이거나 가짜(spurious)일 가능성이 높습니다. 이러한 단점들로 NN 알고리즘은 잘 사용하지 않습니다.





# K-Nearest Neighbors(K-NN)

* NN의 일반화된 방법인 K-NN은 K개의 가장 가까운 지점의 데이터들의 Majority vote를 통해 예측하는 모델입니다.
* 단순하게 가장 가까운 이웃만 찾기보다는 Distance metric을 이용해서 가까운 이웃을 K개의 만큼 찾고, 이웃끼리 투표를 하는 방법입니다. 그리고 가장 많은 특표수를 획득한 레이블로 예측합니다.
* K-Nearest Neighbor Algorithm 을 사용할때, 결정해야하는 두 가지 parameter가 있습니다.
    * K값
    * Distance Metric(거리척도)



## K-Nearest Neighbors(K-NN) - K

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182027762-fb620d6c-454f-4982-88f1-811821328d8c.png">
</p>

위 그림은 동일한 데이터를 사용한 k-nn 분류기들로, 각각 K=1/3/5 에서의 결과입니다.
* K=3 의 경우, 앞서 초록색 영역에 자리 잡았던 노란색 점 때문에 생긴 노란 지역이 깔끔하게 사라졌습니다. 중앙은 초록색이 깔끔하게 점령했습니다. 그리고 왼쪽의 빨강/파랑 사이의 뾰족한 경계들도 다수결에 의해 점차 부드러워지고 있습니다.
* K=5의 경우, 파란/빨간 영역의 경계가 이제는 아주 부드럽고 좋아졌습니다.
* 흰색 영역은 K-NN이 "대다수"를 결정할 수 없는 지역으로, 어떤 식으로든 추론을 해보거나, 임의로 정할 수도 있습니다.
* KNN은 위 슬라이드와 같이, K가 커질수록 decision boundary가 더 smooth해지는 경향이 있습니다.


이러한 방식을 이용하면 좀 더 일반화 된 결정 경계를 찾을 수 있습니다. 여기서 K값의 증가함에 따라서 부드러워 지지만, 흰색 영역이 증가 하는 것을 볼 수 있습니다. 이 흰색 영역 은 어느 쪽에도 분류 할지 알 수 없는 영역입니다. 이러한 부분에서 K값이 증가한다고 항상 좋은 것이 아니라. 데이터나 상황에 따라서 알맞은 K값을 찾아야합니다.



## K-Nearest Neighbors(K-NN) - Distance Metric

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/182028810-f0bbc1a0-3a1a-4624-8bdf-8df3bc358607.png">
</p>

* K-NN을 사용할 때 결정해야 할 한 가지 사항으로, 서로 다른 점들을 어떻게 비교할 것인지 입니다. 즉 데이터 간의 거리를 잴 때 사용하는 기준입니다.
* L1 Manhattan Distance: "픽셀 간 차이 절대값의 합"
* L2 Euclidean distance: "제곱 합의 제곱근"을 거리로 이용




어떤 "거리 척도(distance metric)" 을 선택할지는
아주 흥미로운 주제입니다.

왜냐하면 서로 다른 척도에서는 해당 공간의
근본적인 기하학적 구조 자체가 서로 다르기 떄문입니다.

왼쪽에 보이는 사각형은 사실 L1 Distance의 관점에서는 원입니다.
생긴 모습은 원점을 기준으로 하는 사각형의 모양이죠

L1의 관점에서는 사각형 위의 점들이 모두
원점으로부터 동일한 거리만큼 떨어져 있습니다.

반면 L2, Euclidean distance 의 관점에서는 원입니다.
우리가 예상했던 바로 그 원입니다.

이 두 가지 거리 척도간에는 아주 흥미로운 차이점이 있습니다.
L1은 어떤 좌표 시스템이냐에 따라 많은 영향을 받습니다.

가령 기존의 좌표계를 회전시키면
L1 distance가 변합니다.

반면 L2 Distance의 경우에는 좌표계와 아무 연관이 없습니다.

만약 특징 벡터의 각각 요소들이 개별적인 의미를
가지고 있다면(e.g. 키 몸무게)

L1 Distance가 더 잘 어울릴 수도 있습니다.

하지만 특징 벡터가 일반적인 벡터이고, 요소들간의 실질적인 의미를 잘 모르는
경우라면, 아마도 L2 Distance가 조금은 더 잘 어울릴 수 있습니다.

여기에서 주목할 점은 k-nn에 다양한 거리 척도를 적용하면
k-nn으로 다양한 종류의 데이터를 다룰 수 있다는 점입니다.

벡터나 이미지 외에도 말이죠. 가령 문장을 분류하는 문제가 있다고 해봅시다.

k-nn 분류기로 이 문제를 다루려면
어떤 거리 척도를 사용할지만 정해주면 됩니다.

두 문장 간의 거리를 측정할 수 있는 어떤 것이든 사용하면 됩니다.

거리 척도만 정해주면 어떤 종류의 데이터도 다룰 수 있습니다.

k-nn은 아주 단순한 알고리즘입니다. 하지만 새로운 문제를
접했을 때  간단히 시도해 볼만한 아주 좋은 알고리즘입니다.



<p align="center">
<img alt="image" src="">
</p>



자 그러면 어떤 거리 척도를 사용하는지에 따라서
실제 기하학적으로 어떻게 변하는지 알아봅시다.

양 쪽 모두 동일한 데이터입니다. 다만 왼쪽은 L1 Distance를
오른쪽은 L2 Distance를 사용했습니다.

결과를 보시면 거리 척도에 따라서 결정 경계의 모양
자체가 달라짐을 알 수 있습니다.

왼쪽의 L1 Distance를 살펴보면 결정 경계가
"좌표 축"에 영향을 받는 경향을 알 수 있습니다.

L1 Distance가 좌표 시스템의 영향을 받기 때문입니다.

반면 L2 Distance는 좌표 축의 영향을 받지 않고 결정
경계를 만들기 때문에 조금 더 자연스럽습니다.




<p align="center">
<img alt="image" src="">
</p>




지금까지의 모든 예제는 저의 웹 데모사이트에서 가져왔습니다.
여러분은 여기에서 여러분만의 k-nn 분류기를 설계할 수 있습니다.

이 프로텍터 스크린으로 보여드리기가 참 어렵군요
집에가서 한번 해보시기 바랍니다.

좋습니다 말썽이 좀 있었습니다

오늘은 웹 데모를 생략하겠습니다만
꼭 한번 해보시기 바랍니다.

상당히 재미있습니다. K와  리 척도를 바꿔보면서 어떻게
결정 경계가 만들어지는지에 대한 직관을 얻으시길 바랍니다.




<p align="center">
<img alt="image" src="">
</p>



여러분이 k-nn을 사용하려고 한다면
반드시 선택해야 하는 몇 가지 항목이 있습니다.

앞서 K에 대해서 이야기했습니다.
L1/L2와 같은 거리 척도 또한 다뤘죠

그렇다면 어떻게 하면 "내 문제"와 "데이터"에
꼭 맞는 모델을 찾을 수 있을까요?

K와 거리척도를 "하이퍼 파라미터" 라고 합니다.

하이퍼파라미터는 Train time에 학습하는 것이 아니므로
여러분이 학습 전 사전에 반드시 선택해야만 합니다.

데이터로 직접 학습시킬 방법이 없습니다.




<p align="center">
<img alt="image" src="">
</p>



그렇다면 하이퍼파라미터를 어떻게 정해야 할까요?

하이퍼 파라미터를 정하는 일은
문제의존적(problem-dependent)입니다.

가장 간단한 방법은 데이터에 맞게 다양한
하이퍼파라미터 값을 시도해 보고 가장 좋은 값을 찾습니다.

질문있나요?

[질문하는 학생]

질문은 "어떤 경우에 L1 Distance가
L2 Distance보다 더 좋은지" 입니다.

그것은 문제의존적(problem-dependent)입니다.

어떤 경우에  L1/L2를 써야 하는지
결정하는 것은 어렵겠지만

L1은 좌표계에 의존적이므로 여러분의 데이터가
좌표계에 의존적인지를 판단하는 것이 판단 기준이 될 수 있습니다.

여러분에게 어떤 특징 벡터가 있고 각 요소가
어떤 특별한 의미를 지니고 있다면

가령 직원들을 분류하는 문제가 있을 때, 데이터의 각 요소가
직원들의 다양한 특징에 영향을 줄 수 있습니다.

가령 봉급, 근속년수와 같은 예가 될 수 있겠습니다.

이처럼 각 요소가 특별한 의미를 가지고 있다면
L1 을 사용하는것이 좀 더 괜찮을 지도 모릅니다.

하지만 일반적으로는 하이퍼파라미터 선택은
어떤 문제와 데이터인지에 의존적입니다.



<p align="center">
<img alt="image" src="">
</p>




하이퍼파라미터는 단지 여러가지 시도를 해보고
좋은 것을 선택하는 것이 좋습니다.

하지만 하이퍼파라미터 값들을 실험해 보는 작업도 다양합니다.

"다양한 하이퍼 파라미터를 시도해 보는 것" 과
"그중 최고를 선택하는 것" 이 무슨 뜻일까요?

가장 먼저 떠올릴 수 있는 아이디어는
아주 단순합니다.

"학습데이터의 정확도와 성능"를 최대화하는
하이퍼파라미터를 선택하는 것이죠.

사실 정말 끔찍한 방법입니다.
절대로 이렇게 해서는 안됩니다.

<p align="center">
<img alt="image" src="">
</p>



가령 NN 분류기의 경우 K = 1 일 때
학습 데이터를 가장 완벽하게 분류합니다.

앞서 말씀드린 전략대로라면(트레이닝 데이터의 정확도를 올리는)
항상 K = 1 일 때가 최고입니다.

하지만 앞선 예제에서도 보았듯이, 실제로는 K를 더 큰 값으로 선택하는 것이
학습 데이터에서는 몇 개 잘못 분류할 수는 있지만

학습 데이터에 없던 데이터에 대해서는
더 좋은 성능을 보일 수 있습니다.

궁극적으로 기계학습에서는 학습 데이터를 얼마나 잘 맞추는지는
중요하지 않습니다. 우리가 학습시킨 분류기가

한번도 보지 못한 데이터를 얼마나 잘 예측하는지가 중요하죠

그러므로 학습 데이터에만 신경쓰는 것은 최악입니다.
비추입니다.


<p align="center">
<img alt="image" src="">
</p>




또 다른 아이디어가 있습니다. 전체 데이터셋 중 학습 데이터를 쪼개서
일부를 테스트 데이터로 사용하는 것입니다.

학습 데이터로 다양한 하이퍼파라미터 값들을 학습을 시키고 테스트
데이터에 적용시켜본 다음, 하이퍼파라미터를 선택하는 방법이죠


<p align="center">
<img alt="image" src="">
</p>


이 방법이 조금 더 합리적인 것 같지만 사실은,
이 방법 또한 아주 끔찍한 방법입니다. 절대 하면 안됩니다.

다시한번 기계학습의 궁극적인 목적을 말씀드리자면
한번도 보지 못한 데이터에서 잘 동작해야 합니다.



<p align="center">
<img alt="image" src="">
</p>



테스트셋으로는 한번도 보지 못했던 데이터에서의
알고리즘의 성능을 측정할 수 있어야 합니다.

그런데 만약 학습시킨 모델들 중
테스트 데이터에 가장 잘 맞는 모델을 선택한다면

우리는 그저 "테스트 셋에서만"  잘 동작하는
하이퍼파라미터를 고른 것일 수 있습니다.

그렇게 되면,  더이상 테스트 셋에서의 성능은 한번도 보지못한
데이터에서의 성능을 대표할 수는 없습니다.

그러니 이 또한 하지 말아야 합니다. 좋은 생각이 아닙니다.
그렇게 하면 곤경에 빠질 것입니다.



<p align="center">
<img alt="image" src="">
</p>



훨씬 더 일반적인 방법은 데이터를 세 개로 나누는 것입니다.

데이터의 대부분은 트레이닝 셋으로 나누고, 일부는
밸리데이션 셋, 그리고 나머지는 테스트 셋으로 나눕니다

그리고 다양한 하이퍼파라미터로 "트레이닝 셋" 을 학습시킵니다.

그리고 "벨리데이션 셋" 으로 검증을 합니다. 그리고 벨리데이션 셋에서
가장 좋았던 하이퍼파라미터를 선택합니다.

그리고 최종적으로 개발/디버깅 등 모든 일들을 다 마친 후에

벨리데이션 셋에서 가장 좋았던 분류기를 가지고
테스트 셋에서는  "오로지 한번만" 수행합니다.

이 마지막 수치가 여러분의 논문과 보고서에 에 삽입될 것입니다.

그 숫자가 여러분의 알고리즘이 한번도 보지 못한 데이터에
얼마나 잘 동작해 주는지를 실질적으로 말해줄 수 있는 것입니다.

그리고 실제로 벨리데이션 데이터와 테스트 데이터를
엄격하게 나눠놓는 것은 상당히 중요합니다.

가령, 우리는 연구 논문을 작성할때
테스트 셋을 거의 마지막 쯤에야 한번 사용합니다.

저는 논문을 쓸때 마감 일주일 전 부터만
테스트 셋을 사용합니다.

우리는 정직하게 연구를 수행했고 논문의 수치를 공정하게 측정했다는
것을 보장하기 위해서죠. 상당히 중요합니다.

여러분은 여러분의 테스트 데이터를 잘 통제해야만 합니다.




<p align="center">
<img alt="image" src="">
</p>






또 다른 하이퍼파라미터 선택 전략은
크로스 벨리데이션(교차 검증) 입니다.

사실 이 방법은 작은 데이터셋일 경우 많이 사용하고
딥러닝에서는 많이 사용하진 않습니다.

이 아이디어는, 우선 테스트 데이터를 정해놓습니다.
테스트 데이터는 아주 마지막에만 사용할 것입니다.

그리고 나머지 데이터는 트레이닝/벨리데이션 으로 딱 나눠 놓는 대신에
트레이닝 데이터를 여러 부분으로 나눠줍니다.

이런 식으로 번갈아가면서 벨리데이션 셋을 지정해 줍니다.

이 예제에서는 5-Fold Cross Validation을 사용하고 있습니다.

처음 4개의 fold에서 하이퍼 파라미터를 학습시키고
남은 한 fold에서 알고리즘을 평가합니다.

그리고 1,2,3,5 fold에서 다시 학습시키고 4 fold로 평가합니다.
이런식으로 계속 순환합니다.

이런 방식으로 최적의 하이퍼파라미터를 확인할 수 있을 것입니다.

이런 방식은 거의 표준이긴 하지만 실제로는 딥러닝같은 큰 모델을 학습시킬
때는 학습 자체가 계산량이 많기 때문에 실제로는 잘 쓰지 않습니다.

질문 있나요?

[질문하는 학생]

질문은 "구체적으로 트레이닝 셋과 벨리데이션 셋의 차이가 무엇인지" 입니다.

k-NN의 예를 들어보자면 트레이닝 셋은
우리가 레이블을 기억하고 있는 이미지들 입니다.

어떤 이미지를 분류하려면 트레이닝 데이터의
모든 이미지들과 비교하게 되겠죠

그리고 가장 근접한 레이블을 선택합니다.
알고리즘은 트레이닝 셋 자체를 기억할 것입니다.

이제는 벨리데이션 셋을 가져와서
트레이닝 셋과 비교합니다.

그리고 이를 통해서 벨리데이션 셋에서는 분류기가
얼마만큼의 정확도가 나오는지 확인합니다.

이것이 바로 트레이닝/벨리데이션 셋의 차이점 입니다. 트레이닝 셋의
레이블을 볼 수 있지만 벨리데이션 셋의 레이블은 볼 수 없습니다.

벨리데이션 셋의 레이블은 알고리즘이
얼마나 잘 동작하는지를 확인할 때만 사용합니다.

질문있나요?

[학생이 질문]

질문은 "테스트 셋이 한번도 보지 못한 데이터를 대표할 수 있는지" 입니다.
이것은 실제도 문제가 될 수 있습니다

기본적인 통계학적 가정이 하나 있는데 여러분의 데이터는 독립적이며,
유일한 하나의 분포에서 나온다는 가정입니다 (i.i.d assumption)

그러니 모든 데이터는 동일한 분포를 따른다고 생각해야 합니다.
물론 실제로는 그렇지 않은 경우가 많습니다.

테스트 셋이 한번도 보지 못한 데이터를 잘 표현하지
못하는 경우를 경험하게 되실 것입니다.

그리고 이런 류의 문제는 datasets crators와
dataset curators가 생각해 볼 문제입니다.

하지만,가령 제가 데이터 셋을 만들때
하는 한가지 일은

데이터를 수집할 때, 일관된 방법론을 가지고
대량의 데이터를 한번에 수집하는 전략을 사용합니다.

그다음에 무작위로 트레이닝 데이터와
테스트 데이터를 나줘줍니다.

한가지 주의해야 할 점은
데이터를 지속적으로 모으고 있는 경우입니다.

먼저 수집한 데이터들을 트레이닝 데이터로 쓰고, 이후에 모은 데이터를 테스트
데이터로 사용한다면 문제가 될 수 있습니다.

대신에 데이터셋 전체를 무작위로 섞어서 데이터셋을 나누는 것이
그 문제를 완화 시킬 수 있는 한가지 방법일 수 있습니다.




<p align="center">
<img alt="image" src="">
</p>




크로스 벨리데이션을 수행하고 나면 다음과 같은 그래프를 보실 수 있습니다.
X축은 K-NN의 K입니다. 그리고 Y축은 분류 정확도입니다.

이 경우에는 5-fold 크로스 벨리데이션을 수행하였습니다.

각 K마다 5번의 크로스 벨리데이션을 통해
알고리즘이 얼마나 잘 동작하는지를 알려줍니다.

그리고 "테스트셋이 알고리즘 성능 향상에 미치는 영향" 를 알아보려면
K fold 크로스벨리데이션이 도움을 줄 수 있습니다.

여러 validation folds 별 성능의 분산(variance)을
고려해 볼 수 있습니다.

분산을 같이 계산하게 되면, 어떤 하이퍼파라미터가 가장
좋은지 뿐만 아니라, 그 성능의 분산도 알 수 있습니다.

여러분이 기계학습 모델을 학습시키는 경우에
보통 이런 모습의 그래프를 그리게 될 것입니다.

하이퍼파라미터에 따라 모댈의 정확도와
성능을 평가할 수 있습니다.

그리고 벨리데이션 셋의 성능이 최대인
하이퍼 파라미터를 선택하게 될 것입니다.

이 예제에서는 아마도 K = 7 일 경우에 가장 좋은 성능을 내는군요.


<p align="center">
<img alt="image" src="">
</p>





하지만 실제로는 입력이 이미지인 경우에는
k-NN 분류기를 잘 사용하지 않습니다.

앞서 이야기한 문제들 때문이죠.

우선 한 가지 문제점은 k-nn이 너무 느리다는 것입니다. 우리가
원하는 것과 정 반대이며 이 내용은 앞서 이야기한 적이 있었습니다.

또 하나의 문제는 L1/L2 Distance가 이미지간의 거리를
측정하기에 적절하지 않다는 점입니다.

이 벡터간의 거리 측정 관련 함수들은(L1/L2)
이미지들 간의 "지각적 유사성"을 측정하는 척도로는 적절하지 않습니다.

우리들은 이미지간의 차이를 어떻게 지각하는 것일까요?
가령 여기 왼쪽에 한 여성이 있습니다.

오른쪽에는 세 개의 왜곡된 이미지가 있습니다.

눈과 입을 가려도 보고, 몇 픽셀식 이동도 시켜보고,
전체 이미지에 파란색 색조도 추가시켜 보았습니다.

그리고 각 이미지와 원본의 사이의
Euclidean Distance를 측정해보면

이들은 모두 동일한 L2 Distance를 가집니다.
좋지 않은 현상이죠

이는 L2 Distance가 이미지들 간의 "지각적 유사도" 를
측정하기에는 적합하지 않다는 의미이기 때문입니다.


<p align="center">
<img alt="image" src="">
</p>






K-NN의 또 다른 문제 중 하나는 바로
"차원의 저주" 입니다.

K-NN을 다시한번 살펴보자면

K-NN가 하는 일은 트레이닝 데이터를 이용해서
공간을 분할하는 일이였습니다.

이는 K-NN이 잘 동작하려면 전체 공간을 조밀하게 커버할 만큼의
충분한 트레이닝 샘플이 필요하다는 것을 의미합니다.

그렇지 않다면 이웃이 사실은 엄청 멀 수도 있고 그렇게 되면
테스트 이미지을 제대로 분류할 수 없을 것입니다.

공간을 조밀하게 덮으려면 충분한 량의 학습 데이터가 필요하고
그 양은 차원이 증가함에 따라 기하급수 적으로 증가합니다.

아주 좋지 않은 현상입니다.
기하급수적인 증가는 언제나 옳지 못합니다.

고차원의 이미지라면 모든 공간을 조밀하게 메울만큼의
데이터를 모으는 일은 현실적으로 불가능합니다.

K-NN을 사용할 시 여러분은 항상 이 점을  염두해야 합니다.



<p align="center">
<img alt="image" src="">
</p>



요약을 해보자면 이미지 분류가 무엇인지 설명하기 위해
K-NN 예제를 들어보았습니다.

"이미지"와 "정답 레이블"이 있는 트레이닝 셋이 있었고
테스트 셋을 예측하는데 이용하였습니다.

질문 있나요?

[질문하는 학생]

오 죄송합니다. 질문은
"이전 슬라이드의 그림이 어떤걸 의미하는지"

"그림의 초록 점과 파란 점은 무엇인지" 입니다.

각 점은 트레이닝 샘플들을 의미합니다.
점 하나하나가 트레이닝 샘플입니다.

그리고 각 점의 색은 트레이닝 샘플이 속한
카테고리를 나타낸다고 보시면 됩니다.

가령 맨 왼쪽의 1차원을 보시면
이 공간을 조밀하게 덮으려면 트레이닝 샘플 4개면 충분합니다.

2차원 공간을 다 덮으려면 16개가 필요합니다.
1차원의 4배 입니다.

이렇게 3, 4, 5 차원 같이 고차원을 고려해보면

각 공간을 조밀하게 덮기 위해 필요한 트레이닝 샘플의 수는
차원이 늘어남에 따라 기하급수적으로 증가합니다.

그리고 가령 2차원 공간에서는 커브모양이 있습니다.

혹은 더 높은 차원에서는 일종의
샘플들의 manifolds가 있을 수 있습니다.

하지만 K-NN 알고리즘은 샘플들의 manifolds를
가정하지 않기 때문에

K-NN이 제대로 동작할 수 있는 유일한 방법은 공간을 조밀하게
덮을 만큼 충분히 많은 트레이닝 샘플을 가지는 것입니다.

지금까지 K-NN을 살펴보았습니다.

첫 과제에 K-NN 실습이 있으니 실제로
구현해 볼 기회가 있을 것입니다.

K-NN에 관현한 질문을 받고 다음 주제로 넘어 가겠습니다.

질문있나요?

[학생이 질문]

죄송하지만 다시 말해주세요

[학생이 질문]

질문은 "왜 저 이미지들의 L2 Distance가
같은지" 입니다.

제가 이 이미지들이 같은 L2 Distance를 가지도록
임의로 만들어냈기 때문입니다.

[웃음]

저는 단지 L2 Distance가 이미지간의 유사도을 측정하는데는
좋지 않다는 점을 강조하고 싶었습니다.

실제로 이 이미지들은 저마다 모두 다릅니다.

하지만 여러분이 K-NN을 사용한다면 이미지 간의
유사도를 특정할 수 있는 유일한 방법은

바로 이 단일 거리 성능 척도(L1/L2 등)을 이용하는 수 밖에 없습니다.

이 예시는 여러분들에게 Distance metric이 실제로는
이미지간의 유사도를 잘 포착해 내지 못한다는 것을 알려줍니다.

이 예시에 경우에 이러한 translation과 offset에도
Distance가 일치하도록 제가 임의로 만들어낸 것입니다.

질문 있으십니까?

[학생이 질문]

질문은 "이미지들이 모두 같은 사람이므로
distance가 같으면 좋은 것 아닌지"입니다.

이 예시에서는 그럴 수도 있습니다. 하지만 반례가 있을 수 있습니다.
가령 서로 다른 두 개의 원본 이미지가 있고

어떤 적절한 위치에 박스를 놓거나, 색을 더하거나 하게 되면
결국 두 이미지의 Distance를 엄청 가깝게 만들 수 있을 것입니다.

반대로 이 예시에서 똑같은 하나의 이미지에
(하나의 이미지이니 Distance가 변하면 안되지만)

임의로 움직이거나(shift) 색을 더하면(tinting)
Distance는 제멋대로 변할 것입니다.

그러니 다양한 서로다른 이미지들이 같은 Distance를 가지는
경우라면 잘 못될 수도 있는 것입니다.

질문 있으신가요?

[학생이 질문]

질문은 "최적의 하이퍼파라미터를 찾을 때 까지 학습을
다시 시키는 것이 흔한 방법인지" 입니다.

실제로 사람들은 가끔 그렇게 하곤 합니다.
하지만 그때 그때 다르다고 할 수 있습니다.

만약 여러분이 데드라인에 쫒기고 있고
당장 모델을 사용해야 한다면

그런데 데이터 셋 전부를 다시 학습시키는 것이 너무 오래 걸리면
다시 학습시키기 쉽지 않겠죠

하지만 다시 학습시킬 여유가 있고 1%의 성능이라도
짜내고 싶다면 여러분이 할 수있는 하나의 트릭이 될 수 있습니다.

지금까지는 k-NN이 기계학습 알고리즘이라는 점에서
지닌 다양한 특성들에 대해 배웠습니다.

하지만 실제로는 성능이 엄청 좋지는 않습니다.
특히 이미지에는 잘 사용하지 않습니다.




<p align="center">
<img alt="image" src="">
</p>





다 그럼 다음은
Linear Classification 입니다.

Linear Classification 아주 간단한 알고리즘입니다. 하지만
아주 중요하고 NN과 CNN의 기반 알고리즘이죠



<p align="center">
<img alt="image" src="">
</p>



일부 사람들은 Nerural Network를 레고블럭에 비유합니다.

NN을 구축할 때 다양한 컴포넌트들을 사용할 수 있습니다.
이 컴넌트들을 한데 모아서

CNN이라는 거대한 타워를 지을 수 있는 것입니다.

앞으로 보게될 다양한 종류의 딥러닝 알고리즘들의
가장 기본이 되는 블럭중 하나가 바로 Linear classifier입니다.

때문에 여러분이 Linear classification이 어떻게 동작하는지를
정확히 이해하는것은 아주 중요합니다.

왜냐면 Linear classification이 결국은 전체 NN을
이루게 될 것이기 떄문이지요.



<p align="center">
<img alt="image" src="">
</p>



NN의 구조적 특성을 설명하는 또 다른 예시가 있습니다.

저희 연구실에서 진행하고 있는
Image Captioning과 관련한 것입니다.

Image Captioni 에서는 이미지가 입력이고
이미지를 설명하는 문장이 출력입니다.



<p align="center">
<img alt="image" src="">
</p>



이미지를 인식하기 위해서 CNN을 사용합니다.
그리고 언어를 인식하기 위해서 RNN을 사용합니다.

우리는 그저 두개(CNN + RNN)을 레고 블럭처럼 붙히고 한번에
학습시킵니다. 그러면 이렇게 어려운 문제도 해결할 수 있는 것입니다.

이 모델에 대에서는 앞으로 배우게 되겠지만

여기에서 말하고 싶었던건 NN이 레고블럭과 같다는 것이고.
Linear Classifier가 그것의 기본이 블럭이 된다는 것입니다.




<p align="center">
<img alt="image" src="">
</p>



하지만 2강 에서 하기엔 너무 재밌는 내용이니까
우리는 다시 CIFAR-10으로 잠시 돌아가 봐야 겠습니다.

[웃음]

CIFAR-10이 50,000여개의 트레이닝 샘플이 있고

각 이미지는 32x32 픽셀을 가진
3채널 컬러 이미지라는 것을 다시 상기시켜 봅시다.


<p align="center">
<img alt="image" src="">
</p>




Linear classification에서는 K-NN과는
조금은 다른 접근 방법을 이용합니다.

Linear classifier는 "parametric model"의
가장 단순한 형태입니다.

parametric model에는 두 개의 요소가 있습니다.

입력 이미지가 있습니다.
왼쪽에 보이는 고양이 이미지입니다.

이 입력 이미지를 보통 "X" 로 씁니다.

파라미터, 즉 가중치는 문헌에 따라 다르지만
"W"라고도 하고 세타(theta)라도고 합니다.

이제 우리는 어떤 함수를 작성해야 하는데
이 함수는 data X와 parameter W를 가지고

10개의 숫자를 출력합니다. 이 숫자는
CIFAR-10의 각 10개의 카테고리의 스코어입니다.

이 스코어를 해석해 보자면, "고양이"의 스코어가 높다는 건
입력 X가 "고양이"일 확률이 크다는 것을 의미합니다.

질문 있으십니까?

[학생이 질문]

잘못들었습니다?

[학생이 질문]

질문은 "여기에서 3 무엇인지" 입니다.
(32 x 32 x 3 의 3 이 무엇인지 질문)

3은 Red, Green, Blue 3 채널을 의미합니다. 보통은 컬러
이미지를 다룹니다. 컬러 정보는 버리기 아까운 유용한 정보입니다.

앞서 K-NN은 파라미터가 없었습니다.

그저 전체 트레이닝 셋을 가지고 있었고
모든 트레이닝 셋을 Test time에 사용했습니다.

하지만 parametric approach 에서는
트레이닝 데이터의 정보를 요약합니다.

그리고 그 요약된 정보를 파라미터 W에 모아줍니다.

이런 방식을 사용하면 Test time에서 더이상
트레이닝 데이터가 필요하지 않습니다.

Test time에서는 파라미터 W만 있으면 그만입니다.

이 방법은 핸드폰과 같은 작은 디바이스에서 모델을
동작시켜야 할 때 아주 효율적입니다.

그러니 딥러닝은 바로 이 함수 F의 구조를
적절하게 잘 설계하는 일이라고 할 수 있습니다.

어떤 식으로 가중치 W와 데이터를 조합할지를
여러가지 복잡한 방법으로 고려해 볼 수 있는데

이 과정들이 모두 다양한 NN 아키텍쳐를 설계하는 과정입니다.




<p align="center">
<img alt="image" src="">
</p>




가중치 W와 데이터 X를 조합하는 가장 쉬운 방법은
그냥 이 둘을 곱하는 것입니다.

이 방법이 바로 Linear classification 입니다.

F(x,W) = Wx 입니다.
아주 쉬운 식입니다.



<p align="center">
<img alt="image" src="">
</p>



그러면 이제 이들의 차원을 한번 알아보겠습니다.

입력 이미지는 32 x 32 x 3 이였습니다. 이 값을 길게
펴서 열 벡터로 만들면 3,072-dim 벡터가 됩니다.

3072-dim열 벡터가 10-classes 스코어가 되어야 합니다.

이는 10개 카테고리에 해당하는 각 스코어를 의미하는
10개의 숫자를 얻고 싶은 것입니다.

따라서 행렬 W는 10 x 3072가 되어야 합니다.

이 둘을 곱하면 10-classes 스코어를 의미하는
10 x 1 짜리 하나의 열 벡터를 얻게 됩니다.




<p align="center">
<img alt="image" src="">
</p>





그리고 가끔은 "Bias" 을 보게 될텐데
가끔 Bias term도 같이 더해주기도 합니다.

Bias term은 10-dim 열 벡터입니다.
Bias term은 입력과 직접 연결되지 않습니다.

대신에 "데이터와 무관하게"
특정 클래스에 "우선권"을 부여합니다.

가령 데이터셋이 분균형한 상황을 생각해 볼 수 있습니다.
고양이 데이터가 개 데이터보다 훨씬 더 많은 상황입니다.

이 상황에서는 고양이 클래스에 상응하는
바이어스가 더 커지게 됩니다.



<p align="center">
<img alt="image" src="">
</p>



그러면 이제 이 함수가 어떻게 동작하는지
그림으로 살펴보겠습니다.

이 그림을 보면 왼쪽에 입력 이미지가 있습니다.
2x2 이미지이고 전체 4개의 픽셀입니다.

이 Linear classifier는 2x2 이미지를 입력으로 받고
이미지를 4-dim 열 벡터로 쭉 폅니다.

10개 클래스를 모두 슬라이드에 담을 수 없어서 이 예제에서는
고양이, 개, 배 이렇게 세가지 클래스만 보겠습니다

가중치 행렬 W는 4x3 행렬이 됩니다.

이런 식으로, 입력은 픽셀 4개고
클래스는 총 3개 입니다.

그리고 추가적으로 3-dim bias 벡터가 있습니다.

bias는 데이터와 독립적으로
각 카테고리에 연결됩니다.

"고양이 스코어" 는 입력 이미지의 픽셀 값들과 가중치 행렬을
내적한 값에 bias term을 더한 것입니다.

이러한 관점에서 Linear classification은
템플릿 매칭과 거의 유사합니다.

가중치 행렬 W의 각 행은 각 이미지에 대한 템플릿으로 볼 수  있고
그 행 벡터와 이미지의 열벡터 간의 내적을 계산하는데,

여기에서 내적이란 결국 클래스 간 탬플릿의
유사도를 측정하는 것과 유사함을 알 수 있습니다.

bias는 데이터 독립적으로  각 클래스에
scailing offsets을 더해주는 것입니다.




<p align="center">
<img alt="image" src="">
</p>



템플릿 매칭의 관점에서
Linear classification 해석해보면

가중치 행렬 W의 한 행을 뽑아서
이를 이미지로 시각화 시켜 보면

Linear classifier가 이미지 데이터를 인식하기 위해서
어떤 일을 하는지 짐작할 수 있습니다.

이 예제에서는 Linear classifier가 이미지를 학습합니다.

슬라이드 하단의 이미지는 실제로
가중치 행렬이 어떻게 학습되는지를 볼 수 있습니다.

CIFAR-10의 각 10개의 카테고리에 해당하는
행 벡터를 시각화시킨 것입니다.

이렇게 시각화된 이미지를 살펴보면
어떤 일이 일어나는지 알아볼 수 있습니다.

가령 맨 왼쪽의 이미지는 비행기 클래스에 대한
템플릿 이미지입니다.

이 이미지는 전반적으로 파란 색입니다.
가운데에는 어떤 물체가 있는 것 같군요

이 이미지를 해석해보면 Linear classifier가 비행기를
분류할 때 푸르스름한 것들을 찾고 있는 것 같습니다.

이러한 특징들이 이 분류기가 비행기를 더 잘 찾도록
도와준다고 해석해 볼 수 있습니다.

바로 옆에 있는 자동차의 예시도 한번 보겠습니다.

중앙은 불그스름하고 상단은 푸르스름합니다. 자동차의
앞유리 같군요. 하지만 조금 이상합니다.

실제 자동차 처럼은 보이지가 않습니다.
어떤 자동차도 이렇게 생기진 않았죠.

Linear classifier의 문제 중 하나는 각 클래스에 대해서
단 하나의 템플릿만을 학습하다는 것입니다.

한 클래스 내에 다양한 특징들이 존재할 수 있지만,
모든 것들을 평균화 시키기 때문에

다양한 모습들이 있더라도 각 카테고리를 인식하기
위한 템플릿은 단 하나밖에 없습니다.

이 문제점은 말(馬)을 분류하는 템플릿을 살펴보면
여실히 드러나는 대목입니다.

바닥은 푸르스름해 보입니다. 보통 말이 풀밭에 서 있으니
템플릿이 바닥을 푸르스름하게 학습한 것입니다.

그런데 유심히 살펴보면 말의 머리가 두 개 입니다.
각 사이드 마다 하나씩 달려 있습니다.

머리 두개 달린 말은 존재하지 않습니다.

하지만 Linear classifier가 클래스 당 하나의 템플릿밖에
허용하지 않으므로 이 방법이 최선입니다.

하지만 Neural Network같은 복잡한 모델이라면
조금 더 정확도 높은 결과를 볼 수 있을 것입니다.

클래스 당 하나의 템플릿만 학습 할 수 있다는
것과 같은 제약조건이 없다면 말이죠




<p align="center">
<img alt="image" src="">
</p>



Linear classifier을 또 다른 관점으로 해석할 수 있습니다.
이미지를 고차원 공간의 한 점으로 보는 것입니다.

각 이미지을 고차원 공간의 한 점이라고 생각해 봅시다

Linear classifier는 각 클래스를 구분시켜 주는
선형 결정 경계를 그어주는 역할을 합니다.

가령 왼쪽 상단에 비행기의 예를 볼 수 있습니다.

Linear classifier는 파란색 선을 학습해서
비행기와 다른 클래스를 구분할 수 있습니다.

임의의 값으로 초기화된 모델이 데이터 들을 잘 분류하려고
노력하는 모습을 지켜보면 아주 재밌습니다.



<p align="center">
<img alt="image" src="">
</p>







하지만 이미지가 고차원 공간의 하나의 점 이라는
관점으로 해석해보면

Linear classification이 직면할 수 있는 문제가 있습니다.

이 Linear classifier를 망가뜨릴 수 있는 예제를
만드는 일은 생각보다 어렵지 않습니다.

맨 왼쪽 그림은 두 개의 클래스를 가진 데이터 셋입니다.

데이터가 조금은 인위적일 수 있지만, 아무튼 데이터셋에는
파랑/빨강 두 개의 카테고리가 있습니다.

파랑색 카테고리는 0보다 큰 픽셀이 홀수 개 인 경우입니다.
(예 : [3,-1] 이면 0보다 큰 수 : 3 (1개, 홀수개) -> 파랑)

반면 0보다 큰 수가 짝수 개면 빨간 카테고리로 분류합니다.

좌표 평면에 이 같은 규칙으로 그려보면
두 개의 사분면에는 파란 클래스,

두 사분면에는 빨간색 클래스를 볼 수 있습니다.
(예:(1.1):2(짝), (-1,1):1(홀), (-1, -1):0(짝))

이 데이터를 선 하나로 분류할 방법은 없습니다.
Linear classifie로는 풀기 힘든 문제입니다.

이 예제가 너무 인위적으로 보일 수 있지만 그렇지 않습니다

가령 이렇게 픽셀 갯수를 세는 대신에 영상 내 동물이나 사람의 수가
홀/짝수 인지를 분류하는 문제일 수 있습니다.

홀/짝수를 분류하는 것과 같은
반전성 문제(parity problem)는

일반적으로 Linear classification으로 풀기 힘든 문제입니다.

Linear classifier로는 풀기 힘든 또 하나는
Multimodal problem입니다. (맨 오른쪽)

맨 오른쪽 이미지를 보면 파란색이 분포하는
세 개의 섬들이 있습니다.

그 밖의 빨간색은 전부 다른 카테고리에 속합니다.

앞서 소개드린 말(馬)의 예시처럼
Multimodal problem은 언제든 실제로 일어날 수 있습니다.

왼쪽 머리가 하나의 섬이 될 수 있고
오른쪽 머리가 또 하나의 섬이 될 수 있습니다.

섬이 두 개인데 선을  하나만 긋는 것은
그닥 좋은 방법이 아닌 것입니다.

Multimodal data라면 한 클레스가
다양한 공간에 분포할 수 있으며

이 문제는 Linear classifier로는 풀 수 없습니다.

이처럼 Linear classifier에는 문제점이 일부 있긴 하지만
아주 쉽게 이해하고 해석할 수 있는 알고리즘입니다.

과제1 에서 Linear classifier를 구현하시게 될 것입니다.



<p align="center">
<img alt="image" src="">
</p>




이번 시간을 요약해보면 지금까지 Linear classifier의
수식을 살펴보았습니다.

Linear classifier가 단순히
행렬과 벡터 곱의 형태라는 것을 배웠고

템플릿 매칭과 관련이 있고, 이 관점에서 해석해 보면
각 카테고리에 대해 하나의 템플릿을 학습한다는 것을 배웠습니다.

그리고 가중치 행렬 W 를 학습시키고 나면
새로운 학습 데이터에도 스코어를 매길 수 있습니다.

오늘은 어떻게 가중치 행렬 W를 구할 수 있는지는
배우지 않았습니다.

오늘은 Linear classifier가 어떻게 생겼고, 어떻게
동작하는지만 가볍게 알아보았습니다.

지금 보이는 슬라이드가 우리가 다음 시간에 다룰 내용입니다.

다음 강의에서는 적절한 가중치 행렬 W를 고르는 법과
다양한 알고리즘들에 대해서 다뤄보도록 하겠습니다.

그리고 더 나아가 비용함수, 최적화,
ConvNet에 대해서 배울 것입니다.

다음 주에 배울 내용들이죠

오늘은 여기까지 하겠습니다.
