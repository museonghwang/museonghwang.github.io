---
layout: post
title: CS231n Lecture7 Review
category: CS231n
tag: CS231n
---

[![Hits](https://hits.sh/museonghwang.github.io.svg?view=today-total&style=for-the-badge&label=Visitors&color=007ec6)](https://hits.sh/museonghwang.github.io/)

<br>

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html) 강의와 2022년 슬라이드를 바탕으로 작성되었습니다.




<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185549686-4216e74e-998d-4f5c-852b-d74283a10cc5.png">
</p>

<br>





## Last time: Activation Functions

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185551956-31338de7-ba7b-4993-af42-6e23e8e9c684.png">
</p>

* 지난시간에 Nerural networks를 학습 시킬 때 필요한 여러가지 중요한 것들을 배웠습니다.
* 다양한 **<span style="background-color: #fff5b1">Activation Function</span>** 과 각각의 특성이 존재했는데, 과거에는 sigmoid를 썼지만 Gradients Vanishing 문제때문에 요즘은 대부분 ReLU를 씁니다.

<br>

## Last time: Weight Initialization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185551978-1505b59d-9085-4fe9-a9b6-7711a8fe75a0.png">
</p>

* **<span style="background-color: #fff5b1">Weight Initialization</span>** 에 대해서도 배웠습니다.
    * 가중치가 지나치게 작으면 activation이 사라지는데, 작은 값이 여러 번 곱해지기 때문에 점점 0이 되어, 결국 모든 값이 0이 되고 학습은 일어나지 않습니다.
    * 반면에 가중치가 너무 큰 값으로 초기화되면 그 값이 또 계속 곱해질 것이고 결국은 폭발합니다. 이 경우에도 학습이 일어나지 않을 것입니다.
* Xavier/MSRA(HE) Initialzation 같은 방법으로 초기화를 잘 시켜주면 Activation의 분포를 좋게 유지시킬 수 있습니다.
* 특히 Network가 깊어지면 깊어질수록 가중치를 더 많이 곱하게 되기 때문에 가중치 초기화와 활성함수는 더 중요합니다.

<br>

## Last time: Data Preprocessing

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185551995-9a14cbb2-2c81-48c8-a5df-fcb07cbf12fd.png">
</p>

* **<span style="background-color: #fff5b1">여러가지 Data Preprocessing</span>** 기법이 있었고, image data는 주로 zero-mean을 주로 사용합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552019-ac6538d6-73f1-4d97-8076-d8d7f94c66fd.png">
</p>

* **<span style="background-color: #fff5b1">왜 normalization가 중요한지에 대한 직관</span>**
* Linear Binary classification 문제(빨간/파란 점들을 나누는 것)를 푼다고 가정하겠습니다.
    * 왼쪽
        * **<span style="color:red">not normalized/centered data</span>** 입니다.
        * classification 자체는 가능하지만, 선이 조금만 움직여도 classification이 잘 되지 않습니다.
        * 즉, 손실 함수가 아주 약간의 가중치 변화에도 엄청 예민합니다.
        * **<span style="background-color: #fff5b1">Loss가 파라미터에 너무 민감하기 때문에, 동일한 함수를 쓰더라도 학습 시키기 아주 어렵습니다.</span>**
    * 오른쪽
        * **<span style="color:red">zero-center data</span>**, **<span style="color:red">Unit variance</span>** 로 만들어 준 경우 입니다.
        * 선이 조금만 움직여도 손실 함수는 이런 가중치의 변동에 덜 민감합니다.
        * 이 경우 **<span style="background-color: #fff5b1">최적화가 더 쉬우며, 학습이 더 잘됩니다.</span>**

normalization은 Linear classification의 경우에만 국한되는 것이 아니라, Neural network 내부에도 다수의 linear classifer가 있다고 생각할 수 있는데, 이 경우 Neural network의 입력이 zero-centered가 아니고 Unit variance가 아닌 경우라면 레이어의 Weight matrix가 아주 조금만 변해도 출력은 엄청 심하게 변하게 됩니다. 이는 학습을 어렵게 합니다.

<br>

## Last time: Batch Normalization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552041-6a9e0aaa-445f-4a61-adb6-8bb21c5c332e.png">
</p>

* Normalization이 엄청 중요하다는 것을 알고있기 때문에, **<span style="background-color: #fff5b1">batch normalization</span>** 도 배웠습니다.
* activations이 zero mean과 unit variance가 될 수 있도록 레이어를 하나 추가하는 방법이었습니다.
    * forward pass 시에 미니배치에서의 평균과 표준편차를 계산해서 Normalization을 수행했습니다.
    * 그리고 레이어의 유연한 표현성(expressivity)을 위해서 scale, shift 파라미터를 추가했습니다.

<br>

## Last time: Babysitting Learning

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552057-dee94ad9-f572-4ddb-b8a6-7eed9b670472.png">
</p>

* **<span style="background-color: #fff5b1">학습 과정(Loss curve가 어떻게 보여야 하는지)</span>** 을 다루는 방법도 배웠습니다.
* 위 슬라이드를 해석해보면 Training set의 성능을 계속 올라가며 Loss는 계속 내려갑니다. 하지만 validation은 침체하고 있습니다.
    * 위 상황은 overfititing.
    * 추가적인 regularization이 필요합니다.

<br>

## Last time: Hyperparameter Search

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552070-ccc79b32-f6df-4e4a-ae26-52066b4bb1a6.png">
</p>

* **<span style="background-color: #fff5b1">hyperparameter search</span>** 도 배웠습니다.
* 네트워크에는 무수히 많은 하이퍼파라미터가 존재하며, 이를 올바르게 잘 선택하는 것은 상당히 중요합니다.
* grid search와 random search를 배웠으며, 이론상 random search가 더 좋았습니다.
    * 왜냐하면 성능이 특정 하이퍼파라미터에 의해 크게 좌우될 때 그 파라미터를 좀 더 넓은 범위로 탐색할 수 있기 때문입니다.
* 그리고 하이퍼파라미터 최적화 시에 coarse search 이후 fine search를 합니다.
    * **<span style="color:red">coarse search</span>**
        * 처음에는 하이퍼파라미터를 가능한 최대한 넓은 범위를 설정해서 찾습니다.
        * 그 범위가 하이퍼파라미터 범위의 끝에서 끝까지 다 살펴볼 수 있도록 할수록 좋습니다.
        * Interation도 작게 줘서 학습시켜봅니다.
        * 그리고 결과가 좋은 범위로 좁히는 것입니다.
    * **<span style="color:red">fine search</span>**
        * iterations를 조금 더 돌면서 더 작은 범위를 다시 탐색합니다.
    * 적절한 하이퍼파라미터를 찾을 때 까지 이 과정을 반복합니다.

<br>
<br>





# Gradient descent for optimization

* Fancier optimization을 살펴보기전에 Gradient descent를 다시 짚어보고, 문제점을 살펴보겠습니다.
* 이전에 **<span style="background-color: #fff5b1">Loss function에 대한 정의 및 역할이 제시</span>** 되었습니다.
    * 최적의 $W$ 를 찾아서 classifier가 이미지들을 잘 분류하고 있는지 검사를 해야하는데, 즉, $W$ (weight)가 좋은지 아닌지 정량화 할 수 있는 기준이 필요했기에 Loss function이 등장했습니다.
    * 손실 함수(Loss Function)는 현재 분류기(classifier)가 얼마나 좋은지를 알려줍니다. 다르게 표현하면 현재의 $W$ 가 얼마나 BADNESS한지 를 알려주는 것입니다.
* **<span style="background-color: #fff5b1">기존의 Loss function은 문제점을 가지고 있었습니다.</span>**
    * training data에 대해 좋은 성능을 만들려고 합니다.
    * 하지만 우리가 원하는것은 test data에 대한 일반화입니다.
* **<span style="background-color: #fff5b1">이러한 문제점은 Overfiiting으로 이어집니다.</span>**
    * 즉 모델이 training data에 대해서만 잘 수행하고, unseen data에 대해 낮은 성능을 보이는 현상입니다.
* **<span style="background-color: #fff5b1">Overfitting을 방지하기 위해 Data loss term에 Regularization term을 추가하였습니다.</span>**
    * 즉, Simpler Models을 선호하게 하여 Overfit을 방지합니다.
    * 일반적으로 L2 regularization를 사용합니다.
    * 더 복잡한 regularization은 Dropout, Batch normalization, Cutout, Mixup, Stochastic depth 등등 여러가지 방법론이 존재합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185561594-c01cd0d8-1446-4d92-8c33-72bdc54cd8b9.png">
</p>

정리하면, **<span style="color:red">nice(좋은)한 $W$를 찾기 위한 Loss function은 training data에 맞는 Data Loss과 Overfitting을 방지하기 위한 regularization으로 구성됩니다.</span>**

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185562463-2a0fcbf4-25f7-4388-a206-4907b45b1170.png" style="zoom:30%;">
</p>

* 그리고 **<span style="color:red">최종적으로 $L(W)$ 함숫값을 최소화하는 최적의 W를 구하는 것이 목적입니다.</span>**
* 따라서, 최적의 **<span style="background-color: #fff5b1">$W$</span>** 는 **<span style="background-color: #fff5b1">$W^*\ =\ argmin_wL(W)$</span>** 의 식으로 구합니다.
    * argmin : minimizer
    * $L(W)$를 최솟값으로 만드는 $W$를 $W^*$로 정의합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185564759-f0057690-67e6-462e-b05d-eadddf98ef36.png">
</p>

* 그렇다면 이 **<span style="color:red">$W_*$</span>** 를 어떻게 찾을까?
    * 최적의 $W$ 값을 찾기는 어렵습니다. 하지만 찾고자 하는 방법으로 2가지가 있습니다.
        1. Random search
        2. slope를 따라내려가는 방법
    * Random search는 랜덤으로 $W$ 값의 후보를 정해서 함숫값을 구하는데, 즉, 매번 $W$ 를 조금씩 개선시켜 loss를 낮추고자 하는 것입니다.
        * 좋은 방법이 절대 아니며, 성능또한 좋지 않고, 차원의 저주에 빠질 수 있습니다.
* 위 질문은 **<span style="color:red">gradient descent</span>** 로 이어집니다.
    * **<span style="background-color: #fff5b1">$L(W)$ 는 주로 전체 개형을 알 수 있는 함수가 아닙니다.</span>**
    * 개형은 알지 못하고, $W=W0$ 라는 점이 주어졌을 때 그 근접한 곳만 알 수 있습니다.
    * 여기서 할 수 있는 방법으로 **<span style="background-color: #fff5b1">slope를 따라내려가는 방법</span>** 이 있습니다.
        * 경사를 따라내려가는 방법은 곧 함수에서의 derivative(미분)를 따라가는 것입니다.
        * 그래디언트의 기하학적 의미: 그래디언트 방향은 함수가 가장 증가하는 방향.
        * 다르게 이야기하면, **<span style="background-color: #fff5b1">그 점에서 loss function이 가장 감소할 수 있는 방향을 의미하며, 따라서 negative gradient의 방향으로 점점 가면 최소점에 도달할 것</span>** 입니다.
* 그렇다면 함수의 개형을 알지 못하는 상황에서 어디가 최적점인지 어떻게 알 수 있을까?
    * 그냥 내려가봐야 합니다.
    * 그래디언트를 구해서 함수가 감소하는 방향으로 가는 것이 절대 전체적인 최적의 global minimum으로 가는 것을 보장하지는 않습니다.
    * 즉, 전체 shape에 대해 전혀 알지 못하지만, 내리막길을 따라가면 낮은 곳으로 가게 될 것이라고 믿습니다.(최적점인지에 대한 확신은 없습니다.)

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185565010-27a61b90-ef56-4394-8463-8bcc1e40793f.png">
</p>

* 즉, 그래디언트 계산법에는 2가지가 있습니다.
    * **<span style="background-color: #fff5b1">수치적 그래디언트</span>**: 근삿값을 계산하며, 계산이 느리지만, 쓰기 쉽습니다.
    * **<span style="color:red">해석적 그래디언트</span>**: 정확한 값을 계산하고, 빠르지만, error가 발생하기 쉽습니다.
* 실제로 그래디언트를 계산할때는, 항상 해석 그래디언트를 활용하면서 **<span style="color:red">gradient check를 거칩니다.

<br>
<br>





# (Full-Batch)Gradient descent

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185568772-c104c5c7-4d95-4be7-9e18-0e771ec95386.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185580899-f33c0509-3ccb-4b2a-b515-b4f5d5e32a0b.png">
</p>

* $L(W)$ 의 가장 낮은 점(최적점, global minimum)을 향해서 가는 방법 중 gradient negative한 방향을 따라가는 알고리즘인 **<span style="color:red">Grdient descent</span>** 를 소개합니다.
* Gradient descent는 **<span style="background-color: #fff5b1">negative gradient 방향으로 이동하면서, 계속해서(t=0, 1, 2, . . .반복) $W$ 값을 update시키는 방법</span>** 입니다.
* 위의 간단한 알고리즘은 우리가 initialized_weights를 통해 아무데서나 시작하고 고정된 num of iteration (num_steps)만큼 루프를 반복하며 매 step마다 현재 위치의 gradient를 계산하고 negative gradient direction으로 조금씩 이동하게된다는 코드입니다.
* Gradient descent를 구현할 때 필요한 **<span style="color:red">hyperparameter</span>** 로 다음과 같습니다.
    * Weight 초기값을 설정하는 방법: 초기값이 어디냐에 따라 수렴하는 위치가 달라집니다.
    * 얼마나 반복할 것인지: steps의 수가 너무 크면, 시간이 소요되고, 너무 작으면, 충분히 내려가기전에 알고리즘이 끝날 수 있기때문에 적당히 설정하는 것이 중요합니다.
    * Learning rate: 크게 설정하면, 너무 넓게 뛰어, 오히려 loss값이 높은 곳으로 갈 수 있고, 작게 설정하면 최적점까지 가는 step이 너무 오래 걸립니다.
* 위 슬라이드의 우측 이미지는 Gradient descent의 기하학적인 해석입니다.
    * 현재 $L(W)$ 함수의 형태는 2차원 convex함수로, Negative gradient방향을 따라 이동하면서 $W$ 를 update시키면 optimal로 갈 수 있습니다.

<br>
<br>





# (mini-Batch)Stochastic Gradient descent

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185587631-a703695a-b728-4d71-8d2c-15f5cde5217c.png">
</p>

* 지금까지 설명한 Gradient Descent방식은 모든 트레이닝 데이터를 계산에 활용하는 Full-Batch Gradient Descent입니다.
* **<span style="background-color: #fff5b1">Training data 사이즈가 매우 크다면, 파라미터 W를 한 번 update하기 위해 모든 트레이닝 데이터를 활용하는 것은 비용측면에서 효율적이지 못합니다.</span>**
* Full-batch GD방법은 시간이 너무 오래 걸리기때문에, 트레이닝 셋의 모든 N개를 실행하지않고, 트레이닝셋에서 샘플을 랜덤으로 추출하여 실행하는 **<span style="color:red">Stochastic Gradient Descent(SGD)</span>** 방법을 주로 활용합니다.
    * 트레이닝셋에서 랜덤으로 추출한 것을 미니배치라고 부르며, 미니배치 사이즈는 주로 32, 64, 128개로 정합니다.
* SGD를 활용하면 미리 정해야 할 **<span style="color:red">hyperparameter</span>** 가 더 늘어납니다.
    * Weight 초기값을 설정하는 방법
    * 얼마나 반복할 것인지
    * Learning rate
    * Batch size: 미니배치의 사이즈
    * Data sampling: 데이터 샘플링하는 방법

원래 Stochastic Gradient descent는 mini-batch를 사용하는 것이 아닌 전체 training set에서 랜덤하게 뽑은 단 1개의 데이터의 gradient를 계산하고 weight update시키는 방식을 training set 크기만큼 반복하는 것이라 이러한 random성 때문에 stochastic이라는 이름이 붙은 것 입니다. 하지만 mini-batch gradient descent가 굉장히 보편화되며 SGD라는 용어가 mini-batch gradient descent를 의미하는 경우가 많아졌습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185587731-f3f9e741-aaa3-4f46-b44b-6ca98679cbf2.png">
</p>

* SGD에서는 loss function을 **<span style="background-color: #fff5b1">확률론적으로 접근하기 때문에 stochastic</span>** 이라는 이름이 붙여졌습니다. 우리가 **<span style="background-color: #fff5b1">data set을 확률분포로 부터 샘플링된 것으로 생각했을때 우리는 loss function을 모든 가능한 sample들에 대한 expectation으로 생각</span>**할 수 있으며, 수식과 같이 표현 할 수 있습니다.
    * mini-batch 데이터는 임의의 확률분포 $P$ 를 따르는 확률변수 $X$, $Y$ 를 따릅니다.
    * $∇_WL(W)$ : 우리가 모르는 $P$ 라는 분포에서 N개의 데이터를 샘플링했을 때, 그 샘플링된 데이터에서 그래디언트를 계산하여 모집단 평균 그래디언트를 sample 평균 그래디언트로 근사합니다.

<br>
<br>





# Optimization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185590450-6455863b-356c-4890-8618-e26d6ea2840e.png" style="zoom:60%;">
</p>

* 즉 **<span style="background-color: #fff5b1">Neural network에서 가장 중요한 것</span>** 은 바로 **<span style="color:red">Optimization Problem</span>** 입니다.
    * Nerwork의 가중치에 대해서 Loss function를 정의해 놓으면 이 Loss function은 그 가중치가 얼마나 좋은지 나쁜지를 알려줍니다.
    * 그리고 Loss function이 가중치에 대한 "산(landscape)"이라고 상상해 볼 수 있습니다.
* 오른쪽 이미지인 2차원의 문제를 두 개의 가중치 W_1과 W_2를 최적화 시키는 문제라고 생각해봅니다.
    * X/Y축은 두 개의 가중치를 의미하며, 각 색깔은 Loss의 값을 나타냅니다.
    * 우리의 목적은 가장 붉은색인 지점을 찾는 것입니다. **<span style="color:red">즉 가장 낮은 Loss를 가진 가중치를 찾는 것</span>** 입니다.

가장 간단한 최적화 알고리즘인 Stochastic Gradient Descent를 이용해 미니 배치 안의 데이터에서 Loss를 계산하고, "Gradient의 반대 방향"을 이용해서 parameter vector를 업데이트합니다. 이 단계를 계속 반복하면 결국 붉은색 지역으로 수렴할 것이고 Loss가 낮아질 것입니다.

정리하면, **<span style="color:red">Optimization</span>** 은 **<span style="background-color: #fff5b1">업데이트(학습)를 통해 손실함수(Loss function)의 가장 낮은 곳의 $W$ 값 까지 도달하는 것이 목표</span>** 이고, 이것을 **<span style="background-color: #fff5b1">어떻게 가장 효율적인 방법으로 도달할 수 있느냐에 대한 방법론</span>** 입니다.

<br>
<br>





# Optimization: Problem #1 with SGD

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185744611-e65fd2c4-849a-4411-9bc0-97f4a52a3ac2.png">
</p>

* 하지만, Stochastic Gradient Descent에는 문제가 있습니다.
    * 위 슬라이드와 같이 가로 방향으로 긴 형태(like taco shell)의 손실함수에서는, 가로 축으로의 이동보다 세로 축으로의 이동이 더욱 큰 영향을 주게 됩니다.
        * 이 경우, loss function의 경사가 수직 방향으로의 gradient는 매우 크고, 수평 방향으로의 gradient는 매우 작아, 수평 축의 가중치는 변해도 Loss가 아주 천천히 줄어듭니다.
    * 즉, 수평 방향의 가중치(W_1)가 변하더라도 Loss는 아주 천천히 줄어들기 때문에, 수평 방향의 가중치(W_1)보다 수직 방향의 가중치(W_2) 변화에 더욱 민감하게 반응할 것입니다.
    * 즉 업데이트가 잘 안되는 경우이며, 이를 poor conditioning 라고 부른다.
* 현재 지점에서 Loss는 bad condition number를 지니고 있다고 말할 수 있습니다.
    * condition number: 입력값의 작은변화에 대한 출력값의 변화의 정도를 측정하는 지표로 시스템이 민감한 정도를 정량적으로 보여주는 값을 뜻합니다.
    * 즉 이 지점의 Hessian maxrix의 최대/최소 singular values값의 비율이 매우 안좋다는 뜻입니다.
    * 즉, $∇^2L(W)$ 의 $\frac{largest\ eigenvalue}{smallest\ eigenvalue}$
    * ex. $L(W) = \frac{W_1^2}{64} + W_2^2$
    * $∇^2L(W) = \begin{bmatrix} \frac{1}{32} & 0 \\ 0 & 2 \end{bmatrix} $
    * condition number = $\frac{2}{\frac{1}{32}} = 64$

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185744478-f89173c2-b0bd-48f6-8e7c-1c5524bfab88.png">
</p>

* 이런 상황에서 SGD으로 학습이 되는 모습을 생각해볼때, gradient의 방향이 고르지 못하기 때문에 위 그림과 같이 지그재그로 수렴하는 형상을 띄게 됩니다.
    * 즉 step에 따른 Weight of matrix 의 변화가 zigzag pattern or 지저분하게(nasty) 보이며 변화 할 수 있습니다.
    * 이는 바람직하지 않으며, 더 많은 step이 소요되며, 고차원의 공간에서 주로 발생합니다.
    * 또한 이 때문에 SGD는 Full Batch보다 Overshoot문제에도 더 취약합니다.
* 위의 예에서는 단순히 2차원이지만, 실제로는 가중치가 수억개일 수 있습니다.
    * 이때는 수억개의 방향으로의 불균형한 방향이 존재할 수 있으며, 수억개의 방향으로 움직일 수 있으므로, SGD는 잘 동작하지 않을 것입니다.
    * 고차원 공간에서 발생하는 이런 문제는 실제로도 큰 문제가 됩니다.

<br>





# Optimization: Problem #2 with SGD

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185747069-db707f35-8c15-4ac9-8fbc-755213465321.png">
</p>

* SGD의 또다른 문제는 Local Minima와 Saddle Point 입니다.
    * Local minima는 업데이트 중에 작은 valley를 만나 gradient가 0이 되어 학습을 멈추게 되는 문제입니다.
        * 즉, global minimum point 가 아닌 local minimum point 입니다.
    * Saddle point는 gradient가 0이 되는 지점에서 학습을 멈추는 문제입니다.
    * 즉, 최적이 아닌 위치에서 그래디언트 값이 0이 되면, 여기에 갇혀서 더 이상 W값이 업데이트 되지 못 합니다.
* 위 그림과 같이 1차원의 예에서는 local minima가 더욱 심각해 보이지만, 고차원 공간에서는 그 반대입니다.
    * 고차원 공간에서 Saddle point는 어떤 방향은 loss가 증가하고 몇몇 방향은 loss가 감소하고 있는 지점으로 생각할 수 있는데, 수억차원에서 생각해보면 이는 거의 모든 곳에서 발생한다고 할 수 있습니다.
    * 반면, 고차원 공간에서 local minima는 수억개의 방향을 계산했는데 이 방향이 모두 loss가 상승하는 방향인 경우이므로, 매우 드물게 발생합니다.
* 매우 큰 규모의 신경망 모델에서는 local minima보다 saddle point에 더욱 취약한 것으로 알려져 있습니다.
    * Saddle point에서 gradient가 0이 되는 것도 있지만, saddle point 근처에서 gradient가 아주 작아지기 때문에 업데이트가 느려져서 문제가 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185747920-5b457e04-642b-49a1-8683-01962d067c32.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185747834-24184899-6dbb-4c2b-b30f-e408484151b5.png">
</p>

* 또한, SGD에서(데이터의 크기가 매우 커서) mini-batch로 gradient를 계산할때 small estimate of full data set만을 사용하기 때문에 모든 step에서의 gradient가 minima로 향하는 올바른 direction과는 상관관계가 없기에 그림에서 볼 수 있듯이 noisy 합니다.
    * 이는 mini-batch의 데이터만으로 실제 loss와 gradient를 추정하는 것입니다.
    * 즉 매번 정확한 gradient를 얻을 수가 없다는 것을 의미하며, 부정확한 추정값(noisy estimate)인 gradient를 얻게 된다는 문제가 있습니다.
* 따라서, 위 슬라이드의 오른쪽 그림과 같이 손실함수 공간을 비틀거리면서 minima로 수렴하기 때문에 학습 시간이 오래걸리게 됩니다.

<br>





# Next Optimizer

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185748116-b2e63bce-e678-4b19-be5b-de1b951f3b6d.png">
</p>

* 이러한 SGD의 여러가지 문제점을 해결하기 위해 여러 개의 optimizer들이 차례로 등장하게 됩니다.
* 개선된 optimizer들은 각각의 특징들이 있는데, 크게 방향을 중심으로 하느냐, 보폭을 중심으로 하느냐로 나뉩니다.
* 지금부터 SGD를 개선한 optimizer를 살펴보겠습니다.

<br>
<br>





# SGD + Momentum

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185775788-24aab157-435b-4208-a40f-3f74c79e51a6.png">
</p>

* SGD의 여러 문제점들을 해결하는 간단한 방법은 SGD에 momentum term을 추가하는 것입니다.
* SGD + Momentum의 아이디어는 단순히 velocity를 유지하는 것입니다.
    * 즉, 현재 mini-batch의 gradient 방향만 고려하는 것이 아니라 velocity도 같이 고려하여, gradient descent가 되는 과정에서 생기는 속도(velocity)를 어느 정도 유지하자는 의미입니다.
    * $ρ$ : momentum의 비율(velocity의 영향력) or 속도(velocity)를 얼마나 고려할 것인가의 비율을 나타냅니다.
        * 보통 0.9 or 0.99 로 설정합니다.
    * $V_t$ : velocity
* 즉, Momentum 최적화는 이전 gradient가 얼마였는지를 상당히 중요하게 생각합니다.
    * Momentum의 핵심 idea는 weight를 업데이트 할 때 이 전에 계산한 gradient도 반영을 해주자는 것으로 다르게 말하면, $V_t$ (현재 속도)에 그래디언트가 누적되면서,과거의 그래디언트는 점점 잊혀지는 알고리즘입니다.
    * 식을 살펴볼때, 매 스텝마다 gradient가 동일한 부호를 가지면 v의 절댓값은 계속 증가할 수 밖에 없습니다. v가 증가함에 따라 x의 변화폭이 커지고, 이는 즉 같은 방향으로 이동할수록 더 많이 이동하게 하여 가속화한다는 말과 같습니다.
    * 또한 gradient가 0이라 하더라도, v값이 더해지면서 멈추지 않고 이동할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185774708-858adba1-5ebe-4435-8d61-64d056e7cb1e.png">
</p>

* SGD + Momentum은 정해진 방법으로만 사용하는 것이 아니라 세부 구현은 약간씩 다를 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185774779-dce1c710-658a-4a24-a3e9-667f96be3e36.png">
</p>

* SGD + Momentum은 다음과 같은 문제들을 해결할 수 있습니다.
    * Local minima와 saddle point
        * 위 슬라이드의 왼쪽 위 그림에서, 빨간색 공은 local minima나 saddle point에 도달하더라도 여전히 velocity를 가지고 있기 때문에 gradient가 0이더라도 계속해서 나아갈 수 있습니다.
    * Poor conditioning 문제
        * 위 슬라이드의 왼쪽 아래 그림에서, 지그재그로 수렴하는 움직임도 momentum을 통해 상쇄할 수 있기 때문에 민감한 수직방향의 변동은 줄어들고 수평방향의 움직임은 점차 가속화 됩니다.
    * Gradient Noise
        * 위 슬라이드의 오른쪽 그림에서, 파란색 선은 Momentum이 추가된 SGD이고 검정색 선은 그냥 SGD입니다.
        * Momentum항이 추가되면 noise를 평균내버려서 그냥 SGD보다 더 smooth하게 수렴하게 됩니다.

<br>
<br>





# Nesterov Momentum

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185775198-0d54668d-496c-411f-bce1-b07dfc85eab7.png">
</p>

* 좌측 슬라이드는 SGD + Momentum이 나아가는 형태를 직관적으로 보여줍니다.
    * Momentum 업데이트는 현재 지점까지 그동안 누적되어온 velocity와 gradient값이 더해져서 실제 step을 이루는 방식이었습니다.
    * 실제 업데이트는(autual step) 현재 지점에서의 gradient의 방향과 Velocity vector의 가중평균으로 구할 수 있습니다. 이는 gradient의 noise를 극복할 수 있게 해줍니다.
* 우측 슬라이드는 Nesterov Momentum으로, 앞에서 살펴본 momentum을 추가하는 방식의 변형입니다.
    * NAG(Nesterov Accelerated Gradient) 라고도 부릅니다.
    * 원점에서 구한 gradient와 velocity를 더하는 SGD+Momentum와 달리, Nesterov는 원점에서 velocity를 구한 다음 velocity의 지점에서 gradient를 계산하고 다시 본래 자리로 돌아와 actual step을 나아가는 형태입니다.
        * 이 방법은 velocity의 방향이 잘못된 경우에 현재 gradient의 방향을 활용하여 좀 더 올바른 방향으로 가게하는 것으로 이해할 수 있습니다.
        * 즉 누적된 과거의 gradient가 가리키는 방향을 현재 gradient에 보정하는 효과를 가지게 됩니다.
    * Nesterov Momentum은 convex optimization에서는 뛰어난 성능을 보이지만, neural network와 같은 non-convex problem에서는 성능이 보장되지 않습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185776814-8fc280b8-083d-4a5d-81f8-a1d5c581914b.png">
</p>

* 위 슬라이드는 Nesterov Momentum을 구하는 식입니다.
* 위쪽 검은색 박스의 파란색 박스 식을 보면, gradient를 한 지점에서 구하지 않는 것을 확인할 수 있습니다. 이러한 형태는 까다로우므로, 변수를 적절히 바꿔주어 아래쪽 검은색 박스의 식과 같이 gradient를 한 지점에서 구하는 식으로 다시 나타낼 수 있습니다.
* 아래쪽 검은색 박스의 식으로부터 Nesterov는 다음과 같이 동작한다.
    * $v_{t+1}$ 은 Vanilla SGD+Momentum과 같이 velocity와 gradient를 일정 비율로 섞어 준 것으로 이해할 수 있습니다.
    * $\tilde{x}_{t+1}$ 는 마지막 식의 형태만 보면 됩니다.
        * 현재 위치 $\tilde{x}_t$ 와 velocity $v_{t+1}$ 를 더한 값에 (현재 velocity $v_{t+1}$ - 이전 velocity $v_t$)에 일정 비율 $ρ$ 를 곱한 값을 더해주며 구하게 됩니다.
* 따라서, Nesterov는 현재와 이전의 velocity간 error-correcting term(에러 보정 항)이 추가된 것으로 이해할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185776826-93545493-3984-4b19-98c0-5264d18df347.png">
</p>

* 위 슬라이드는 SGD, Mementum, Nesterov의 움직임을 나타낸 것으로, SGD+Momentum과 Nesterov는 빠르게 수렴하는데 반해 SGD는 느리게 수렴합니다.
* 또한, SGD+Momentum과 Nesterov는 velocity 때문에 minima를 지나친 후, 다시 경로를 틀어 minima로 수렴하는 형태를 보입니다.
    * SGD+Momentum과 Nesterov는 아주 좁고 깊은 minima를 지나칠 수 있습니다. 하지만, 그러한 minima는 아주 overfit된 경우이기 때문에 test data에서 좋은 일반화 성능을 보이는 지점이 아닙니다.
    * 따라서, 위 슬라이드 그림과 같이 넓고 평평한 지점에서의 minima를 찾는 것이 우리의 목적이며, 좁고 깊은 minima를 건너뛰면서 우리가 원하는 minima를 찾는데 momentum 방법들이 도움이 된다는 것도 좋은 점이라고 볼 수 있습니다.

<br>
<br>





# AdaGrad

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185777780-1fe84275-bd0a-4518-a967-2695929ecaac.png">
</p>

* 또다른 최적화 방법으로 AdaGrad가 있습니다.
* AdaGrad는 Adaptive Gradient를 뜻하며, gradient descent가 되는 정도를 상황에 맞게 조정하는 것입니다.
    * 조금더 엄밀히 말하면, 이전의 learning rate가 일괄적으로 적용되고 있어 overshooting 되었던 문제를 learning rate를 가변적으로 적용하여 해결하고자 합니다.
    * velocity term 대신에 grad squared term을 이용합니다.
    * 이 방법은 학습 중에 계산되는 모든 gradient의 제곱을 계속해서 누적합시켜 더해갑니다. 그리고 gradient를 루트를 취해 나눠준 값으로 업데이트 하여 step마다 learning rate를 가변적으로 적용하는 방법입니다.
        * 이는 기울기가 가파를수록 조금만 이동하고, 완만할 수록 조금 더 이동하게 함으로써 변동을 줄이는 효과가 있습니다.
        * 또 행렬곱 연산을 통해 가중치마다 다른 학습률을 적용한다는 점에서 더욱 정교한 최적화가 가능해집니다..
* 즉, 학습이 진행됨에 따라 gradient의 제곱이 계속 커질 것이고, 업데이트 되는 gradient는 점점 작아지는 것입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185777864-78ede298-d964-42dc-9c9d-149fe5250556.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185777875-bb5bb8d8-02bd-4ade-81c7-8326fd6d2f23.png">
</p>

* 위 슬라이드의 그림에서, gradient를 가로, 세로 축으로 생각하고 AdaGrad의 동작을 이해해보면 다음과 같습니다.
    * 가로 축으로는 gradient가 항상 작고, 세로 축으로는 gradient가 항상 큽니다. 따라서, 제곱해서 더해지는 항이 가로 축에서는 작을 것이고, 세로 축에서는 클 것입니다.
        * y축 방향("steep"): grad_square is big
            * 따라서 grad_square로 나누면, 가려던 방향으로 원래 가려던 것보다 덜 갑니다.
        * x축 방향("flat"): grad_square is small
            * 따라서 grad_square로 나누면, 가려던 방향으로 원래 가려던 것보다 더 갑니다.
    * 이로 인해, 세로 축 방향으로의 움직임에서는 gradient가 큰 값으로 나누어 진행을 약화시키고, 가로 축 방향으로의 움직임에서는 작은 값으로 나누어 accelerating motion 효과를 줍니다.
    * 따라서, 중앙 지점까지 가로축 세로축 모두에서 적절한 속도로 수렴하게 되는 것입니다.
* 하지만, AdaGrad는 Step을 진행할 수록, 계속해서 값이 작아진다는 문제가 있습니다.
    * Convex case에서는 minimum에 근접하면서 속도를 줄일 수 있기 때문에 좋은 방법이지만, Non-convex case에서는 saddle point에 걸렸을 때, 더이상 진행하지 못하기 때문에 좋은 방법이 아닙니다.
* 또한 non-covex problem 경우에서 학습이 길어질 경우, 학습 속도가 점점 줄어들어 결국 멈추어버린다는 문제점도 있습니다.
* 따라서, Neural Network를 학습시킬 때에는 AdaGrad를 잘 사용하지 않습니다.

<br>
<br>





# RMSProp: “Leaky AdaGrad”

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779062-42978bf4-5971-42a0-a39e-c0932b6f5d1c.png">
</p>

* RMSProp은 AdaGrad의 앞선 문제점을 개선시킨 최적화 방법으로 Leaky AdaGrad라고 합니다.
    * AdaGrad는 과거 정보를 계속 누적하는 반면
    * RMSProp은 과거정보를 조금씩 축소시켜서 받아들이기 때문입니다.
* AdaGrad에서처럼 제곱 항을 그냥 누적하지 않고, decay_rate를 곱해서 누적하는 형태로 문제점을 개선하였습니다.
    * 누적된 gradient 값에 decay rate를 곱해주고, 현재 gradient의 제곱값에 1 - decay rate를 곱하여 누적해서 더해줌으로써 아주 미세하게(leaky) step size를 조절할 수 있게 되었습니다.
    * 이때, decay_rate는 주로 0.9나 0.99를 사용합니다.
    * 이전 스텝의 기울기를 더 크게 반영하여 h 값이 단순 누적되는 것을 방지할 수 있습니다.
* 즉, RMSProp은 gradient의 제곱을 나눠준다는 점은 동일하지만, 속도가 줄어드는 문제는 해결한 형태라고 볼 수 있습니다.
    * 보폭을 갈수록 줄이되, 이전 기울기 변화의 맥락을 살피자는 것입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779278-c94b8f3d-bd17-478e-ad14-d8e2a7142df0.png">
</p>

* 위 슬라이드는 SGD, SGD+Momentum, RMSProp을 비교한 것입니다.
* SGD+Momentum은 한번 overshoot 한 뒤에 다시 minima로 수렴하는 궤적을 그리지만, RMSProp은 각 step마다 각 차원의 상황에 맞도록 적절하게 궤적을 수정하면서 수렴하는 형태를 보입니다.

<br>
<br>





# Adam = RMSProp + Momentum

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779705-3f8d131e-3ffe-476f-9fad-b8da2801a032.png">
</p>

* **<span style="color:red">Adam 알고리즘</span>** 은 **<span style="background-color: #fff5b1">RMSProp과 Momentum이 합쳐진 개념</span>** 이라고 생각할 수 있습니다.
* 위 슬라이드의 식은 완전한 Adam은 아니지만, Adam의 컨셉을 나타낸 것입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779713-f48cc720-d0a4-4e73-bbe8-db24b02659b4.png">
</p>

* **<span style="background-color: #fff5b1">Momentum에서 과거 velocity와 현재의 gradient를 더해서 나아가는 방식을 차용</span>** 합니다.
* 과거의 정보는 조금씩 잊으면서, 현재 점에서의 새로운 그래디언트로 보강하여 $W$ 를 업데이트합니다. 

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779723-eb1960ae-d0a6-4236-a78c-a27ff7cc9e8b.png">
</p>

* **<span style="background-color: #fff5b1">AdaGrad와 RMSProp에서는 gradient를 제곱하여 과거의 정보를 누적하면서 $W$ 를 업데이트시에 가려던 방향으로 나눠주는 방식을 차용</span>** 합니다.
* (과거는 축소시키면서) dw의 제곱값을 누적시키고, 분모로 moment2 값을 사용하여 $W$ 를 업데이트합니다. 

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185780891-94ebc230-2d4b-4af7-a77c-ca7af76e89f9.png">
</p>

* 하지만, **<span style="color:red">문제가 있습니다.</span>** 만약 Beta2가 0.999일때, 초기 step에서 어떤 일이 발생할지 생각해봅니다.
    * moment2 는 처음에 0으로 초기화 됩니다. 그리고 beta2(decay_rate에 해당)도 보통 0.9~0.99의 값을 가지므로 및 (1 - beta2) * dx * dx 항도 0에 가까울 것입니다. 따라서, moment2 는 처음에 1번 업데이트 한 이후에도 여전히 0에 가깝게 됩니다.
    * 이로 인한 문제는 바로 다음의 업데이트 단계에서 일어나는데, 매우 작은 moment2 로 나누게 되어 초기 step이 매우 커지게 되는 문제인데, 이는 한번 발생하면 매우 나쁜 상황이 됩니다.
    * 매우 커진 초기 step으로 인해 초기화가 엉망이 될 것이고, 전혀 엉뚱한 곳으로 이동하게 될 수도 있습니다. 이는 수렴할 수 없는 현상을 초래하기도 하므로 매우 좋지 못합니다.
* 위 수식에서는 moment1, moment2 둘 다 0 에 수렴하기 때문에 상쇄될 수도 있지만, 상황에 따라 엄청 큰 step size가 발생하는 경우도 있어서 아주 큰 문제가 됩니다.
* 따라서 실제 Adam 에서는 first moment와 second_moment에 **<span style="color:red">bias correction term을 추가</span>** 해 이것을 보정합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185781020-c7d9169f-fbe9-4f84-a56c-d630afcf8133.png">
</p>

* 위 슬라이드는 완전한 형태의 Adam입니다.
* 앞에서 살펴본 (완전하지 않은) Adam은 초기 step이 매우 커질 수 있다는 문제가 있었습니다. 따라서, 실제 (완전한 형태의) Adam은 이를 해결하기 위해 보정하는 항(bias correction term)을 추가한 형태입니다.
* 위 식을 하나씩 살펴보면 다음과 같습니다.
    * 먼저, moment1와 moment2를 업데이트 합니다.
    * 그리고 t(현재 step)에 맞는 적절한 unbiased term(moment1_unbiased과 moment2_unbiased)을 계산합니다.(앞에서의 문제를 해결하기 위해 추가된 부분)
    * 마지막으로, 앞서 구한 unbiased term을 통해 업데이트를 수행합니다.
* 즉, **<span style="background-color: #fff5b1">(완전한 형태의) Adam은 moment1와 moment2만 계산하는 것이 아니라, unbiased term을 계산해서 동작하는 것입니다.</span>**
* Adam은 다양한 문제들에서 잘 동작하기 때문에 아주 좋습니다.
    * 특히, **<span style="background-color: #fff5b1">beta_1 = 0.9</span>**, **<span style="background-color: #fff5b1">beta_2 = 0.999</span>**, **<span style="background-color: #fff5b1">learning_rate = 1e-3, 5e-4, 1e-4</span>** 정도로만 설정하면 거의 모든 모델에서 잘 동작하는 기본 설정이 될 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185781427-a7d1ec57-530c-461a-b24d-091f1131d8f5.png">
</p>

* 동일한 환경에서 SGD, Momentum, RMSProp, Adam을 비교한 그림입니다.
* Adam의 궤적은 SGD+Momentum과 RMSProp의 궤적을 절충한 형태로 그려집니다.
    * overshoot하기는 하지만 SGD+Momentum보다는 정도가 약합니다.
    * RMSProp처럼 각 차원의 상황에 맞도록 적절하게 궤적을 수정하면서 step을 이동합니다.

<br>





# Optimization Algorithm Comparison

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185782171-d41d257f-7c4a-4a49-beda-4ccfb45c7c2e.png">
</p>

* 지금까지 소개한 최적화 알고리즘들을 비교하겠습니다.
    1. 첫번째의 속도를 추적할 수 있는가?
        * SGD+Momentum, Nesterov, Adam
    2. Elementwise하게 곱해서 제곱term을 보관하는가?(너무 많이 간 방향으로는 많이 가지 못하게 보정)
        * AdaGrad, RMSProp, Adam
    3. 과거를 잊고있는가?
        * RMSProp, Adam
    4. 처음 가려는 방향(초기step)에 대해 너무 느린 것을 보정하는가?
        * Adam

<br>
<br>





# Learning Rate Schedules

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185783775-a2c47ee9-6703-4f43-857d-692297c57273.png">
</p>

* 이전에 보았던 모든 optimization기법들은 **<span style="color:red">learning rate</span>** 를 hyperparameter로 가지고 있었습니다. 이것은 deep learning model에서 가장 중요한 우선순위의 hyperparameter로 다루어지고 있습니다.
* 학습 과정에서 learning rate을 하나의 값으로만 정해놓는다면, 좋은 값을 설정하기가 어렵습니다. 그렇다면 **<span style="background-color: #fff5b1">"어떻게 최적의 learning rate를 찾아야할까?"</span>** 라는 질문에, **<span style="color:red">Learning rate decay</span>** 가 좋은 전략이 될 수 있습니다.
    * training 초반에는 green line을 따르는 high learning rate을 사용하여 빠르게 학습시켜야하고, 시간이 지남에 따라 blue line을 보이는 low learning rate로 learning rate를 decay 시키는, 즉 red line을 따르는 것이 이상적인 방법입니다.
* Learning rate decay는 처음에 learning rate을 높게 설정한 후, 학습이 진행될수록 learning rate를 점점 낮추는 방법이며 다음의 **<span style="background-color: #fff5b1">두가지 방법</span>** 이 있습니다.
    1. **<span style="background-color: #fff5b1">특정 순간마다 learning rate을 감소시키는 방법입니다.</span>**
        * 예) Step decay : 몇번의 epoch마다 learning rate을 감소시킵니다.
    2. **<span style="background-color: #fff5b1">꾸준히 learning rate을 감소시키는 방법입니다.</span>**
        * 학습 동안에 꾸준히 learning rate을 감소시키는 방법입니다.
        * 예) exponential decay와 1/t decay : 위 슬라이드의 식 참고
        * 꾸준히 learning rate을 감소시키는 방법에는 다양한 전략이 있을 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784167-d88a0801-90e0-4422-a029-25688b028975.png">
</p>

* 위 슬라이드는 ResNet 논문에 있는 학습 그래프로, 화살표로 표시된 지점은 step decay가 적용되어 learning rate가 줄어든 지점입니다.
* **<span style="color:red">learning rate가 decay되어야 하는 순간</span>** 은 다음과 같습니다.
    * **<span style="background-color: #fff5b1">현재 수렴을 잘 하고 있는 상태에서 gradient가 작아졌고, learning rate가 너무 높아서 더 깊게 들어가지 못하는 상태(bouncing around too much)</span>** 에 decay가 되어야 합니다.
    * **<span style="color:red">이때, learning rate를 낮추게 되면 속도가 줄어들어 더 깊게 들어가며 loss를 낮출 수 있습니다.</span>**
* Learning rate decay와 관련된 추가적인 내용들
    * Learning rate decay는 Adam보다 SGD Momentum을 사용할 때 자주 사용합니다.
    * Learning rate decay는 두번째로 고려해야 하는 하이퍼파라미터입니다.
        * 학습 초기에는 learning rate decay를 고려하지 말고 learning rate 자체를 잘 선택해야 합니다. 그 이유는 초기 learning rate와 decay를 cross-validate하려고 하면 너무 문제가 복잡해 지기 때문입니다.
        * 따라서, Learning rate decay를 적용할 때에는 먼저, decay 없이 학습을 시도하고, loss curve를 살펴보면서 decay가 필요한 곳이 어디인지 고려해서 사용해야 합니다.
* 이렇게 training process동안 시간이 지남에 따라 learning rate를 변경시키는것을 **<span style="color:red">learning rate schedule</span>** 이라고 합니다.

<br>

## Learning Rate Decay: Step

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784462-c1c63e14-96a2-4195-9f15-6cfe245d2487.png">
</p>

* Learning rate를 decay시키는 방법인 **<span style="color:red">Step</span>** 방법으로, noncontinous하게 decay시키는 방법입니다.
    * 일정 범위의 epoch마다 discrete value를 learning rate로 사용하고 점차 decay시켜 계단모양의 learning rate를 보이게 되는 decay schedule입니다.
* **<span style="background-color: #fff5b1">어떤 고정된 지점들에서 learning rate를 낮추는 것</span>** 입니다.
    * 예를 들면, ResNet에서는 epoch가 30씩 지날때마다, LR를 0.1씩 곱해서 감소시킵니다.
* step learning rate schedule은 step마다 learning rate를 고정시킬 epoch의 범위, step마다 learning rate 를 감쇠시킬 decay rate가 새로운 hyperparameter로 추가되는 문제가 있습니다.

<br>

## Learning Rate Decay: Cosine

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784528-bb54a495-859d-45d6-8634-5c28060315f8.png">
</p>

* 다음은 **<span style="color:red">Cosine</span>** 방법으로, continous하게 decay시키는 방법입니다.
* Step learning rate schedule과는 달리 decay rate나 epoch의 범위를 설정해 줄 필요 없이 위 그림의 공식을 따라 **<span style="background-color: #fff5b1">매 epoch마다 learning rate을 decay시키게 됩니다.</span>**
    * T는 training하는 전체 epoch의 수, t는 현재 epoch, $\alpha_0$ 은 사전 지정이 필요한 hyperparameter입니다.
    * 사전에, $alpha_0$과 T만 지정하면, epoch가 지남에 따라 LR이 감소하며, loss도 smooth하게 감소합니다.
* tuning해주어야 할 hyperparameter가 적기 때문에 최근에 많이 사용된다고 합니다.

<br>

## Learning Rate Decay: Linear

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784536-212a5f90-62e5-46ed-bac9-eb3bd32370f7.png">
</p>

* 다음은 **<span style="color:red">Linear</span>** 방법으로, continous하게 decay시키는 방법입니다.
* 직선의 형태로 learning rate가 감소합니다.
    * 기울기로 활용할 $alpha_0$ 와 T가 hyperparameter로 필요합니다.
* linear learning rate schedule은 간단하지만 자주 사용되며 각 model마다, 각 problem마다 사용하는 learning rate schedule은 다르며, 이런 learning rate schedule들은 어느게 좋은지 비교할 수는 없습니다.
    * **<span style="background-color: #fff5b1">computer vision</span>** 분야에서는 **<span style="background-color: #fff5b1">Cosine learning rate decay schedule</span>** 을 많이 사용합니다.
    * **<span style="background-color: #fff5b1">large-scale netural language processing</span>** 에서는 **<span style="background-color: #fff5b1">linear learning rate schedule</span>** 을 많이 사용합니다.

<br>

## Learning Rate Decay: Inverse Sqrt

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784545-0654f202-a4fa-4e9c-acc1-dfa7a3845dd2.png">
</p>

* 다음은 **<span style="color:red">Inverse Sqrt</span>** 방법으로, LR를 초반에 빠르게 decay시킵니다.
* Cosine schedule, Linear schedule 과 다르게 거의 사용되지 않습니다.

<br>

## Learning Rate Decay: Constant

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784551-0273f761-64ee-4dae-ab52-a5ad53f0f7d4.png">
</p>

* 다음은 **<span style="color:red">Constant</span>** 방법으로, epoch가 진행되어도 LR는 같은 값을 유지합니다. 
* Constant learning rate schedule은 가장 흔하게 볼 수 있으며, **<span style="background-color: #fff5b1">Constant schedule은 다른 복잡한 laerning rate schedule과 비교해 model, problem에 따라 성능이 좋아질 수도 나빠질 수도 있으며, 가능한 빠르게 model을 만들어야 할때는 좋은 선택이 됩니다.</span>**
* 또한 momentum같은 optimizer를 사용할때는 복잡한 learning rate decay schedule을 사용하는것이 좋지만 Adam or RmsProp를 사용할때는 (Bias correction이 붙었으므로) learning rate decay가 중요하지 않기에 그냥 Constant learning rate를 사용하는것이 좋습니다.


<br>

## Learning Rate Decay: Linear Warmup

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784986-db1ba28d-a0ef-4bde-a6ef-92923c2f67fc.png">
</p>

* Goyal et al, “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour”, arXiv 2017
* 다음은 **<span style="color:red">Linear Warmup</span>** 으로, 높은 초기 learning rates는 Loss를 explode하게 만들 수 있기때문에, 첫 번째 ~ 5,000회 동안 학습 속도를 0에서 선형적으로 증가시키면 이를 방지할 수 있습니다.

<br>
<br>





# Early Stopping

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185785176-3a70c5b4-e8d4-4b8a-8ce5-eb5f581b7932.png">
</p>

* 결국 좋은 optimization과 learning rate decay 등의 방법론을 쓸때, Loss는 계속 낮아지겠지만, validation set의 accuracy가 감소하려고할때 즉, **<span style="background-color: #fff5b1">Loss decay와 accuracy plot을 함께 보면서 overfitting되기 전에 iteration을 멈추어야합니다.</span>**
* 학습시 우리가 얼만큼의 epoch or iteration으로 model을 훈련시켜야 할 지 모르겠을때 사용할 수 있는 좋은 mechanism으로 **<span style="color:red">Early Stopping</span>** 이 있습니다.
* 따라서, 매 iteration마다의 모델의 스냅샷을 저장한 후, val set에서 가장 잘 워크할 때의 iteration때의 weight를 불러옵니다.

<br>
<br>





# why use First-Order Optimization?

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786455-e6a2b125-dd1a-40d3-bd89-85c4416d806e.png">
</p>

* 그렇다면 왜 2차 근사 대신 1차 근사를 활용하여 최적화를 하는지 살펴보겠습니다.

<br>





# First-Order Optimization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786245-8c714290-3fce-4f03-bb3f-a4d42cd26a97.png">
</p>

* 지금까지 배운 최적화 기법들은 모두 1차 미분을 이용한(first-order) 형태였습니다.
* 즉, 위 슬라이드와 같이 현재 지점에서 gradient를 계산하고, 이를 통해 loss 함수를 선형 함수로 근사시키는 방법이었습니다.(일종의 1차 first-order Taylor approximation)
* 하지만 1차 근사 함수의 미분값으로는 멀리 나아갈 수 없습니다.(수렴 속도가 느리다)

<br>





# Second-Order Optimization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786648-9aeb7e36-1bbf-4d2a-8055-807d20baa839.png">
</p>

* 1차 근사 함수의 미분보다 조금 더 빠른 방법으로, 2차 근사 함수를 추가적으로 사용하는 것을 생각해 볼 수 있습니다. (Second-order optimization의 기본 아이디어)
    * 2차 근사는 2차 다변수함수의 최적점을 찾아서 이동하는 방식입니다.
* 이는 위 그림과 같이 minima로 더 빨리 수렴할 수 있다는 장점이 있다.
* 또다른 장점으로는 flat한 지역이 있을 때, 한 번에 많은 steps를 이동할 수 있다는 점입니다. 즉, loss surface에 따라 유연하게 대처할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786726-00d4c6df-c91e-4fc6-9410-47838585c566.png">
</p>

* 2차 근사함수를 사용하는 Optimization 방법을 **<span style="background-color: #fff5b1">Newton’s Method</span>** 라고 합니다.
* 2차 미분 값들로 된 행렬인 Hessian Matrix를 계산하고 이 행렬의 inverse를 이용하게 되면, 실제 Loss함수의 2차 근사를 이용해 minima로 바로 이동할 수 있게 됩니다.
* 이와 같은 Newton’s Method에서는 단지 매 step마다 2차 근사 함수의 minima로 이동하면 되기 때문에, learning rate가 필요 없다는 특징이 있습니다.
    * 실제로는 minima로 이동하는게 아니라 minima의 방향으로 이동하는 것이기 때문에 learning rate가 필요하지만, 기본 형태에서는 learning rate가 없습니다.
* **<span style="color:red">하지만 이 방법은 딥러닝에 사용할 수 없습니다.</span>**
    * Hessian Matrix는 N X N의 크기인데, 여기서 N은 파라미터 수이다. 따라서, 이러한 큰 행렬을 메모리에 저장하는 것은 불가능하며, 역행렬을 구하는 것도 불가능합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786898-4f2ec0d7-05e5-4da8-a056-fe400c81e156.png">
</p>

* 그래서 실제로는 Full Hessian을 Low-rank로 approximation하는 **<span style="background-color: #fff5b1">Quasi-Newton method</span>** 를 사용하게 됩니다.

<br>





# Second-Order Optimization: L-BFGS

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786904-7ff4731c-5455-4d9a-ae22-671ab035fc61.png">
</p>

* Hessian Matrix를 근사시키는 방법을 사용한 Second-order optimization 방법으로는 **<span style="background-color: #fff5b1">L-BFGS</span>** 가 있습니다.
* 그러나 Full batch에서는 잘 작동하지만, 소량의 샘플을 뽑아서 그래디언트/헤시안 매트릭스를 계산하는 mini-batch 환경에서는 잘 작동하지 않습니다. Large-scale에 2차근사 방법을 적용시키면, Stochastic setting의 노이즈가 커집니다.
* 또한 L-BFGS와 같은 이러한 2nd order opproximation은 stochastic case와 non-convex case에서 잘 동작하지 않기 때문에, DNN에서는 잘 사용되지 않습니다.

<br>





# Optimization Summary

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786912-ca162f20-c46d-4bb2-91f2-e4997d190a16.png">
</p>

* W의 최적점을 찾는 최적화 알고리즘으로 Adam을 주로 좋은 디폴트 초이스로 활용합니다.
* SGD + Momentum이 더 좋은 성능을 낼 수 있지만, 튜닝에의 더 많은 노력이 필요합니다.
* 또한 만약 full batch updates를 하게된다면, L-BFGS를 활용해보는 것도 좋습니다.

<br>
<br>





<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185831898-3ad639fb-b21b-45f2-8ce4-f45f85e77b1a.png">
</p>

<br>





# Beyond Training Error

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185831997-b8799c35-69ac-4fb3-8db7-d33e7a86331c.png">
</p>

* 지금까지 이야기했던 모든 것들을 전부 training error를 줄이기 위한 방법들이었습니다.
    * optimization 알고리즘들은 training error를 줄이고 손실함수를 최소화시키기 위한 역할을 수행합니다.
* 즉 최적화(Optimization) 과정이 손실함수의 최솟값을 찾아나가며 train error를 줄이는데 집중했다면 이제는 valid error와의 차이를 줄여 과적합을 방지할 차례입니다.
    * 정리하자면, **<span style="background-color: #fff5b1">test set의 loss가 낮게 나오는 것을 원하기에, test set의 loss값과 training set의 loss값의 차이(gap)를 줄여야 합니다.</span>**
* 학습에서 Loss 함수의 최적화시, **<span style="background-color: #fff5b1">overfitting을 방지하는 동시에 Test에서의 성능을 높이기 위한 가장 쉬운 방법은 무엇이 있을까?</span>**
    * 이 질문은 **<span style="color:red">Regularization</span>** 으로 이어집니다.

<br>





# Model Ensembles

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185833219-ab3510d9-df50-414f-b175-c2a14743977a.png">
</p>

* 우선 가장 쉬운 방법으로는 **<span style="color:red">Ensembles</span>** 이 있습니다.
* 앙상블은 **<span style="background-color: #fff5b1">여러개의 독립된 여러모델을 학습시킨 후, test time시에 이들의 결과를 평균내는 방법</span>** 입니다.
    * 모델의 수가 늘어날수록 overfitting이 줄어들고 성능이 조금씩 향상됨 (보통 2% 정도 증가함)
    * 이를 통해 validation set과 training set의 갭을 줄이고 **<span style="background-color: #fff5b1">robust한 결과</span>** 를 낼 수 있도록 합니다.
    * Imagenet 같은 대회에서는 모델의 성능을 최대화 시키기 위해서 앙상블을 사용하는 경향이 다수 존재합니다.

<br>





# Model Ensembles: Tips and Tricks

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185833663-4964ca71-9868-47b5-8414-19eb20425804.png">
</p>

* 하나의 모델에서 학습 도중 중간중간에 snap shot을 찍은 후(모델의 가중치를 저장), Test 시에 이들 snap shot들에서 나온 prediction들을 평균내어 사용하는 앙상블 방법이 있습니다.
* ICLR에서 발표된 한 논문에서는 매우 독특한 Learning rate scheduling을 사용하여 조금 더 향상된 앙상블 알고리즘이 발표되었습니다.
    * Learning rate을 매우 낮췄다가 매우 높였다가를 반복하면서 학습 과정에서 Loss 함수의 다양한 지역에서 수렴하도록 만듭니다. 이때, 수렴할 때마다 snap shot을 찍습니다.
    * 그리고 이들 snap shot들을 모두 앙상블합니다.
    * 이 방법은 모델을 한번만 학습시켜도 좋은 성능을 얻을 수 있게 해줍니다.
* 정리하면 **<span style="background-color: #fff5b1">모델을 여러 개 training</span>** 해서 model ensembles를 하는경우도 있으며, **<span style="background-color: #fff5b1">learning rate schedule을 활용하여 하나의 모델 훈련만으로도 앙상블의 효과</span>** 를 낼 수 있는 방법이 있습니다. LR decay를 주고, 특정 시점마다 다시 LR을 높게 주면서, 구간 별 모델의 스냅샷을 저장하여 모델이 낸 결과를 평균내어 앙상블을 구현합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185834299-d57449a1-c2b2-4880-82f6-9e00e0a6791e.png">
</p>

* 또다른 앙상블 방법으로는 Polyak averaging이라는 방법이 있습니다.
* 이 방법은 학습하는 동안에 파라미터 벡터의 exponentially decaying average를 keep해뒀다가, Test시에 checkpoint에서의 파라미터가 아닌 smoothly decaying average를 사용하는 방법입니다.
* 이는 학습중인 네트워크의 smooth 앙상블 효과를 얻을 수 있으며, 때때로 약간의 성능향상을 보이게 됩니다.(시도할만하지만 실제로는 잘 사용하지 않음)

<br>





# How to improve single-model performance?

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186036937-1f8bce99-1520-4d06-8105-c87d0cbb510d.png">
</p>

* 그렇다면 앙상블이 아닌 단일 모델의 성능을 향상시키기 위해서는 어떻게 해야 할까?
* 여러 모델을 ensemble 하지 않고, **<span style="background-color: #fff5b1">단일 모델로 validation set(test set)의 성능을 올리는 좋은 방법</span>** 으로 **<span style="color:red">Regularization</span>** 이 있습니다.
    * 즉, 단일 모델의 성능을 올리는 것이 우리가 정말 원하는 것으로, 훈련과정 자체에서 regularization을 추가하여 모델이 training data에만 fit하는 것을 막아주어서, unseen data에서의 성능을 향상시켜 줍니다.

<br>
<br>





# Regularization: Add term to loss

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186036969-8e1a7805-072c-4dd9-8e1e-2bf92a826556.png">
</p>

* 앞 장에서 Loss에 regularization term을 붙이는 방법을 배웠습니다. $R(W)$ term을 통해, $L(W)$ 가 취할 수 있는 영역에 제한을 두어, training을 방해합니다.
    * 즉 Loss항에 L1 regularization, L2 regularization과 같은 항을 추가해줌으로써 training data에만 잘 맞는 상황에 패널티를 주는 것입니다.
        * L1 regularization은 학습에 기여하지 못하는 가중치를 0으로 만들어 버립니다.
        * L2 regularization은 학습에 기여하지 못하는 가중치를 0에 가깝게 만들어 버립니다.
    * 통계학에서 유의하지 않은 독립변수를 제거하는 검정과 유사합니다.
* 하지만 이러한 Regularization 방법은 Neural Network 에서는 잘 어울리지 않아 다른 방법을 사용합니다.

<br>





# Regularization: Dropout

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186038370-70166a82-97da-47c3-8306-6098b11ab578.png">
</p>

* Neural Network에서 가장 많이 사용되는 regularization 방법은 Dropout입니다.
* Dropout의 동작 방식은 단순히 Forward pass 과정에서 임의로 일부 뉴런의 출력을 0으로 만드는 것입니다.
    * 이때, Random하게 일부 뉴런을 선택하므로 매 forward pass 반복마다 출력이 0이 되는 뉴런은 바뀝니다.
    * 말 그대로 랜덤으로 뉴런 값을 0으로 보내버리는 것입니다.
    * 뉴런을 얼마나 drop 할 것인가는 하이퍼파라미터입니다. 보통은 0.5를 사용합니다.
* 즉 Dropout은 Neural network의 forward pass에서 각 layer의 neuron(hidden unit)을 ramdom하게 0로 만들어 해당 노드를 삭제(drop)하는 것이 핵심 idea입니다.
    * 보통 Fully Connected Layer에서 더 많이 쓰이며 CNN에서는 보통 채널 단위로 적용됩니다.
* 최근 Network 아키텍쳐에서는 많이 사용하지는 않습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186039306-f3c72de2-7259-4263-9607-ea8df6d7aa54.png">
</p>

* 위 코드는 3-layer neural network의 (forward pass중) 각 layer마다 binary mask (p를 0.5로 설정해줬으니 binary)를 취해주어 drop시키는 형태의 구현입니다.
    * drop 할 뉴런의 activation을 0으로 만들어 필요한 뉴런만 사용하게 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186039789-cd767c6c-b019-4867-879c-27cb66194111.png">
</p>

* 그렇다면 일부 뉴런을 drop하는 dropout이 어떻게 regularization효과를 가져올 수 있는 것일까?
* Dropout의 원리를 대략적으로 이해하자면, feature들 간의 상호작용(co-adaption)을 예방 또는 방지하는 것이라고 볼 수 있습니다.
    * 예를 들어, 고양이를 인식하는, 분류하는 어떤 네트워크가 있다고 할 때, 어떤 뉴런은 고양이의 귀, 어떤 뉴런은 고양이의 털, 어떤 뉴런은 고양이의 꼬리에 대해 학습된다고 생각해보겠습니다.
    * 고양이 인식 모델은 이들의 정보를 모두 모아서 출력을 내게 되는데, 이때, dropout을 적용하게 되면, 네트워크가 일부 feature에 의존하지 못하게 해줍니다.
    * 즉, 네트워크가 고양이라고 예측할 때, 다양한 feature들을 골고루 이용할 수 있게 되는 것이며, 이는 Overfitting을 어느정도 막아준다고 볼 수 있습니다.
* 풀어 말하면 Randomness를 부여해 모든 feature에 대한 weights를 분산시키는 역할을 하여, model이 많은 feature들 중 특정 feature에 의존하게 되는 현상을 줄이고, 여러 feature 들을 골고루 사용할 수 있게 하는것입니다.
    * 모델이 학습을 하다 보면 비슷한 정보를 가지는 노드가 생기기 마련입니다.
    * 이는 과적합을 야기할 수 있기 때문에 입력값의 일부를 0으로 두어 역전파시 파라미터 업데이트가 되지 않게하여, 모형의 불확실성을 증가시켜 과적합 해결에 기여하게 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186039798-8d68a85a-4515-401c-baea-06bea3b39916.png">
</p>

* Dropout에 대한 새로운 해석으로는 단일 모델로 ensemble 하는 효과를 가질 수 있다는 것입니다.
    * 위 슬라이드의 왼쪽 그림과 같이, dropout을 적용한 network를 보면 일종의 sub network라고 볼 수 있습니다. 그리고 매 반복마다 이러한 sub network들은 다양하게 생성됩니다.
    * 따라서, Dropout은 이러한 sub network들의 거대한 앙상블을 동시에 학습시키는 것이라고도 해석할 수 있습니다.
* 즉 training시 각 sample마다 다른 mask가 적용되어 하나의 모델이 되어서, 결국 full-training에선 각 model들의 ensemble이 된다는 것입니다.

<br>





# Dropout: Test time

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186042241-a980d165-371f-476e-bdb1-ffdc6d9c09d4.png">
</p>

* 여기서 dropout을 train data로 모델을 학습시킬 때만 활용하고 test data를 적용할때는 사용하지 않는 것으로만 알고 있는 사람들이 많은데, test 단계에서 dropout이 아예 쓰이지 않는 것이 아닙니다. 정확하게 짚고 가겠습니다.
* 우선 Test time 때에도 dropout을 적용하게 되면, 이미 학습된 네트워크의 Test time에 randomness를 부여하는 문제를 야기시킵니다.
    * 즉 테스트할 때마다 결과가 다르게 나옵니다.
    * 기존의 Neural network의 출력은 $f(w, x)$ 였지만, dropout으로 인해 입력에 random dropout mask $z$ 가 추가됩니다. 하지만, Test time에 이러한 randomness를 부여하는 것은 nice하지 않습ㄴ디ㅏ.
    * Ex) 고양이와 개를 분류하는 모델이 같은 이미지에 대해서, 어제는 개를 출력하고 오늘은 고양이를 출력하면 신뢰할 수 없습니다.
* 즉 dropout은 test-time operation에서 사실상 random output을 만들기에 문제가 됩니다. 이는 매번 다른 예측을 가져올 수 있다는 것을 뜻하므로, 우리는 output이 deterministic이기를 원합니다.
* 그렇므로 test-time에서 randomness를 없애기 위해서 average out 해야합니다.
    * output을 z에대한 expectation으로 볼 수 있습니다. 다른말로 적분을 통해 randomness를 marginalize out시키는 것으로 생각할 수 있습니다.
    * 즉 모든 뉴런을 켜고, 각 뉴런의 결과값에 확률 $p(z)$ 를 곱합니다.
    * 하지만, 이러한 적분을 다루기는 상당히 까다롭습니다.
* 적분이 어렵다면, 간단히 샘플링을 통해서 적분을 근사하는 방법도 생각해 볼 수 있습니다.
    * z를 여러번 샘플링해서 Test time에 average out시키는 방법
    * 하지만, 이 방법도 여전히 Test time에서의 randomness를 만들어 내기 때문에 좋지 않습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186050254-94f21b87-693d-4c5c-ad7f-133235157801.png">
</p>

* 앞에서 살펴본 문제들로 인해, dropout에서는 다음과 같이 단순한 방법을 통해 randomness 없이 적분을 근사하게 됩니다.
    * Training과 Test에서의 기댓값
        * Test
            * 단순히, weighted sum을 구하면 됩니다.
            * 기댓값 : $w_1x + w_2y$
        * Training
            * x를 활성화/비활성화, y를 활성화/비활성화하는 총 4가지의 경우가 있습니다. 확률과 뉴런의 결과값을 모두 곱해줍니다.
            * 0.5의 dropout이라고 가정할 때, 나올 수 있는 4가지 네트워크에서의 출력들을 구하고, 4로 나눠서 평균낸 후 더합니다.
            * 기댓값 : $\frac{1}{2}(w_1x+w_2y)$
    * 위와 같이 Train과 Test에서의 기댓값이 서로 다른 경우에서 randomness 없이 적분을 근사할 수 있는 단순한 방법 중 하나는 dropout probability를 출력에 곱하는 것입니다.
        * Dropout probability 0.5를 Test의 출력인 $w_1x + w_2y$ 에 곱해주면, training에서의 기댓값과 같은 결과를 얻게 됩니다.
        * 기댓값 : $(0.5) × (w_1x + w_2y) = \frac{1}{2}(w_1x+w_2y)$
* 정리하면, test time에는 어떤 노드도 버리지않고, p값을 곱해주어야 합니다.
    * 이는 dot product의 single scalar output에 dropdout probability를 곱해준 형태가 됩니다.
    * 따라서 test time시 dropout probability를 곱하여 계산하기만 하면 간단하게 적분을 근사할 수 있습니다.
* 이 방법은 이전 슬라이드에서 살펴본 복잡한 적분식을 보다 cheap하게 locally approximate한 방법이며, 실제로 Dropout을 사용할 때 이 방법을 많이 사용합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186052845-ee4de454-fea4-4fbe-aefe-4332b85134a5.png">
</p>

* Dropout의 Test time에서는 모든 뉴런을 활성화한 결과에 dropout probability를 곱합니다.
* 이러한 test-time에서의 구현은, training시 dropout을 거친 neuron의 output 기대값과 test-time시 neuron의 output 기대값의 scale이 달라지게되는 문제를, test-time에서 기댓값을 scaling시켜주어 해결 할 수 있게 되었습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186053663-13a20dcb-23b4-4499-ac99-e35ac7906746.png">
</p>

* Dropout은 위와 같이 몇줄의 코드로 쉽게 구현할 수 있습니다.
    * Training시 매 layer사이에서 mask를 drop시킵니다.
    * Test시 값에 확률값 p를 곱해줍니다.
* Neural network의 Regularization에 상당히 효과적입니다.
* 하지만 test-time은 predict를 위한 것이니 우리는 위와같이 결과에 ramdomness가 추가되는 것을 원치 않을 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186054219-c851a4db-3547-4471-84bb-6fb9ba7349f9.png">
</p>

* 그리고 더 일반적으로 test time에서의 계산효율을 더 높이기 위해 Inverted dropout을 사용합니다.
    * Test time에서 dropout probability p를 곱해주는 연산을 줄이는 방법 대신, Train time에서 p로 나눠주는 방법이 있습니다.
* 이러한 방법을 사용하는 이유는, Train time에서는 GPU를 사용해서 곱하기 몇번 추가되는 것이 큰 영향이 없지만, Test time에서는 계산효율, 즉 가능한 빠른속도로 효율적으로 동작해야 하기 때문입니다.
    * 이로써 test time 에서의 계산을 최소화 할 수 있습니다.
* Dropout을 사용하게 되면 전체 학습시간은 늘어나지만 모델이 수렴한 후에는 더 좋은 일반화 능력을 얻을 수 있습니다.
    * dropout을 사용하게 되면 Train time에서 Dropout이 0이 아닌 뉴런에대해서 gradient Backprop이 발생하게 됩니다.
    * 각 스텝마다 업데이드되는 파라미터의 수가 줄어들기 때문에 dropout을 사용하게 되면 전체 학습시간이 늘어납니다.

<br>





# Dropout architectures

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186055986-e73cbb54-8834-486d-86fa-bed224a52e18.png">
</p>

* AlexNet, VGG에서는 많은 parameter들이 사용되는, 아키텍쳐의 맨 윗단 레이어인 fully-connected layer에서 dropout을 적용합니다.
    * 보통 regularization을 사용하는 이유는 overfitting을 방지하기 위해서인데 어떤 layer에 hidden unit size가 클 수록 (parameter수가 많아지니) overfitting될 가능성이 높기 때문에 dropout을 사용하며, size가 작은 layer에서는 dropout을 사용하지 않습니다.
* 하지만 비교적 최근 architecture인 GoogLeNet, ResNet에선 fc layer대신 global polling layer를 사용하기에 dropout을 사용하지 않습니다.

<br>
<br>





# Regularization: A common pattern

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186056527-8362d99b-cc4a-4f1c-86a4-d869c6285e4d.png">
</p>


이러한 dropout에서 나타나는 것 처럼 neural network의 다른 regularization에서도 찾아볼 수있는 흔한 패턴이 있습니다.

* Regularization의 일반적인 패턴을 정리하면 다음과 같습니다.
    * Training : 무작위성(randomness)를 추가하여 training data에 대해 overfitting을 막습니다.
    * Test : randomness를 평균 또는 근사해서 generalization 효과를 줍니다.
* Dropout외에도 이러한 패턴으로 regularization 효과를 불러오는 방법으로는 Batch Normalization이 있습니다.
    * Training에서는 mini-batch로 데이터가 샘플링 될 때마다 서로 다른 데이터들과 만나게 됩니다. 이때 각 데이터들을 얼마나 normalize 시킬 것인지에 대한 randomness(또는 stochasticity)를 부여하여 training data에 대해 overfitting을 막습니다.
    * Test에서는 mini-batch 단위가 아닌 global 단위로 normalization을 수행함으로써 이러한 randomness(또는 stochasticity)를 평균내어 generalization 효과를 줍니다.
    * 즉, Batch Normalization은 train에서 stochasticity(noise)가 추가되지만, Test time에서 모두 average out하기 때문에 regularization 효과가 있게 되는 것입니다.
* 이러한 이유로 인해, 실제로 Batch Normalization을 사용할 때에는 Dropout을 사용하지 않습니다.
    * 충분한 Regularization 효과가 있기 때문입니다.

<br>
<br>





# Regularization: Data Augmentation

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186061813-f7f4ddd8-b4a6-44e5-b06e-cb7d812c9ecd.png">
</p>

* regularization의 패턴에 부합하는 또다른 regularization 방법으로는 data augmentation이 있습니다.
* Data Augmentation이란, training data 를 무작위로 변형시키고 원래의 label 값을 그대로 이용하여 학습시키는 방법입니다.
    * 즉 원본 이미지를 사용하는 것이 아니라 무작위로 변환시킨 이미지로 학습하게 됩니다.
* 비슷한 이미지지만, CNN 모델은 다른 이미지로 인식하며, 밝기 조절 혹은 좌우 대칭 등을 randomness의 일종으로 볼 수 있습니다.

<br>

## Data Augmentation: Horizontal Flips

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186062272-9ca2cbab-dfc1-4129-935f-d79f3b857aa5.png">
</p>

* 간단한 예로, 이미지를 horizontal flips 할 수 있습니다. 이미지가 반전되도 고양이는 여전히 고양이 입니다.
* 이러한 이미지 특징을 이용해 다양한 데이터에 대해 모델이 학습할 수 있게 해주는 것이 Data Augmentation 입니다.

<br>

## Data Augmentation: Random Crops and Scales

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186062709-de0e0755-4738-436d-ba9a-896abd65bf82.png">
</p>
<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186062724-1beb9839-1589-46a8-a5e6-da261eded69a.png">
</p>

* 이미지를 임의의 다양한 사이즈로 잘라서(crop) 사용할 수도 있습니다. 여전히 이미지는 고양이 입니다.
* Data Augmentation이 regularization pattern에 부합하는지를 살펴보면 다음과 같습니다.(Resnet)
    * Training
        * Random하게 cropping하거나 scaling 등을 수행해서 학습합니다.
        * 즉, randomness가 추가되는 것입니다.
    * Testing
        * Test할 하나의 이미지에서, 5개의 scale로 조정하고, 각 scale별 (네개의 각 코너와 중앙에서 crop한 이미지와 이들의 flipped image) = (4개코너 + 중앙) X (원본 + flipped) = 10개의 이미지를 추출해서 성능을 평가합니다.
        * 즉, stochasticity를 average out하게 되는 것입니다.

<br>

## Data Augmentation: Color Jitter

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186063784-36a01112-91df-4582-b0ec-f6b344717446.png">
</p>

* Data augmentation으로 color jittering도 있습니다.
    * 학습 시 이미지의 contrast 또는 brightness를 바꿔줍니다.
* 조금 더 복잡한 방법으로, R, G, B 픽셀에 대해 PCA의 방향을 고려하여 color offset을 조절하는 방법입니다.
    * 즉 조도를 조절하는 방법입니다.
    * 이 방법은 color jittering을 좀 더 data-dependent한 방법으로 진행하는 것입니다.
    * 자주 사용하는 방법은 아닙니다.

<br>

# Data Augmentation: Get creative for your problem!

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186064489-b62c4324-96d0-4bb7-b47f-95cfd06ae77d.png">
</p>

* 다양한 augmentation 방법들이 있으며, 이는 각 data, 각 problem에따라 다른 transform이 필요할 것이며 명확한 답이 없습니다.
* Data Augmentation은 어떠한 문제에도 적용해 볼 수 있는 아주 일반적인 regularization 방법이라고 볼 수 있습니다.
    * Label을 바꾸지 않고 데이터에만 변환을 줄 수 있는 많은 방법들이 모두 Data Augmentation에 사용될 수 있습니다.

<br>
<br>





# Regularization: A common pattern

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186064904-c4fdf392-5575-4fc2-836a-ff6cce56d314.png">
</p>

* 정리하면 Dropout, Batch Normalization, Data Augmentation같은 기법은 일반적인 Regularization 기법입니다.
* regularization의 common pattern의 관점에서 보면, data augmentation을 함으로써 train time에 다양한 stochasicity를 일으키고 최종적으로 test time에 average out하는 효과를 얻음으로써 regularization효과가 있습니다.

<br>





# Regularization: DropConnect

<p align="center">
<img alt="image" src="">
</p>

* 지금까지 살펴본 Regularization pattern을 잘 숙지하고 논문을 읽다보면, 여러가지 regularization 방법들이 눈에 들어오게 됩니다.
* 그 중 한 예로는, DropConnect라는 방법이 있습니다.
    * 이는 Dropout과 유사한 방법이며 activation이 아닌 weight matrix를 0으로 만들어 주는 것이 차이점이다.
    * Dropout과 동작이 아주 비슷합니다.
* 즉, DropConnect는 활성화가 아닌 가중치를 임의적으로 0을 만드는 걸 말합니다.
    * 가중치를 0으로 만드는 건 training 때 그렇고, testing에는 모든 연결선을 사용합니다.

<br>





# Regularization: Fractional Pooling

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186065717-43a74ee9-b650-4607-b0fc-7e20364e8989.png">
</p>

* 또 다른 Regularization 방법으로는 Fractional Max Pooling이 있습니다.
* 일반적인 2x2 Max Pooling 연산은 고정된 2x2 지역에서 수행하지만, 이 방법에서는 Pooling 연산을 수행할 지역이 random하게 선정하게 됩니다.
    * 위 슬라이드의 그림은 Train time에 샘플링될 수 있는 random한 pooling region을 나타낸 것입니다.
    * Test time에서는 pooling 영역을 고정하거나 또는 여러개의 pooling 영역을 만들고 average out시키는 방법을 사용합니다.

<br>





# Regularization: Stochastic Depth

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186065749-ac54ead3-262d-404c-be68-72f2788b4a34.png">
</p>

* Stochastic Depth이라는 방법도 있습니다.
    * Train time에는 layer중 일부를 random 하게 drop하고 일부만 사용해서 학습을 수행합니다.
        * 즉 랜덤하게 몇 개의 레지듀얼 블락을 무시합니다.
    * Test time에서는 전체 네트워크를 사용합니다.
* 이 방법도 마찬가지로 regularization 효과는 dropout과 같은 다른 방법들과 유사합니다.

<br>





# Regularization: Cutout

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186067150-eb0719f9-de0b-4ae8-992c-1580f77fa220.png">
</p>

* 비교적 최신기법인 Cutout 방법론이 있습니다.
    * Cutout은 image에 일정 region을 zero로 만들어 randomness를 부여하는 방식입니다.
    * test시엔 whole image를 사용합니다.

<br>





# Regularization: Mixup

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186068161-f54be84d-293e-4c84-ab13-6d6fce71b5dc.png">
</p>

* 다음은 Mixup 방법론이 있습니다.
    * Mixup은 training image를 label별 random한 weight 비율로 blend시키는 방식입니다.
    * 이때 sample은 beta distribution을 통해 blend weights가 확률적으로 정해집니다.

<br>





# Regularization Summary

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186068306-facd2581-3ebc-4de9-b7b4-f803688dc153.png">
</p>

* 여러 종류의 regularization을 살펴보았습니다.
* 각각 regularization의 scheme은 다를 수 있지만 대부분 regularization은 training시에 randomness를 부여하고 test시에 average out 시키는 형태의 패턴을 갖고있습니다.
* 결국 regularization은 model이 high variance를 가져 overfitting되는 현상을 완화시키고자 각 regularization마다 (intput값이던, pooling layer의 kernel이건 등등의) 특정 randomness를 부여하여 training시킨다는 것이 핵심입니다.

<br>
<br>





# Transfer Learning

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186069346-7d9b778a-05c7-45e7-8f26-32b91ed067d1.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186074905-25cfc03f-99a0-49c3-bfa9-94c08a7bdb3d.png">
</p>

* 지금까지 Overfitting을 해결하는 Regularization 방법들에 대해 알아봤습니다.
* 사실 Overfitting을 해결하는 가장 좋은 방법은 train data의 양을 늘리는 것입니다.
* 그렇다면, 좋은 성능의 classifier를 만들기 위해서는 무조건 많은 데이터가 필요할까? 
    * 이 질문은 Transfer Learning으로 이어집니다.
* Transfer Learning의 기본 철학은 좋은 성능의 classifier를 만들기 위해서는 무조건 많은 데이터가 필요한 것은 아니라는 것입니다.
    * 즉, Transfer learning을 사용하면 적은 양의 데이터로도 Deep Neural Network 모델을 사용할 수 있습니다.

<br>





# Transfer Learning with CNNs

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186078793-00f82f44-e586-4c92-bec9-4862ea264da3.png">
</p>

* Transfer learning 의 개념은, 기존에 사전 학습된 (pre-trained) 가중치 를 이용하고 추가적으로 내 데이터에 맞게 $W$ 의 일부만을 학습시키는 것입니다.
    1. 기본 idea는 첫번째로 imagenet과 같은 큰 data set으로 학습된 CNN model을 가져옵니다.
        * VGG CNN 아키텍쳐를 다운받았다고 생각해봅시다.
    2. 그리고 다음 단계는 앞서 ImageNet에서 학습된 feature를 우리가 학습할 작은 데이터셋에 적용하는 것입니다.
        * 가져온 model의 Last layer인 fully-connected layer는 Imagenet에서의 클래스 분류를 위한 것이므로 삭제를 합니다.
        * 직전 layer까지의 weight들은 모두 requires_grad = False로 설정하여 freeze하고, 업데이트에 사용하지 않습니다. 즉 feature extractor로 사용하는 것입니다.
* 데이터 수에 따라 학습 방법에 조금 차이가 있습니다.
    * 데이터가 적은 경우
        * 우리가 원하는 출력 클래스 수 C를 출력하도록 제일 마지막 layer만 초기화 한 후, 이 layer를 제외한 나머지 layer는 freeze시킨 후, 학습을 진행합니다.
        * 즉, 마지막 출력 layer만 학습시키는 것으로, linear classifier를 학습시키는 것과 같습니다.
    * 데이터가 조금 더 있는 경우
        * 데이터가 적은 경우에서와 같이 마지막 layer를 초기화 하는 것은 동일하지만, 여기서는 데이터가 더 많으므로, 몇개의 layer를 더 학습에 추가할 수 있습니다.
* 이렇게 pre-trained된 model을 가져와 transfer learning을 하는 것은, 한정된 dataset을 가지는 상황에서 성능을 향상시키는데 도움이 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186078808-be14fd23-e4b6-4f12-ac76-4ae2ad6a6681.png">
</p>

* 위 그림은 "Caltech-101" 이라는 101개의 categories를 가진 적은 양의 dataset의 classification 문제에서 모델별 성능을 나타내는 그래프입니다.
    * 빨간 그래프는 기존의 Caltech-101 dataset만을 학습시킨 모델의 성능을 나타냅니다.
    * 파란색과 초록색 그래프는 transfer learning을 활용하여 imagenet을 pre-trained 시킨 AlexNet을 사용하여 feature vector를 뽑아내고, 간단한 logistic regressor와 SVM을 사용하여 train시킨 model이다.
* 즉 파란색과 초록색 그래프는 top layer를 제거하여 feature extractor로 활용한 후, 그 위에 linear classification을 적용하여 학습한 방법입니다. 이 방법이 성능이 더 좋음을 알 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186079616-22a2d9db-109b-4077-89f2-e0c284684a20.png">
</p>

* 위 그래프는 AlexNet의 FC 레이어를 날리고, 다른 방법을 활용하면 성능을 더 올릴 수 있다는 것을 보여주는 그래프입니다.
* 정리하면, 우리가 가진 data가 적다면 pre-trained CNN model을 feature extractor로 사용하고 output layer만 liniear classifier같은 간단한 algorithm으로 수정하여 사용할 수 있습니다.

<br>
<br>





# Fine-Tuning

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186079936-ea662592-ccfa-4d94-b941-71dd0359cf6b.png">
</p>

* 하지만, 만약에 좀더 큰 dataset을 가지고있다면 Fine-Tuning 이라는 과정을 사용하여 transfer learning을 통해 좀더 좋은 성능을 낼 수 있습니다.
* Fine-Tuning이란 다운받은 모델(ex. AlexNet)의 weight를 initial weight로 설정하고, 우리가 가진 데이터셋에 새로 훈련하는 것입니다.
    * pre-trained entire model을 new dataset에 학습시키는 것으로, 전체 model의 weights들을 backpropagation을 통해 계속 update시켜 성능을 향상시키는 것입니다.

* Fine-tuning할 때의 몇 가지 tricks
    1. Fine-tuning하기 전에, feature extractor로 한 번 사용해보면, 이 것이 baseline 성능이 됩니다.
        * 즉, Fine-tuning의 목적은 baseline보다 성능을 끌어올리는 것입니다.
    2. 낮은 Learning rate를 사용합니다.
        * 기존에 ImageNet에서 학습된 가중치들이 잘 학습되어 있고, 대부분 잘 동작하기 때문에, 우리의 데이터셋에서 성능을 높이는데에는 약간의 가중치 수정만 필요하기 때문입니다.
    3. 연산 비용을 절약하기 위해 Image input 근처의 lower level layer들은 그대로 사용할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186083197-7d3da962-30e5-4804-bb81-5ea4e9d20dc3.png">
</p>

* 위 그래프를 보면, feature extractor로만 사용했을 때보다, Fine-tuning을 했을 때 성능이 훨씬 좋아졌음을 알 수 있습니다.

<br>





# Transfer Learning with CNNs: Architecture Matters!

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186083757-259a0830-2d93-47b0-a29c-1bfa4eb4f0e9.png">
</p>

* 실제로 매우 많이 활용합니다.
* AlexNet이외에 다른 아키텍쳐들을 활용할 수 있고, Imagenet에서 잘 수행하는 아키텍쳐일 수록 다른 데이터셋에 대해서도 잘 할 가능성이 큽니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186083816-61ff4031-231c-4267-a96e-f566860b5032.png">
</p>

* 또한 Object Detection 또는 다른 Task 에서는 밑단에 feature extractor가 필요합니다.
* 이때 feature extractor를 AlexNet, Vgg, ResNet, ResNeXt로 활용했을 때, 점점 성능이 높아짐을 알 수 있습니다.

<br>





# Transfer Learning with CNNs: Strategy

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186084279-8b2b40d3-eb9b-4f6a-8bdf-d7a9ffbcf2f3.png">
</p>

* 그렇다면, CNN 아키텍쳐들을 Imagenet과 같이 다른 데이터셋에 적용을 할 때, 경우에 따라 어떤 방법을 활용하면 좋을 지를 살펴보겠습니다.
    * very similar dataset
        * 유사한 데이터셋에 대한 것입니다.
        * 즉, 데이터 distribution가 가깝기 때문에, pretrained model instance들이 더 많은 도움을 줄 것입니다.
    * very different dataset
        * 반대로 유사하지 않은 데이터셋에 대한 것입니다.
        * 데이터 distribution도 매우 다르기때문에, 다운받은 model instance들이 큰 도움을 주지 못할 것입니다. 
    * very little data
        * 일반적으로 10 ~ 100개의 카테고리 개수를 가집니다.
    * quite a lot of data
        * 일반적으로 100 ~ 1000개의 카테고리 개수를 가집니다.
* 따라서 데이터의 보유량, 데이터가 pre-trained dataset 과 비슷한지 여부에 따라 어떻게 transfer learning 을 진행할지를 정리할 수 있습니다.
    * 데이터 많고, 기존 데이터셋과 비슷할 경우
        * 더 많은 레이어들을 fine tuning에 활용하면 더 좋은 성능을 기대해볼 수 있습니다.
        * 즉 적은 양의 Layer 만 fine-tunning 하는 것이 좋습니다. 
    * 데이터 적고, 기존 데이터셋과 비슷할 경우
        * 마지막 top layer만 날리고 feature extractor로 활용하는 것이 좋습니다.
        * 즉 마지막 FC-Layer만 학습하는 것이 좋습니다.
    * 데이터 많고, 기존 데이터셋과 다른 경우
        * 데이터셋의 Domain과 많이 다르고, 데이터셋이 크다면 더 많은 레이어들을 fine tuning에 활용하면 해당 데이터셋에 맞게 활용할 수 있습니다.
        * 즉 많은 양의 layer 를 fine-tunning 하는 것이 좋습니다.
    * 데이터 적고, 기존 데이터셋과 다른 경우
        * 만약, 우리가 가진 데이터가 Transfer learning을 수행하려는 모델에 사용한 데이터와 차이가 난다면 Transfer Learning이 큰 효과를 내지 못할 수 있습니다.
            * 예를 들어, 의료영상 데이터는 ImageNet 데이터와 큰 차이가 있기 때문에, 큰 효과를 내지 못할 수도 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186086401-9c8c39e0-6b0f-4ab6-93f9-11eb992f4323.png">
</p>

* CNN에서는 Transfer learning이 매우 보편적으로 사용됩니다.
* Object detection, image caption에서도 이미지를 인식하는 feature extractor부분이 필요합니다.
    * 위 슬라이드의 모델에서도 모두 ImageNet에서 학습된 CNN을 사용하고 있습니다.
    * (좌)Object Detection, (우)Image Captioning
* 위 슬라이드의 오른쪽 초록색 박스부분을 보면 CNN뿐만 아니라 Image captioning의 language model파트에서는 NLP에서 주로 활용하는 word2vec의 pretrain된 모델을 사용하고 있는 것을 볼 수 있습니다.
* 즉 거의 모든 computer vision 관련 응용 알고리즘들이 모델들을 밑바닥부터(from scrtch) 학습시키지 않으며, 대부분은 ImageNet pretrained-model을 사용하고 현재 본인의 task에 맞도록 fine tune 합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186087160-3131ce3c-b44a-4c87-8300-ba39c0375db4">
</p>

* 이미지 모델 파트에서는 잘 pretrain된 CNN 모델을 활용해서 fine tuning을 하고, 언어 모델 파트에서는 BERT를 활용하면, Visual question answering를 잘할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186087177-209b0a8d-92ce-4075-b54b-970379bd24d5.png">
</p>

* 그러나 transfer learning은 역시 practical한 이야기이므로, 잘 작동하지 않을 수 있습니다.
* 그래프에서 회색 선이 pretrain 된 모델, 보라색 선이 random initialization으로 한 모델에 대한 성능을 나타냅니다.
    * 그래프를 보면, 트레이닝을 3배 정도 더 하면, 랜덤 초기화방법도 pretrain된 모델과 비슷한 성능을 낼수 있습니다.
    * 즉, ImagNet에 대해 pretraining한 것이 이론적으로 더 높은 성능을 낸다는 것은 아닙니다.
* 결국은 pretraining 하는 것 보다 더 많은 데이터를 수집하는 것이 더 효과적입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186087797-ab9db434-7007-4209-8e03-2a4cafce3b34.png">
</p>

* Transfer learning에 대한 Justin의 시각은 다음과 같습니다.
    * Pretrain + finetuning은 빠르고, 괜찮은 성능을 내기때문에 첫번째로 시도해볼 만 합니다.
    * 데이터셋이 많다면, random initialization으로 직접 트레이닝을 해도 괜찮은 성능을 낼 수 있습니다.
    * Transfer learning은 아직 이론적으로 많은 규명 작업이 필요합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186088135-faee77fb-bc04-4119-95f3-834b0cf60365.png">
</p>

* 따라서, 프로젝트 수행 시 데이터가 부족할 때 Transfer Learning을 사용하는 것이 좋습니다.
    * 만약 어떤 문제가 있는데, 이 문제에 대한 데이터셋이 크지 않은 경우라면 우선 프로젝트의 task와 유사한 데이터셋으로 학습된 pretrained model을 다운로드 받이서 모델의 일부를 초기화시키고, 새로운 데이터로 모델을 fine-tune 합니다.
* 또한, 전이학습은 아주 일반적인 방법이므로 대부분의 딥러닝 프레임워크에서도 다양한 pretrained model을 쉽게 다운받을 수 있도록 Model Zoo를 제공합니다.

<br>





# Summary

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/186088560-4ac6ca9d-c447-49c5-b24e-051390073581.png">
</p>

* 요약하자면 training loss를 개선 optimization을 배웠습니다.
* test data에서의 성능을 향상시키는 방법 regularization에 대해서도 배웠습니다.
* 그리고 데이터가 적을때 할 수 있는 아주 좋은 방법인 transfer learning에 대해서도 배웠습니다.




