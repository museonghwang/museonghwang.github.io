---
layout: post
title: CS231n Lecture7 Review
category: CS231n
tag: CS231n
---

해당 게시물은 [Standford 2017 CS231n](http://cs231n.stanford.edu/2017/syllabus.html) 강의와 2022년 슬라이드를 바탕으로 작성되었습니다.




<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185549686-4216e74e-998d-4f5c-852b-d74283a10cc5.png">
</p>

<br>





## Last time: Activation Functions

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185551956-31338de7-ba7b-4993-af42-6e23e8e9c684.png">
</p>

* 지난시간에 Nerural networks를 학습 시킬 때 필요한 여러가지 중요한 것들을 배웠습니다.
* 다양한 Activation Function과 각각의 특성이 존재했는데, 과거에는 sigmoid를 썼지만 Gradients Vanishing 문제때문에 요즘은 대부분 ReLU를 씁니다.

<br>

## Last time: Weight Initialization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185551978-1505b59d-9085-4fe9-a9b6-7711a8fe75a0.png">
</p>

* 가중치 초기화에 대해서도 배웠습니다.
    * 가중치가 지나치게 작으면 activation이 사라지는데, 작은 값이 여러 번 곱해지기 때문에 점점 0이 되어, 결국 모든 값이 0이 되고 학습은 일어나지 않습니다.
    * 반면에 가중치가 너무 큰 값으로 초기화되면 그 값이 또 계속 곱해질 것이고 결국은 폭발합니다. 이 경우에도 학습이 일어나지 않을 것입니다.
* Xavier/MSRA(HE) Initialzation 같은 방법으로 초기화를 잘 시켜주면 Activation의 분포를 좋게 유지시킬 수 있습니다.
* 특히 Network가 깊어지면 깊어질수록 가중치를 더 많이 곱하게 되기 때문에 가중치 초기화와 활성함수는 더 중요합니다.

<br>

## Last time: Data Preprocessing

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185551995-9a14cbb2-2c81-48c8-a5df-fcb07cbf12fd.png">
</p>

* image data는 주로 zero-mean을 주로 사용합니다.

<br>

##  Last time: Data Preprocessing

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552019-ac6538d6-73f1-4d97-8076-d8d7f94c66fd.png">
</p>

* 왜 normalization가 중요한지에 대한 직관
* Linear Binary classification 문제(빨간/파란 점들을 나누는 것)를 푼다고 가정하겠습니다.
    * 왼쪽
        * not normalized/centered data 입니다.
        * classification 자체는 가능하지만, 선이 조금만 움직여도 classification이 잘 되지 않습니다.
        * 즉, 손실 함수가 아주 약간의 가중치 변화에도 엄청 예민합니다.
        * Loss가 파라미터에 너무 민감하기 때문에, 동일한 함수를 쓰더라도 학습 시키기 아주 어렵습니다.
    * 오른쪽
        * zero-center data, Unit variance로 만들어 준 경우 입니다.
        * 선이 조금만 움직여도 손실 함수는 이런 가중치의 변동에 덜 민감합니다.
        * 이 경우 최적화가 더 쉬우며, 학습이 더 잘됩니다.

normalization은 Linear classification의 경우에만 국한되는 것이 아니라, Neural network 내부에도 다수의(interleavings) linear classifer가 있다고 생각할 수 있는데, 이 경우 Neural network의 입력이 zero-centered가 아니고 Unit variance가 아닌 경우라면 레이어의 Weight matrix가 아주 조금만 변해도 출력은 엄청 심하게 변하게 됩니다. 이는 학습을 어렵게 합니다.

<br>

## Last time: Batch Normalization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552041-6a9e0aaa-445f-4a61-adb6-8bb21c5c332e.png">
</p>

* Normalization이 엄청 중요하다는 것을 알고있기 때문에, batch normalization도 배웠습니다.
* activations이 zero mean과 unit variance가 될 수 있도록 레이어를 하나 추가하는 방법이었습니다.
    * forward pass 시에 미니배치에서의 평균과 표준편차를 계산해서 Normalization을 수행했습니다.
    * 그리고 레이어의 유연한 표현성(expressivity)을 위해서 scale, shift 파라미터를 추가했습니다.

<br>

## Last time: Babysitting Learning

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552057-dee94ad9-f572-4ddb-b8a6-7eed9b670472.png">
</p>

* 학습 과정(Loss curve가 어떻게 보여야 하는지)을 다루는 방법도 배웠습니다.
* 위 슬라이드를 해석해보면 Training set의 성능을 계속 올라가며 Loss는 계속 내려갑니다. 하지만 validation은 침체하고 있습니다.
    * 위 상황은 overfititing.
    * 추가적인 regularization이 필요합니다.

<br>

## Last time: Hyperparameter Search

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185552070-ccc79b32-f6df-4e4a-ae26-52066b4bb1a6.png">
</p>

* hyperparameter search도 배웠습니다.
* 네트워크에는 무수히 많은 하이퍼파라미터가 존재하며, 이를 올바르게 잘 선택하는 것은 상당히 중요합니다.
* grid search와 random search를 배웠으며, 이론상 random search가 더 좋았습니다.
    * 왜냐하면 성능이 특정 하이퍼파라미터에 의해 크게 좌우될 때 그 파라미터를 좀 더 넓은 범위로 탐색할 수 있기 때문입니다.
* 그리고 하이퍼파라미터 최적화 시에 coarse search 이후 fine search를 합니다.
    * coarse search
        * 처음에는 하이퍼파라미터를 가능한 최대한 넓은 범위를 설정해서 찾습니다.
        * 그 범위가 하이퍼파라미터 범위의 끝에서 끝까지 다 살펴볼 수 있도록 할수록 좋습니다.
        * Interation도 작게 줘서 학습시켜봅니다.
        * 그리고 결과가 좋은 범위로 좁히는 것입니다.
    * fine search
        * iterations를 조금 더 돌면서 더 작은 범위를 다시 탐색합니다.
    * 적절한 하이퍼파라미터를 찾을 때 까지 이 과정을 반복합니다.

<br>
<br>





# Gradient descent for optimization

* Fancier optimization을 살펴보기전에 Gradient descent를 다시 짚어보고, 문제점을 살펴보겠습니다.
* 이전에 Loss function에 대한 정의 및 역할이 제시되었습니다.
    * 최적의 $W$ 를 찾아서 classifier가 이미지들을 잘 분류하고 있는지 검사를 해야하는데, 즉, $W$ (weight)가 좋은지 아닌지 정량화 할 수 있는 기준이 필요했기에 Loss function이 등장했습니다.
    * 손실 함수(Loss Function)는 현재 분류기(classifier)가 얼마나 좋은지를 알려줍니다. 다르게 표현하면 현재의 $W$ 가 얼마나 BADNESS한지 를 알려주는 것입니다.
* 기존의 Loss function은 문제점을 가지고 있었습니다.
    * training data에 대해 좋은 성능을 만들려고 합니다.
    * 하지만 우리가 원하는것은 test data에 대한 일반화입니다.
* 이러한 문제점은 Overfiiting으로 이어집니다.
    * 즉 모델이 training data에 대해서만 잘 수행하고, unseen data에 대해 낮은 성능을 보이는 현상입니다.
* Overfitting을 방지하기 위해 Data loss term에 Regularization term을 추가하였습니다.
    * 즉, Simpler Models을 선호하게 하여 Overfit을 방지합니다.
    * 일반적으로 L2 regularization를 사용합니다.
    * 더 복잡한 regularization은 Dropout, Batch normalization, Cutout, Mixup, Stochastic depth 등등 여러가지 방법론이 존재합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185561594-c01cd0d8-1446-4d92-8c33-72bdc54cd8b9.png">
</p>

정리하면, nice(좋은)한 $W$를 찾기 위한 Loss function은 training data에 맞는 Data Loss과 Overfitting을 방지하기 위한 regularization으로 구성됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185562463-2a0fcbf4-25f7-4388-a206-4907b45b1170.png">
</p>

* 그리고 최종적으로 $L(W)$ 함숫값을 최소화하는 최적의 W를 구하는 것이 목적입니다.
* 따라서, 최적의 $W$ 는 $W^*\ =\ argmin_wL(W)$ 의 식으로 구합니다.
    * argmin : minimizer
    * $L(W)$를 최솟값으로 만드는 $W$를 $W*$로 정의합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185564759-f0057690-67e6-462e-b05d-eadddf98ef36.png">
</p>

* 그렇다면 이 $W_*$ 를 어떻게 찾을까?
    * 최적의 $W$ 값을 찾기는 어렵습니다. 하지만 찾고자 하는 방법으로 2가지가 있습니다.
        1. Random search
        2. slope를 따라내려가는 방법
    * Random search는 랜덤으로 $W$ 값의 후보를 정해서 함숫값을 구하는데, 즉, 매번 $W$ 를 조금씩 개선시켜 loss를 낮추고자 하는 것입니다.
        * 좋은 방법이 절대 아니며, 성능또한 좋지 않고, 차원의 저주에 빠질 수 있습니다.
* 위 질문은 gradient descent로 이어집니다.
    * $L(W)$ 는 주로 전체 개형을 알 수 있는 함수가 아닙니다.
    * 개형은 알지 못하고, $W=W0$ 이라는 점이 주어졌을 때 그 근접한 곳만 알 수 있습니다.
    * 여기서 할 수 있는 방법으로 slope를 따라내려가는 방법이 있습니다.
        * 경사를 따라내려가는 방법은 곧 함수에서의 derivative(미분)를 따라가는 것입니다.
        * 그래디언트의 기하학적 의미: 그래디언트 방향은 함수가 가장 증가하는 방향.
        * 다르게 이야기하면, 그 점에서 loss function이 가장 감소할 수 있는 방향을 의미하며, 따라서 negative gradient의 방향으로 점점 가면 최소점에 도달할 것 입니다.
* 그렇다면 함수의 개형을 알지 못하는 상황에서 어디가 최적점인지 어떻게 알 수 있을까?
    * 그냥 내려가봐야 합니다.
    * 그래디언트를 구해서 함수가 감소하는 방향으로 가는 것이 절대 전체적인 최적의 global minimum으로 가는 것을 보장하지는 않습니다.
    * 즉, 전체 shape에 대해 전혀 알지 못하지만, 내리막길을 따라가면 낮은 곳으로 가게 될 것이라고 믿습니다.(최적점인지에 대한 확신은 없습니다.)

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185565010-27a61b90-ef56-4394-8463-8bcc1e40793f.png">
</p>

* 즉, 그래디언트 계산법에는 2가지가 있습니다.
    * 수치적 그래디언트: 근삿값을 계산하며, 계산이 느리지만, 쓰기 쉽습니다.
    * 해석적 그래디언트: 정확한 값을 계산하고, 빠르지만, error가 발생하기 쉽습니다.
* 실제로 그래디언트를 계산할때는, 항상 해석 그래디언트를 활용하면서 gradient check를 거칩니다.

<br>
<br>





# (Full-Batch)Gradient descent

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185568772-c104c5c7-4d95-4be7-9e18-0e771ec95386.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185580899-f33c0509-3ccb-4b2a-b515-b4f5d5e32a0b.png">
</p>

* $L(W)$ 의 가장 낮은 점(최적점, global minimum)을 향해서 가는 방법 중 gradient negative한 방향을 따라가는 알고리즘인 Grdient descent를 소개합니다.
* Gradient descent는 negative gradient 방향으로 이동하면서, 계속해서(t=0, 1, 2, . . .반복) $W$ 값을 update시키는 방법입니다.
* 위의 간단한 알고리즘은 우리가 initialized_weights를 통해 아무데서나 시작하고 고정된 num of iteration (num_steps)만큼 루프를 반복하며 매 step마다 현재 위치의 gradient를 계산하고 negative gradient direction으로 조금씩 이동하게된다는 코드입니다.
* Gradient descent를 구현할 때 필요한 hyperparameter로 다음과 같습니다.
    * Weight 초기값을 설정하는 방법: 초기값이 어디냐에 따라 수렴하는 위치가 달라집니다.
    * 얼마나 반복할 것인지: steps의 수가 너무 크면, 시간이 소요되고, 너무 작으면, 충분히 내려가기전에 알고리즘이 끝날 수 있기때문에 적당히 설정하는 것이 중요합니다.
    * Learning rate: 크게 설정하면, 너무 넓게 뛰어, 오히려 loss값이 높은 곳으로 갈 수 있고, 작게 설정하면 최적점까지 가는 step이 너무 오래 걸립니다.
* 위 슬라이드의 우측 이미지는 Gradient descent의 기하학적인 해석입니다.
    * 현재 $L(W)$ 함수의 형태는 2차원 convex함수로, Negative gradient방향을 따라 이동하면서 $W$ 를 update시키면 optimal로 갈 수 있습니다.

<br>
<br>





# (mini-Batch)Stochastic Gradient descent

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185587631-a703695a-b728-4d71-8d2c-15f5cde5217c.png">
</p>

* 지금까지 설명한 Gradient Descent방식은 모든 트레이닝 데이터를 계산에 활용하는 Full-Batch Gradient Descent입니다.
* Training data 사이즈가 매우 크다면, 파라미터 W를 한 번 update하기 위해 모든 트레이닝 데이터를 활용하는 것은 비용측면에서 효율적이지 못합니다.
* Full-batch GD방법은 시간이 너무 오래 걸리기때문에, 트레이닝 셋의 모든 N개를 실행하지않고, 트레이닝셋에서 샘플을 랜덤으로 추출하여 실행하는 Stochastic Gradient Descent(SGD) 방법을 주로 활용합니다.
    * 트레이닝셋에서 랜덤으로 추출한 것을 미니배치라고 부르며, 미니배치 사이즈는 주로 32, 64, 128개로 정합니다.
* SGD를 활용하면 미리 정해야 할 hyperparameter가 더 늘어납니다.
    * Weight 초기값을 설정하는 방법
    * 얼마나 반복할 것인지
    * Learning rate
    * Batch size: 미니배치의 사이즈
    * Data sampling: 데이터 샘플링하는 방법

원래 Stochastic Gradient descent는 mini-batch를 사용하는 것이 아닌 전체 training set에서 랜덤하게 뽑은 단 1개의 데이터의 gradient를 계산하고 weight update시키는 방식을 training set 크기만큼 반복하는 것이라 이러한 random성 때문에 stochastic이라는 이름이 붙은 것 입니다. 하지만 mini-batch gradient descent가 굉장히 보편화되며 SGD라는 용어가 mini-batch gradient descent를 의미하는 경우가 많아졌습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185587731-f3f9e741-aaa3-4f46-b44b-6ca98679cbf2.png">
</p>

* SGD에서는 loss function을 확률론적으로 접근하기때문에 stochastic 이라는 이름이 붙여졌습니다. 우리가 data set을 확률분포로 부터 샘플링된 것으로 생각했을때 우리는 loss function을 모든 가능한 sample들에 대한 expectation으로 생각할 수 있으며, 수식과 같이 표현 할 수 있습니다.
    * mini-batch 데이터는 임의의 확률분포 $P$ 를 따르는 확률변수 $X$, $Y$ 를 따릅니다.
    * $∇_WL(W)$ : 우리가 모르는 $P$ 라는 분포에서 N개의 데이터를 샘플링했을 때, 그 샘플링된 데이터에서 그래디언트를 계산하여 모집단 평균 그래디언트를 sample 평균 그래디언트로 근사합니다.

<br>
<br>





# Optimization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185590450-6455863b-356c-4890-8618-e26d6ea2840e.png">
</p>

* 즉 Neural network에서 가장 중요한 것은 바로 최적화 문제입니다.
    * Nerwork의 가중치에 대해서 Loss function를 정의해 놓으면 이 Loss function은 그 가중치가 얼마나 좋은지 나쁜지를 알려줍니다.
    * 그리고 Loss function이 가중치에 대한 "산(landscape)"이라고 상상해 볼 수 있습니다.
* 오른쪽 이미지인 2차원의 문제를 두 개의 가중치 W_1과 W_2를 최적화 시키는 문제라고 생각해봅니다.
    * X/Y축은 두 개의 가중치를 의미하며, 각 색깔은 Loss의 값을 나타냅니다.
    * 우리의 목적은 가장 붉은색인 지점을 찾는 것입니다. 즉 가장 낮은 Loss를 가진 가중치를 찾는 것입니다.

가장 간단한 최적화 알고리즘인 Stochastic Gradient Descent를 이용해 미니 배치 안의 데이터에서 Loss를 계산하고, "Gradient의 반대 방향"을 이용해서 parameter vector를 업데이트합니다. 이 단계를 계속 반복하면 결국 붉은색 지역으로 수렴할 것이고 Loss가 낮아질 것입니다.

정리하면, Optimization은 업데이트(학습)를 통해 손실함수(Loss function) 의 가장 낮은 곳의 $W$ 값 까지 도달하는 것이 목표이고, 이것을 어떻게 가장 효율적인 방법으로 도달할 수 있느냐에 대한 방법론입니다.

<br>
<br>





# Optimization: Problem #1 with SGD

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185744611-e65fd2c4-849a-4411-9bc0-97f4a52a3ac2.png">
</p>

* 하지만, Stochastic Gradient Descent에는 문제가 있습니다.
    * 위 슬라이드와 같이 가로 방향으로 긴 형태(like taco shell)의 손실함수에서는, 가로 축으로의 이동보다 세로 축으로의 이동이 더욱 큰 영향을 주게 됩니다.
        * 이 경우, loss function의 경사가 수직 방향으로의 gradient는 매우 크고, 수평 방향으로의 gradient는 매우 작아, 수평 축의 가중치는 변해도 Loss가 아주 천천히 줄어듭니다.
    * 즉, 수평 방향의 가중치(W_1)가 변하더라도 Loss는 아주 천천히 줄어들기 때문에, 수평 방향의 가중치(W_1)보다 수직 방향의 가중치(W_2) 변화에 더욱 민감하게 반응할 것입니다.
    * 즉 업데이트가 잘 안되는 경우이며, 이를 poor conditioning 라고 부른다.
* 현재 지점에서 Loss는 bad condition number를 지니고 있다고 말할 수 있습니다.
    * condition number: 입력값의 작은변화에 대한 출력값의 변화의 정도를 측정하는 지표로 시스템이 민감한 정도를 정량적으로 보여주는 값을 뜻합니다.
    * 즉 이 지점의 Hessian maxrix의 최대/최소 singular values값의 비율이 매우 안좋다는 뜻입니다.
    * 즉, $∇^2L(W)$ 의 $\frac{largest eigenvalue}{smallest eigenvalue}$
    * ex. $L(W) = \frac{W_1^2}{64} + W_2^2$
    * $∇^2L(W) = \begin{bmatrix} \frac{1}{32} & 0 \\ 0 & 2 \end{bmatrix} $
    * condition number = $\frac{2}{\frac{1}{32}} = 64$

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185744478-f89173c2-b0bd-48f6-8e7c-1c5524bfab88.png">
</p>

* 이런 상황에서 SGD으로 학습이 되는 모습을 생각해볼때, gradient의 방향이 고르지 못하기 때문에 위 그림과 같이 지그재그로 수렴하는 형상을 띄게 됩니다.
    * 즉 step에 따른 Weight of matrix 의 변화가 zigzag pattern or 지저분하게(nasty) 보이며 변화 할 수 있습니다.
    * 이는 바람직하지 않으며, 더 많은 step이 소요되며, 고차원의 공간에서 주로 발생합니다.
    * 또한 이 때문에 SGD는 Full Batch보다 Overshoot문제에도 더 취약합니다.
* 위의 예에서는 단순히 2차원이지만, 실제로는 가중치가 수억개일 수 있습니다.
    * 이때는 수억개의 방향으로의 불균형한 방향이 존재할 수 있으며, 수억개의 방향으로 움직일 수 있으므로, SGD는 잘 동작하지 않을 것입니다.
    * 고차원 공간에서 발생하는 이런 문제는 실제로도 큰 문제가 됩니다.

<br>





# Optimization: Problem #2 with SGD

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185747069-db707f35-8c15-4ac9-8fbc-755213465321.png">
</p>

* SGD의 또다른 문제는 Local Minima와 Saddle Point 입니다.
    * Local minima는 업데이트 중에 작은 valley를 만나 gradient가 0이 되어 학습을 멈추게 되는 문제입니다.
        * 즉, global minimum point 가 아닌 local minimum point 입니다.
    * Saddle point는 gradient가 0이 되는 지점에서 학습을 멈추는 문제입니다.
    * 즉, 최적이 아닌 위치에서 그래디언트 값이 0이 되면, 여기에 갇혀서 더 이상 W값이 업데이트 되지 못 합니다.
* 위 그림과 같이 1차원의 예에서는 local minima가 더욱 심각해 보이지만, 고차원 공간에서는 그 반대입니다.
    * 고차원 공간에서 Saddle point는 어떤 방향은 loss가 증가하고 몇몇 방향은 loss가 감소하고 있는 지점으로 생각할 수 있는데, 수억차원에서 생각해보면 이는 거의 모든 곳에서 발생한다고 할 수 있습니다.
    * 반면, 고차원 공간에서 local minima는 수억개의 방향을 계산했는데 이 방향이 모두 loss가 상승하는 방향인 경우이므로, 매우 드물게 발생합니다.
* 매우 큰 규모의 신경망 모델에서는 local minima보다 saddle point에 더욱 취약한 것으로 알려져 있습니다.
    * Saddle point에서 gradient가 0이 되는 것도 있지만, saddle point 근처에서 gradient가 아주 작아지기 때문에 업데이트가 느려져서 문제가 됩니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185747920-5b457e04-642b-49a1-8683-01962d067c32.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185747834-24184899-6dbb-4c2b-b30f-e408484151b5.png">
</p>

* 또한, SGD에서(데이터의 크기가 매우 커서) mini-batch로 gradient를 계산할때 small estimate of full data set만을 사용하기 때문에 모든 step에서의 gradient가 minima로 향하는 올바른 direction과는 상관관계가 없기에 그림에서 볼 수 있듯이 noisy 합니다.
    * 이는 mini-batch의 데이터만으로 실제 loss와 gradient를 추정하는 것입니다.
    * 즉 매번 정확한 gradient를 얻을 수가 없다는 것을 의미하며, 부정확한 추정값(noisy estimate)인 gradient를 얻게 된다는 문제가 있습니다.
* 따라서, 위 슬라이드의 오른쪽 그림과 같이 손실함수 공간을 비틀거리면서 minima로 수렴하기 때문에 학습 시간이 오래걸리게 됩니다.

<br>





# Next Optimizer

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185748116-b2e63bce-e678-4b19-be5b-de1b951f3b6d.png">
</p>

* 이러한 SGD의 여러가지 문제점을 해결하기 위해 여러 개의 optimizer들이 차례로 등장하게 됩니다.
* 개선된 optimizer들은 각각의 특징들이 있는데, 크게 방향을 중심으로 하느냐, 보폭을 중심으로 하느냐로 나뉩니다.
* 지금부터 SGD를 개선한 optimizer를 살펴보겠습니다.

<br>
<br>





# SGD + Momentum

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185775788-24aab157-435b-4208-a40f-3f74c79e51a6.png">
</p>

* SGD의 여러 문제점들을 해결하는 간단한 방법은 SGD에 momentum term을 추가하는 것입니다.
* SGD + Momentum의 아이디어는 단순히 velocity를 유지하는 것입니다.
    * 즉, 현재 mini-batch의 gradient 방향만 고려하는 것이 아니라 velocity도 같이 고려하여, gradient descent가 되는 과정에서 생기는 속도(velocity)를 어느 정도 유지하자는 의미입니다.
    * $ρ$ : momentum의 비율(velocity의 영향력) or 속도(velocity)를 얼마나 고려할 것인가의 비율을 나타냅니다.
        * 보통 0.9 or 0.99 로 설정합니다.
    * $V_t$ : velocity
* 즉, Momentum 최적화는 이전 gradient가 얼마였는지를 상당히 중요하게 생각합니다.
    * Momentum의 핵심 idea는 weight를 업데이트 할 때 이 전에 계산한 gradient도 반영을 해주자는 것으로 다르게 말하면, $V_t$ (현재 속도)에 그래디언트가 누적되면서,과거의 그래디언트는 점점 잊혀지는 알고리즘입니다.
    * 식을 살펴볼때, 매 스텝마다 gradient가 동일한 부호를 가지면 v의 절댓값은 계속 증가할 수 밖에 없습니다. v가 증가함에 따라 x의 변화폭이 커지고, 이는 즉 같은 방향으로 이동할수록 더 많이 이동하게 하여 가속화한다는 말과 같습니다.
    * 또한 gradient가 0이라 하더라도, v갑이 더해지면서 멈추지 않고 이동할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185774708-858adba1-5ebe-4435-8d61-64d056e7cb1e.png">
</p>

* SGD + Momentum은 정해진 방법으로만 사용하는 것이 아니라 세부 구현은 약간씩 다를 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185774779-dce1c710-658a-4a24-a3e9-667f96be3e36.png">
</p>

* SGD + Momentum은 다음과 같은 문제들을 해결할 수 있습니다.
    * Local minima와 saddle point
        * 위 슬라이드의 왼쪽 위 그림에서, 빨간색 공은 local minima나 saddle point에 도달하더라도 여전히 velocity를 가지고 있기 때문에 gradient가 0이더라도 계속해서 나아갈 수 있습니다.
    * Poor conditioning 문제
        * 위 슬라이드의 왼쪽 아래 그림에서, 지그재그로 수렴하는 움직임도 momentum을 통해 상쇄할 수 있기 때문에 민감한 수직방향의 변동은 줄어들고 수평방향의 움직임은 점차 가속화 됩니다.
    * Gradient Noise
        * 위 슬라이드의 오른쪽 그림에서, 파란색 선은 Momentum이 추가된 SGD이고 검정색 선은 그냥 SGD입니다.
        * Momentum항이 추가되면 noise를 평균내버려서 그냥 SGD보다 더 smooth하게 수렴하게 됩니다.

<br>
<br>





# Nesterov Momentum

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185775198-0d54668d-496c-411f-bce1-b07dfc85eab7.png">
</p>

* 좌측 슬라이드는 SGD + Momentum이 나아가는 형태를 직관적으로 보여줍니다.
    * Momentum 업데이트는 현재 지점까지 그동안 누적되어온 velocity와 gradient값이 더해져서 실제 step을 이루는 방식이었습니다.
    * 실제 업데이트는(autual step) 현재 지점에서의 gradient의 방향과 Velocity vector의 가중평균으로 구할 수 있습니다. 이는 gradient의 noise를 극복할 수 있게 해줍니다.
* 우측 슬라이드는 Nesterov Momentum으로, 앞에서 살펴본 momentum을 추가하는 방식의 변형입니다.
    * NAG(Nesterov Accelerated Gradient) 라고도 부릅니다.
    * 원점에서 구한 gradient와 velocity를 더하는 SGD+Momentum와 달리, Nesterov는 원점에서 velocity를 구한 다음 velocity의 지점에서 gradient를 계산하고 다시 본래 자리로 돌아와 actual step을 나아가는 형태입니다.
        * 이 방법은 velocity의 방향이 잘못된 경우에 현재 gradient의 방향을 활용하여 좀 더 올바른 방향으로 가게하는 것으로 이해할 수 있습니다.
        * 즉 누적된 과거의 gradient가 가리키는 방향을 현재 gradient에 보정하는 효과를 가지게 됩니다.
    * Nesterov Momentum은 convex optimization에서는 뛰어난 성능을 보이지만, neural network와 같은 non-convex problem에서는 성능이 보장되지 않습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185776814-8fc280b8-083d-4a5d-81f8-a1d5c581914b.png">
</p>

* 위 슬라이드는 Nesterov Momentum을 구하는 식입니다.
* 위쪽 검은색 박스의 파란색 박스 식을 보면, gradient를 한 지점에서 구하지 않는 것을 확인할 수 있습니다. 이러한 형태는 까다로우므로, 변수를 적절히 바꿔주어 아래쪽 검은색 박스의 식과 같이 gradient를 한 지점에서 구하는 식으로 다시 나타낼 수 있습니다.
* 아래쪽 검은색 박스의 식으로부터 Nesterov는 다음과 같이 동작한다.
    * $v_{t+1}$ 은 Vanilla SGD+Momentum과 같이 velocity와 gradient를 일정 비율로 섞어 준 것으로 이해할 수 있습니다.
    * $\tilde{x}_{t+1}$ 는 마지막 식의 형태만 보면 됩니다.
        * 현재 위치 $\tilde{x}_t$ 와 velocity $v_{t+1}$ 를 더한 값에 (현재 velocity $v_{t+1}$ - 이전 velocity $v_t$)에 일정 비율 $ρ$ 를 곱한 값을 더해주며 구하게 됩니다.
* 따라서, Nesterov는 현재와 이전의 velocity간 error-correcting term(에러 보정 항)이 추가된 것으로 이해할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185776826-93545493-3984-4b19-98c0-5264d18df347.png">
</p>

* 위 슬라이드는 SGD, Mementum, Nesterov의 움직임을 나타낸 것으로, SGD+Momentum과 Nesterov는 빠르게 수렴하는데 반해 SGD는 느리게 수렴합니다.
* 또한, SGD+Momentum과 Nesterov는 velocity 때문에 minima를 지나친 후, 다시 경로를 틀어 minima로 수렴하는 형태를 보입니다.
    * SGD+Momentum과 Nesterov는 아주 좁고 깊은 minima를 지나칠 수 있습니다. 하지만, 그러한 minima는 아주 overfit된 경우이기 때문에 test data에서 좋은 일반화 성능을 보이는 지점이 아닙니다.
    * 따라서, 위 슬라이드 그림과 같이 넓고 평평한 지점에서의 minima를 찾는 것이 우리의 목적이며, 좁고 깊은 minima를 건너뛰면서 우리가 원하는 minima를 찾는데 momentum 방법들이 도움이 된다는 것도 좋은 점이라고 볼 수 있습니다.

<br>
<br>





# AdaGrad

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185777780-1fe84275-bd0a-4518-a967-2695929ecaac.png">
</p>

* 또다른 최적화 방법으로 AdaGrad가 있습니다.
* AdaGrad는 Adaptive Gradient를 뜻하며, gradient descent가 되는 정도를 상황에 맞게 조정하는 것입니다.
    * 조금더 엄밀히 말하면, 이전의 learning rate가 일괄적으로 적용되고 있어 overshooting 되었던 문제를 learning rate를 가변적으로 적용하여 해결하고자 합니다.
    * velocity term 대신에 grad squared term을 이용합니다.
    * 이 방법은 학습 중에 계산되는 모든 gradient의 제곱을 계속해서 누적합시켜 더해갑니다. 그리고 gradient를 루트를 취해 나눠준 값으로 업데이트 하여 step마다 learning rate를 가변적으로 적용하는 방법입니다.
        * 이는 기울기가 가파를수록 조금만 이동하고, 완만할 수록 조금 더 이동하게 함으로써 변동을 줄이는 효과가 있습니다.
        * 또 행렬곱 연산을 통해 가중치마다 다른 학습률을 적용한다는 점에서 더욱 정교한 최적화가 가능해집니다..
* 즉, 학습이 진행됨에 따라 gradient의 제곱이 계속 커질 것이고, 업데이트 되는 gradient는 점점 작아지는 것입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185777864-78ede298-d964-42dc-9c9d-149fe5250556.png">
</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185777875-bb5bb8d8-02bd-4ade-81c7-8326fd6d2f23.png">
</p>

* 위 슬라이드의 그림에서, gradient를 가로, 세로 축으로 생각하고 AdaGrad의 동작을 이해해보면 다음과 같습니다.
    * 가로 축으로는 gradient가 항상 작고, 세로 축으로는 gradient가 항상 큽니다. 따라서, 제곱해서 더해지는 항이 가로 축에서는 작을 것이고, 세로 축에서는 클 것입니다.
        * y축 방향("steep"): grad_square is big
            * 따라서 grad_square로 나누면, 가려던 방향으로 원래 가려던 것보다 덜 갑니다.
        * x축 방향("flat"): grad_square is small
            * 따라서 grad_square로 나누면, 가려던 방향으로 원래 가려던 것보다 더 갑니다.
    * 이로 인해, 세로 축 방향으로의 움직임에서는 gradient가 큰 값으로 나누어 진행을 약화시키고, 가로 축 방향으로의 움직임에서는 작은 값으로 나누어 accelerating motion 효과를 줍니다.
    * 따라서, 중앙 지점까지 가로축 세로축 모두에서 적절한 속도로 수렴하게 되는 것입니다.
* 하지만, AdaGrad는 Step을 진행할 수록, 계속해서 값이 작아진다는 문제가 있습니다.
    * Convex case에서는 minimum에 근접하면서 속도를 줄일 수 있기 때문에 좋은 방법이지만, Non-convex case에서는 saddle point에 걸렸을 때, 더이상 진행하지 못하기 때문에 좋은 방법이 아닙니다.
* 또한 non-covex problem 경우에서 학습이 길어질 경우, 학습 속도가 점점 줄어들어 결국 멈추어버린다는 문제점도 있습니다.
* 따라서, Neural Network를 학습시킬 때에는 AdaGrad를 잘 사용하지 않습니다.

<br>
<br>





# RMSProp: “Leaky AdaGrad”

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779062-42978bf4-5971-42a0-a39e-c0932b6f5d1c.png">
</p>

* RMSProp은 AdaGrad의 앞선 문제점을 개선시킨 최적화 방법으로 Leaky AdaGrad라고 합니다.
    * AdaGrad는 과거 정보를 계속 누적하는 반면
    * RMSProp은 과거정보를 조금씩 축소시켜서 받아들이기 때문입니다.
* AdaGrad에서처럼 제곱 항을 그냥 누적하지 않고, decay_rate를 곱해서 누적하는 형태로 문제점을 개선하였습니다.
    * 누적된 gradient 값에 decay rate를 곱해주고, 현재 gradient의 제곱값에 1 - decay rate를 곱하여 누적해서 더해줌으로써 아주 미세하게(leaky) step size를 조절할 수 있게 되었습니다.
    * 이때, decay_rate는 주로 0.9나 0.99를 사용합니다.
    * 이전 스텝의 기울기를 더 크게 반영하여 h 값이 단순 누적되는 것을 방지할 수 있습니다.
* 즉, RMSProp은 gradient의 제곱을 나눠준다는 점은 동일하지만, 속도가 줄어드는 문제는 해결한 형태라고 볼 수 있습니다.
    * 보폭을 갈수록 줄이되, 이전 기울기 변화의 맥락을 살피자는 것입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779278-c94b8f3d-bd17-478e-ad14-d8e2a7142df0.png">
</p>

* 위 슬라이드는 SGD, SGD+Momentum, RMSProp을 비교한 것입니다.
* SGD+Momentum은 한번 overshoot 한 뒤에 다시 minima로 수렴하는 궤적을 그리지만, RMSProp은 각 step마다 각 차원의 상황에 맞도록 적절하게 궤적을 수정하면서 수렴하는 형태를 보입니다.

<br>
<br>





# Adam = RMSProp + Momentum

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779705-3f8d131e-3ffe-476f-9fad-b8da2801a032.png">
</p>

* Adam 알고리즘은 RMSProp과 Momentum이 합쳐진 개념이라고 생각할 수 있습니다.
* 위 슬라이드의 식은 완전한 Adam은 아니지만, Adam의 컨셉을 나타낸 것입니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779713-f48cc720-d0a4-4e73-bbe8-db24b02659b4.png">
</p>

* Momentum에서 과거 velocity와 현재의 gradient를 더해서 나아가는 방식을 차용합니다.
* 과거의 정보는 조금씩 잊으면서, 현재 점에서의 새로운 그래디언트로 보강하여 $W$ 를 업데이트합니다. 

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185779723-eb1960ae-d0a6-4236-a78c-a27ff7cc9e8b.png">
</p>

* AdaGrad와 RMSProp에서는 gradient를 제곱하여 과거의 정보를 누적하면서 $W$ 를 업데이트시에 가려던 방향으로 나눠주는 방식을 차용합니다.
* (과거는 축소시키면서) dw의 제곱값을 누적시키고, 분모로 moment2 값을 사용하여 $W$ 를 업데이트합니다. 

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185780891-94ebc230-2d4b-4af7-a77c-ca7af76e89f9.png">
</p>

* 하지만, 문제가 있습니다. 만약 Beta2가 0.999일때, 초기 step에서 어떤 일이 발생할지 생각해봅니다.
    * moment2 는 처음에 0으로 초기화 됩니다. 그리고 beta2(decay_rate에 해당)도 보통 0.9~0.99의 값을 가지므로 및 (1 - beta2) * dx * dx 항도 0에 가까울 것입니다. 따라서, moment2 는 처음에 1번 업데이트 한 이후에도 여전히 0에 가깝게 됩니다.
    * 이로 인한 문제는 바로 다음의 업데이트 단계에서 일어나는데, 매우 작은 moment2 로 나누게 되어 초기 step이 매우 커지게 되는 문제인데, 이는 한번 발생하면 매우 나쁜 상황이 됩니다.
    * 매우 커진 초기 step으로 인해 초기화가 엉망이 될 것이고, 전혀 엉뚱한 곳으로 이동하게 될 수도 있습니다. 이는 수렴할 수 없는 현상을 초래하기도 하므로 매우 좋지 못합니다.
* 위 수식에서는 moment1, moment2 둘 다 0 에 수렴하기 때문에 상쇄될 수도 있지만, 상황에 따라 엄청 큰 step size가 발생하는 경우도 있어서 아주 큰 문제가 됩니다.
* 따라서 실제 Adam 에서는 first moment와 second_moment에 bias correction term을 추가해 이것을 보정합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185781020-c7d9169f-fbe9-4f84-a56c-d630afcf8133.png">
</p>

* 위 슬라이드는 완전한 형태의 Adam입니다.
* 앞에서 살펴본 (완전하지 않은) Adam은 초기 step이 매우 커질 수 있다는 문제가 있었다. 따라서, 실제 (완전한 형태의) Adam은 이를 해결하기 위해 보정하는 항(bias correction term)을 추가한 형태입니다.
* 위 식을 하나씩 살펴보면 다음과 같습니다.
    * 먼저, first_moment와 second_moment를 업데이트 합니다.
    * 그리고 t(현재 step)에 맞는 적절한 unbiased term(first_unbiased과 second_unbiased)을 계산합니다.(앞에서의 문제를 해결하기 위해 추가된 부분)
    * 마지막으로, 앞서 구한 unbiased term을 통해 업데이트를 수행합니다.
* 즉, (완전한 형태의) Adam은 moment1와 moment2만 계산하는 것이 아니라, unbiased term을 계산해서 동작하는 것입니다.
* Adam은 다양한 문제들에서 잘 동작하기 때문에 아주 좋습니다.
    * 특히, beta_1 = 0.9, beta_2 = 0.999, learning_rate = 1e-3, 5e-4, 1e-4 정도로만 설정하면 거의 모든 모델에서 잘 동작하는 기본 설정이 될 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185781427-a7d1ec57-530c-461a-b24d-091f1131d8f5.png">
</p>

* 동일한 환경에서 SGD, Momentum, RMSProp, Adam을 비교한 그림입니다.
* Adam의 궤적은 SGD+Momentum과 RMSProp의 궤적을 절충한 형태로 그려집니다.
    * overshoot하기는 하지만 SGD+Momentum보다는 정도가 약합니다.
    * RMSProp처럼 각 차원의 상황에 맞도록 적절하게 궤적을 수정하면서 step을 이동합니다.

<br>





# Optimization Algorithm Comparison

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185782171-d41d257f-7c4a-4a49-beda-4ccfb45c7c2e.png">
</p>

* 지금까지 소개한 최적화 알고리즘들을 비교하겠습니다.
    1. 첫번째의 속도를 추적할 수 있는가?
        * SGD+Momentum, Nesterov, Adam
    2. Elementwise하게 곱해서 제곱term을 보관하는가?(너무 많이 간 방향으로는 많이 가지 못하게 보정)
        * AdaGrad, RMSProp, Adam
    3. 과거를 잊고있는가?
        * RMSProp, Adam
    4. 처음 가려는 방향(초기step)에 대해 너무 느린 것을 보정하는가?
        * Adam

<br>
<br>





# Learning Rate Schedules

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185783775-a2c47ee9-6703-4f43-857d-692297c57273.png">
</p>

* 이전에 보았던 모든 optimization기법들은 learning rate를 hyperparameter로 가지고 있었습니다. 이것은 deep learning model에서 가장 중요한 우선순위의 hyperparameter로 다루어지고 있습니다.
* 학습 과정에서 learning rate을 하나의 값으로만 정해놓는다면, 좋은 값을 설정하기가 어렵습니다. 그렇다면 "어떻게 최적의 learning rate를 찾아야할까?" 라는 질문에, Learning rate decay가 좋은 전략이 될 수 있습니다.
    * training 초반에는 green line을 따르는 high learning rate을 사용하여 빠르게 학습시켜야하고, 시간이 지남에 따라 blue line을 보이는 low learning rate로 learning rate를 decay 시키는, 즉 red line을 따르는 것이 이상적인 방법입니다.
* Learning rate decay는 처음에 learning rate을 높게 설정한 후, 학습이 진행될수록 learning rate를 점점 낮추는 방법이며 다음의 두가지 방법이 있습니다.
    1. 특정 순간마다 learning rate을 감소시키는 방법입니다.
        * 예) Step decay : 몇번의 epoch마다 learning rate을 감소시킵니다.
    2. 꾸준히 learning rate을 감소시키는 방법입니다.
        * 학습 동안에 꾸준히 learning rate을 감소시키는 방법입니다.
        * 예) exponential decay와 1/t decay : 위 슬라이드의 식 참고
        * 꾸준히 learning rate을 감소시키는 방법에는 다양한 전략이 있을 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784167-d88a0801-90e0-4422-a029-25688b028975.png">
</p>

* 위 슬라이드는 ResNet 논문에 있는 학습 그래프로, 화살표로 표시된 지점은 step decay가 적용되어 learning rate가 줄어든 지점입니다.
* learning rate가 decay되어야 하는 순간은 다음과 같습니다.
    * 현재 수렴을 잘 하고 있는 상태에서 gradient가 작아졌고, learning rate가 너무 높아서 더 깊게 들어가지 못하는 상태(bouncing around too much)에 decay가 되어야 합니다.
    * 이때, learning rate를 낮추게 되면 속도가 줄어들어 더 깊게 들어가며 loss를 낮출 수 있습니다.
* Learning rate decay와 관련된 추가적인 내용들
    * Learning rate decay는 Adam보다 SGD Momentum을 사용할 때 자주 사용합니다.
    * Learning rate decay는 두번째로 고려해야 하는 하이퍼파라미터입니다.
        * 학습 초기에는 learning rate decay를 고려하지 말고 learning rate 자체를 잘 선택해야 합니다. 그 이유는 초기 learning rate와 decay를 cross-validate하려고 하면 너무 문제가 복잡해 지기 때문입니다.
        * 따라서, Learning rate decay를 적용할 때에는 먼저, decay 없이 학습을 시도하고, loss curve를 살펴보면서 decay가 필요한 곳이 어디인지 고려해서 사용해야 합니다.
* 이렇게 training process동안 시간이 지남에 따라 learning rate를 변경시키는것을 learning rate schedule 이라고 합니다.

<br>

## Learning Rate Decay: Step

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784462-c1c63e14-96a2-4195-9f15-6cfe245d2487.png">
</p>

* Learning rate를 decay시키는 방법인 Step방법으로, noncontinous하게 decay시키는 방법입니다.
    * 일정 범위의 epoch마다 discrete value를 learning rate로 사용하고 점차 decay시켜 계단모양의 learning rate를 보이게 되는 decay schedule입니다.
* 어떤 고정된 지점들에서 learning rate를 낮추는 것입니다.
    * 예를 들면, ResNet에서는 epoch가 30씩 지날때마다, LR를 0.1씩 곱해서 감소시킵니다.
* step learning rate schedule은 step마다 learning rate를 고정시킬 epoch의 범위, step마다 learning rate 를 감쇠시킬 decay rate가 새로운 hyperparameter로 추가되는 문제가 있습니다.

<br>

## Learning Rate Decay: Cosine

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784528-bb54a495-859d-45d6-8634-5c28060315f8.png">
</p>

* 다음은 Cosine방법으로, continous하게 decay시키는 방법입니다.
* Step learning rate schedule과는 달리 decay rate나 epoch의 범위를 설정해 줄 필요 없이 위 그림의 공식을 따라 매 epoch마다 learning rate을 decay시키게 됩니다.
    * T는 training하는 전체 epoch의 수, t는 현재 epoch, $\alpha_0$ 은 사전 지정이 필요한 hyperparameter입니다.
    * 사전에, $alpha_0$과 T만 지정하면, epoch가 지남에 따라 LR이 감소하며, loss도 smooth하게 감소합니다.
* tuning해주어야 할 hyperparameter가 적기 때문에 최근에 많이 사용된다고 합니다.

<br>

## Learning Rate Decay: Linear

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784536-212a5f90-62e5-46ed-bac9-eb3bd32370f7.png">
</p>

* 다음은 Linear방법으로, continous하게 decay시키는 방법입니다.
* 직선의 형태로 learning rate가 감소합니다.
    * 기울기로 활용할 $alpha_0$ 와 T가 hyperparameter로 필요합니다.
* linear learning rate schedule은 간단하지만 자주 사용되며 각 model마다, 각 problem마다 사용하는 learning rate schedule은 다르며, 이런 learning rate schedule들은 어느게 좋은지 비교할 수는 없습니다.
    * computer vision분야에서는 Cosine learning rate decay schedule을 많이 사용합니다.
    * large-scale netural language processing에서는 linear learning rate schedule을 많이 사용합니다.

<br>

## Learning Rate Decay: Inverse Sqrt

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784545-0654f202-a4fa-4e9c-acc1-dfa7a3845dd2.png">
</p>

* 다음은 Inverse Sqrt방법으로, LR를 초반에 빠르게 decay시킵니다.
* Cosine schedule, Linear schedule 과 다르게 거의 사용되지 않습니다.

<br>

## Learning Rate Decay: Constant

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784551-0273f761-64ee-4dae-ab52-a5ad53f0f7d4.png">
</p>

* 다음은 Constant방법으로, epoch가 진행되어도 LR는 같은 값을 유지합니다. 
* Constant learning rate schedule은 가장 흔하게 볼 수 있으며, Constant schedule은 다른 복잡한 laerning rate schedule과 비교해 model, problem에 따라 성능이 좋아질 수도 나빠질 수도 있으며, 가능한 빠르게 model을 만들어야 할때는 좋은 선택이 됩니다.
* 또한 momentum같은 optimizer를 사용할때는 복잡한 learning rate decay schedule을 사용하는것이 좋지만 Adam or RmsProp를 사용할때는 (Bias correction이 붙었으므로) learning rate decay가 중요하지 않기에 그냥 Constant learning rate를 사용하는것이 좋습니다.


<br>

## Learning Rate Decay: Linear Warmup

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185784986-db1ba28d-a0ef-4bde-a6ef-92923c2f67fc.png">
</p>

* Goyal et al, “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour”, arXiv 2017
* 다음은 Linear Warmup으로, 높은 초기 learning rates는 Loss를 explode하게 만들 수 있기때문에, 첫 번째 ~ 5,000회 동안 학습 속도를 0에서 선형적으로 증가시키면 이를 방지할 수 있습니다.

<br>
<br>





# Early Stopping

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185785176-3a70c5b4-e8d4-4b8a-8ce5-eb5f581b7932.png">
</p>

* 결국 좋은 optimization과 learning rate decay 등의 방법론을 쓸때, Loss는 계속 낮아지겠지만, validation set의 accuracy가 감소하려고할때 즉, Loss decay와 accuracy plot을 함께 보면서 overfitting되기 전에 iteration을 멈추어야합니다.
* 학습시 우리가 얼만큼의 epoch or iteration으로 model을 훈련시켜야 할 지 모르겠을때 사용할 수 있는 좋은 mechanism으로 Early Stopping이 있습니다.
* 따라서, 매 iteration마다의 모델의 스냅샷을 저장한 후, val set에서 가장 잘 워크할 때의 iteration때의 weight를 불러옵니다.

<br>
<br>





# why use First-Order Optimization?

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786455-e6a2b125-dd1a-40d3-bd89-85c4416d806e.png">
</p>

* 그렇다면 왜 2차 근사 대신 1차 근사를 활용하여 최적화를 하는지 살펴보겠습니다.

<br>





# First-Order Optimization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786245-8c714290-3fce-4f03-bb3f-a4d42cd26a97.png">
</p>

* 지금까지 배운 최적화 기법들은 모두 1차 미분을 이용한((first-order)) 형태였습니다.
* 즉, 위 슬라이드와 같이 현재 지점에서 gradient를 계산하고, 이를 통해 loss 함수를 선형 함수로 근사시키는 방법이었습니다.(일종의 1차 first-order Taylor approximation)
* 하지만 1차 근사 함수의 미분값으로는 멀리 나아갈 수 없습니다. (수렴 속도가 느리다)

<br>





# Second-Order Optimization

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786648-9aeb7e36-1bbf-4d2a-8055-807d20baa839.png">
</p>

* 1차 근사 함수의 미분보다 조금 더 빠른 방법으로, 2차 근사 함수를 추가적으로 사용하는 것을 생각해 볼 수 있습니다. (Second-order optimization의 기본 아이디어)
    * 2차 근사는 2차 다변수함수의 최적점을 찾아서 이동하는 방식입니다.
* 이는 위 그림과 같이 minima로 더 빨리 수렴할 수 있다는 장점이 있다.
* 또다른 장점으로는 flat한 지역이 있을 때, 한 번에 많은 steps를 이동할 수 있다는 점입니다. 즉, loss surface에 따라 유연하게 대처할 수 있습니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786726-00d4c6df-c91e-4fc6-9410-47838585c566.png">
</p>

* 2차 근사함수를 사용하는 Optimization 방법을 Newton’s Method라고 합니다.
* 2차 미분 값들로 된 행렬인 Hessian Matrix를 계산하고 이 행렬의 inverse를 이용하게 되면, 실제 Loss함수의 2차 근사를 이용해 minima로 바로 이동할 수 있게 됩니다.
* 이와 같은 Newton’s Method에서는 단지 매 step마다 2차 근사 함수의 minima로 이동하면 되기 때문에, learning rate가 필요 없다는 특징이 있습니다.
    * 실제로는 minima로 이동하는게 아니라 minima의 방향으로 이동하는 것이기 때문에 learning rate가 필요하지만, 기본 형태에서는 learning rate가 없습니다.
* 이 방법은 딥러닝에 사용할 수 없습니다.
    * Hessian Matrix는 N X N의 크기인데, 여기서 N은 파라미터 수이다. 따라서, 이러한 큰 행렬을 메모리에 저장하는 것은 불가능하며, 역행렬을 구하는 것도 불가능합니다.

<br>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786898-4f2ec0d7-05e5-4da8-a056-fe400c81e156.png">
</p>

* 그래서 실제로는 Full Hessian을 Low-rank로 approximation하는 Quasi-Newton method를 사용하게 됩니다.

<br>





# Second-Order Optimization: L-BFGS

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786904-7ff4731c-5455-4d9a-ae22-671ab035fc61.png">
</p>

* Hessian Matrix를 근사시키는 방법을 사용한 Second-order optimization 방법으로는 L-BFGS가 있습니다.
* 그러나 Full batch에서는 잘 작동하지만, 소량의 샘플을 뽑아서 그래디언트/헤시안 매트릭스를 계산하는 mini-batch 환경에서는 잘 작동하지 않습니다. Large-scale에 2차근사 방법을 적용시키면, Stochastic setting의 노이즈가 커집니다.
* 또한 L-BFGS와 같은 이러한 2nd order opproximation은 stochastic case와 non-convex case에서 잘 동작하지 않기 때문에, DNN에서는 잘 사용되지 않습니다.

<br>





# Optimization Summary

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/77891754/185786912-ca162f20-c46d-4bb2-91f2-e4997d190a16.png">
</p>

* W의 최적점을 찾는 최적화 알고리즘으로 Adam을 주로 좋은 디폴트 초이스로 활용합니다.
* SGD + Momentum이 더 좋은 성능을 낼 수 있지만, 튜닝에의 더 많은 노력이 필요합니다.
* 또한 만약 full batch updates를 하게된다면, L-BFGS를 활용해보는 것도 좋습니다.

<br>
<br>
