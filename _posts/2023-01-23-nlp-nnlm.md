---
layout: post
title: 인공 신경망 언어 모델(NNLM)과 RNN 언어 모델(RNNLM)
category: NLP(Natural Language Processing)
tag: NLP(Natural Language Processing)
---




1. 기존 N-gram 언어 모델의 한계
2. 단어의 의미적 유사성
3. 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)
4. NNLM의 이점 : 희소 문제(sparsity problem) 해결
5. NNLM의 한계점 : 고정된 길이의 입력(Fixed-length input)
6. RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)

<br>
<br>




# 1. 기존 N-gram 언어 모델의 한계

**<span style="color:red">언어 모델(Language Model, LM)</span>** 이란 언어라는 현상을 모델링하고자 **<u>단어 시퀀스(문장)에 확률을 할당(assign)하는 모델</u>** 입니다. 이때 언어 모델의 전통적인 접근 방법인 **<span style="color:red">통계적 언어 모델(Statistical Language Model, SLM)</span>** 은 **<u>카운트에 기반하여 이전 단어로부터 다음 단어에 대한 확률을 계산</u>** 합니다.

다음은 이전 단어들로부터 다음 단어를 예측하는 **SLM 기반(N-gram)의 언어 모델링(Language Modeling)** 의 예를 보여줍니다.
```
# 다음 단어 예측하기
An adorable little boy is spreading ____
```

<br>

**n-gram** 언어 모델은 언어 모델링에 바로 앞 **n-1** 개의 단어를 참고합니다. **4-gram** 언어 모델이라고 가정했을때, 모델은 바로 앞 3개의 단어만 참고하며 더 앞의 단어들은 무시합니다. 이후 훈련 코퍼스에서 **(n-1)-gram** 을 카운트한 것을 분모로, **n-gram** 을 카운트한 것을 분자로 하여 다음 단어가 등장 확률을 예측합니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/4dfb529a-0514-4f2a-a8e6-3c4feaf282bc">
</p>

<br>


하지만 **<span style="color:red">SLM 기반의 언어 모델</span>** 은 **<u>충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제</u>** 가 있는데 이를 **<span style="color:red">희소 문제(sparsity problem)</span>** 라고 합니다. 


예를 들어 훈련 코퍼스에 **'boy is spreading smiles'** 라는 단어 시퀀스가 존재하지 않으면 n-gram 언어 모델에서 해당 단어 시퀀스의 확률 $P (smiles|boy\ is\ spreading)$ 는 0이 됩니다. 이는 언어 모델이 판단하기에 **'boy is spreading'** 다음에는 **'smiles'** 이란 단어가 나올 수 없다는 의미이지만, **<u>해당 단어 시퀀스는 현실에서 존재 가능한 시퀀스이므로 적절한 모델링이 아닙니다.</u>**

<br>




# 2. 단어의 의미적 유사성


**<span style="color:red">희소 문제는 기계가 단어의 의미적 유사성을 알 수 있다면 해결할 수 있는 문제</span>** 입니다. 예를 들어, 저는 최근 '톺아보다'라는 생소한 단어를 배웠고, '톺아보다'가 '샅샅이 살펴보다'와 유사한 의미임을 학습했습니다. 그리고 '발표 자료를 살펴보다'라는 표현 대신 '발표 자료를 톺아보다'라는 표현을 써봤습니다. 저는 '발표 자료를 톺아보다'라는 예문을 어디서 읽은 적은 없지만 **<span style="color:red">두 단어가 유사함을 학습</span>** 하였으므로 단어를 대신 선택하여 자연어 생성을 할 수 있었습니다.

기계도 마찬가지입니다. '발표 자료를 살펴보다'라는 단어 시퀀스는 존재하지만, '발표 자료를 톺아보다'라는 단어 시퀀스는 존재하지 않는 코퍼스를 학습한 언어 모델이 있다고 가정해본다면, 언어 모델은 아래 선택지에서 다음 단어를 예측해야 합니다.

$$
𝑃(톺아보다 | 발표\ 자료를)
$$

$$
𝑃(냠냠하다 | 발표\ 자료를)
$$

<br>

저는 '살펴보다'와 '톺아보다'의 유사성을 학습하였고 이를 근거로 두 선택지 중에서 '톺아보다'가 더 맞는 선택이라고 판단할 수 있습니다. 하지만 **n-gram** 언어 모델은 '발표 자료를' 다음에 '톺아보다'가 나올 확률 $P (톺아보다 | 발표\ 자료를)$ 를 0으로 연산합니다. **n-gram** 언어 모델은 **<span style="color:red">'살펴보다'와 '톺아보다'의 단어의 유사도를 알 수 없으므로 예측에 고려할 수 없습니다.</span>**

**<span style="color:red">만약 언어 모델 또한 단어의 의미적 유사성을 학습할 수 있도록 설계</span>** 한다면, **<u>훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측</u>** 을 할 수 있습니다. 그리고 이러한 아이디어를 반영한 언어 모델이 **<span style="color:red">신경망 언어 모델 NNLM</span>** 입니다. 그리고 이 아이디어는 **<u>단어 벡터 간 유사도를 구할 수 있는 벡터를 얻어내는</u>** **<span style="color:red">워드 임베딩(word embedding)</span>** 의 아이디어이기도 합니다. 

<br>

즉, **<span style="color:red">인공 신경망 언어 모델(NNLM)은 희소문제(sparsity problem)를 Word Embedding 으로 해결</span>** 하였습니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/87bdd325-4535-4f0c-9bd7-cf4f444c9557">
</p>

<br>





# 3. 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)

**NNLM** 이 언어 모델링을 학습하는 과정(훈련 과정에서 단어의 유사도를 학습)을 살펴보겠습니다.

<br>


예를 들어 **훈련 코퍼스** 에 **'what will the fat cat sit on'** 과 같은 문장이 있다고 했을때, 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측합니다. 훈련 과정에서는 **'what will the fat cat'** 이라는 단어 시퀀스가 입력으로 주어지면, 다음 단어 **'sit'** 을 예측하는 방식으로 훈련됩니다. 즉 **<span style="color:red">NNLM은 $n$ 개의 이전 단어들로부터 $n+1$ 단어를 예측하는 모델입니다.</span>** 신경망의 입력(기계가 단어를 인식)은 **원-핫 인코딩** 을 사용하여 얻은 **원-핫 벡터** 로 하고, 다음과 같은 예문을 훈련한다 해보겠습니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/ce1527c1-fce4-460f-8e80-286f0999a90c">
</p>

<br>

위 **원-핫 벡터** 들은 훈련을 위한 NNLM의 입력이면서 예측을 위한 레이블이 됩니다. 위 그림은 'what will the fat cat'를 입력을 받아서 'sit'을 예측하는 일은 기계에게 what, will, the, fat, cat의 원-핫 벡터를 입력받아 sit의 원-핫 벡터를 예측하는 문제입니다. 즉, **NNLM** 은 n-gram 언어 모델처럼 **<u>다음 단어를 예측할 때, 앞의 모든 단어를 참고하는 것이 아니라 정해진 개수의 단어만을 참고</u>** 합니다. 이때 이 범위를 **<span style="color:red">윈도우(window)</span>** 라고 하기도 하는데, 여기서 윈도우의 크기인 n은 4입니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/1481b30c-016c-49ce-9db6-8c1f4e16f056">
</p>

<br>




**NNLM의 구조** 를 보면 다음과 같습니다. NNLM은 다음 그림과 같이 총 4개의 **<span style="color:red">층(layer)으로 이루어진 인공 신경망</span>** 입니다. **입력층(input layer)** 을 보면 앞에서 윈도우의 크기는 4로 정하였으므로 입력은 4개의 단어 'will, the, fat, cat'의 원-핫 벡터입니다. **출력층(output layer)** 을 보면 모델이 예측해야하는 정답에 해당되는 단어 'sit'의 원-핫 벡터는 모델이 예측한 값의 오차를 구하기 위해 레이블로서 사용됩니다. 그리고 오차로부터 손실 함수를 사용하여 인공 신경망이 학습을 하게 됩니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/ab7b7405-14a3-4a91-a56a-88c931272c80">
</p>

<br>



4개의 원-핫 벡터를 입력 받은 **NNLM** 은 다음층인 **투사층(projection layer)** 을 지나게 됩니다. 인공 신경망에서 입력층과 출력층 사이의 층은 보통 은닉층이라고 부르는데, 여기서 투사층이라고 명명한 이 층은 일반 은닉층과 다르게 가중치 행렬과의 곱셈은 이루어지지만 활성화 함수가 존재하지 않습니다. **입력층(Input layer)** 과 **투사층(Projection layer)** 사이의 연산을 생각해보면, **<u>각각의 원-핫 벡터는 가중치 행렬과 곱해져서 투사층을 형성</u>** 할 것입니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/d1525296-004a-4297-9f49-ffdea58ba13b">
</p>

<br>



**투사층의 크기** 를 $M$ 으로 설정하면, $V$ 차원의 **원-핫벡터인 각 입력 단어** 들은 투사층에서 $V × M$ 크기의 **가중치 행렬** 과 곱해져 $M$ 차원의 벡터를 얻습니다. 여기서 $V$ 는 **단어 집합의 크기** 를 의미합니다. 만약 원-핫 벡터의 차원 $V$ 가 7이고, $M$ 이 5라면 가중치 행렬 $W$ 는 $7 × 5$ 행렬이 됩니다.

각 단어의 원-핫 벡터와 가중치 $W$ 행렬의 곱이 어떻게 이루어지는지 보겠습니다. 위 그림에서는 각 원-핫 벡터를 $x$ 로 표기하였습니다. 원-핫 벡터의 특성으로 인해 $i$ 번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 **<u>원-핫 벡터와 가중치 $W$ 행렬의 곱은 사실 $W$ 행렬의 $i$ 번째 행을 그대로 읽어오는 것과(lookup) 동일</u>** 합니다. 그래서 이 작업을 **<span style="color:red">룩업 테이블(lookup table)</span>** 이라고 합니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/ee39ec5b-4d47-4109-990f-515342aabee2">
</p>

<br>


**<span style="color:red">룩업 테이블(lookup table) 후에는 $V$ 차원을 가지는 원-핫 벡터는 $M$ 차원의 벡터로 맵핑</span>** 됩니다. 위 그림에서 단어 'fat' 을 의미하는 원-핫 벡터를 $x_{fat}$ 으로 표현했고, 테이블 룩업 과정을 거친 후의 단어 벡터는 $e_{fat}$ 으로 표현했습니다. **<span style="background-color: #fff5b1">이 벡터들은 초기에는 랜덤한 값을 가지지만 학습 과정에서 값이 계속 변경</span>** 되는데 **<span style="color:red">이 단어 벡터를 임베딩 벡터(embedding vector)</span>** 라고 합니다.




**Projection Layer** 는 신경망 그림으로 해석한다면 다음과 같습니다. 단, Weight Matrix는 존재하지만 Bias는 사용하지 않으며, 활성화 함수도 사용하지 않습니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/a73abb51-75f9-435a-914a-35581784a420">
</p>

<br>



즉, **<span style="color:red">각 단어를 의미하는 원-핫 벡터가 lookup table을 거치면 임베딩 벡터(embedding vector)로 변경되고, 투사층(Projection Layer)에서 모두 concatenate 됩니다.</span>** 여기서 concatenate란 벡터를 단순히 나열하는 것을 의미하는데, 예를 들어 5차원 벡터 4개를 concatenate하면 20차원 벡터가 됩니다. **<span style="color:red">벡터의 concatenate</span>** 는 딥러닝 연산에서 정보를 모두 사용한다는 의미로 **<u>두 개 이상의 정보(벡터)를 함께 사용하고자 할 때 쓰는 연산</u>** 입니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/acec3ff2-e305-4815-89be-fcab6a5c6512">
</p>

<br>


정리하면 **<span style="color:red">lookup table으로 차원이 축소된 각 단어 벡터들이 모두 연결되면 Projection Layer의 결과물입니다.</span>** $x$ 를 각 단어의 원-핫 벡터, NNLM이 예측하고자 하는 단어가 문장에서 $t$ 번째 단어라고 하고, 윈도우의 크기를 $n$, 룩업 테이블을 의미하는 함수를 $lookup$, 세미콜론(;)을 연결 기호로 하였을 때 투사층을 식으로 표현하면 아래와 같습니다.

$$
p^{layer}(투사층) = (lookup(x_{t-n}),\ ...;\ lookup(x_{t-2});\ lookup(x_{t-1})) \\
=(e_{t-n};\ ...;\ e_{t-2};\ e_{t-1})
$$

<br>

일반적인 은닉층이 활성화 함수를 사용하는 비선형층(nonlinear layer)인 것과는 달리 **<u>투사층은 활성화 함수가 존재하지 않는 선형층(linear layer)</u>** 이라는 점이 다소 생소하지만, 이 다음은 다시 은닉층을 사용하는 일반적인 피드 포워드 신경망과 동일합니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/c1492bbd-6bd3-4990-8d64-f79e1e25d40d">
</p>

<br>



투사층의 결과는 $h$ 의 크기를 가지는 은닉층을 지납니다. 이때의 가중치와 편향을 $W_h$ 와 $b_h$ 라고 하고, 은닉층의 활성화 함수를 하이퍼볼릭탄젠트 함수라고 하였을 때, 은닉층을 식으로 표현하면 아래와 같습니다.

$$
h^{layer}(은닉층) = tanh(W_h p^{layer} + b_h)
$$

<br>

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/a16c71d8-bcc4-4dc0-8bf8-4bb72422e129">
</p>

<br>



은닉층의 출력은 $V$ 의 크기를 가지는 출력층으로 향합니다. 출력층에서는 활성화 함수로 소프트맥스(softmax) 함수를 사용하는데, $V$ 차원의 벡터는 소프트맥스 함수를 지나면서 벡터의 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀝니다. 이 벡터를 **NNLM** 의 예측값이라는 의미에서 $\hat{y}$ 라고 할 때, 이를 식으로 표현하면 아래와 같습니다.

$$
\hat{y}(출력층) = softmax(W_y h^{layer} + b_y)
$$

<br>

벡터 $\hat{y}$ 의 각 차원 안에서의 값이 의미하는 것은 이와 같습니다. $\hat{y}$ 의 $j$ 번째 인덱스가 가진 0과 1사이의 값은 $j$ 번째 단어가 다음 단어일 확률을 나타냅니다. 그리고 $\hat{y}$ 는 실제값. **즉, 실제 정답에 해당되는 단어인 원-핫 벡터의 값에 가까워져야 합니다.** 실제값에 해당되는 다음 단어를 $y$ 라고 했을 때, 이 두 벡터가 가까워지게 하기위해서 **NNLM** 는 손실 함수로 크로스 엔트로피(cross-entropy) 함수를 사용합니다. 해당 문제는 단어 집합의 모든 단어라는 $V$ 개의 선택지 중 정답인 'sit'을 예측해야하는 다중 클래스 분류 문제입니다. 그리고 역전파가 이루어지면 모든 가중치 행렬들이 학습되는데, 여기에는 **<u>투사층에서의 가중치 행렬도 포함되므로 임베딩 벡터값 또한 학습</u>**됩니다.

<br>




# 4. NNLM의 이점 : 희소 문제(sparsity problem) 해결

만약 충분한 훈련 데이터가 있다는 가정 하에 **NNLM** 의 핵심은, **<span style="color:red">투사층의 가중치 행렬 $W$ 의 각 행은 각 단어와 맵핑되는 밀집 벡터(Dense Vector)</span>** 이므로, 충분한 양의 훈련 코퍼스를 학습한다면 결과적으로 수많은 문장에서 **<span style="color:red">유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게된다는 것</span>** 입니다. 이렇게 되면 **훈련이 끝난 후 다음 단어를 예측하는 과정에서 훈련 코퍼스에서 없던 단어 시퀀스라 하더라도 다음 단어를 선택할 수 있습니다.** 만약 a cute dog를 학습했다면, a cute cat이 훈련 데이터에 없더라도 a cute cat을 생성 가능합니다.


결과적으로 **<span style="color:red">NNLM은 단어를 표현하기 위해 임베딩 벡터를 사용하므로서 단어의 유사도를 계산할 수 있게되었으며</span>**, 이를 통해 **<span style="color:red">희소 문제(sparsity problem)를 해결</span>** 하였습니다. 단어 간 유사도를 구할 수 있는 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe 등으로 발전되어서 딥 러닝 자연어 처리 모델에서는 필수적으로 사용되는 방법이 되었습니다.


<br>





# 5. NNLM의 한계점 : 고정된 길이의 입력(Fixed-length input)

하지만 **NNLM** 의 한계점 또한 존재합니다. 즉 **NNLM** 은 희소 문제(sparsity problem)를 해결했지만 n-gram 언어 모델과 마찬가지로 **<u>다음 단어를 예측하기 위해 모든 이전 단어를 참고하는 것이 아닌</u>**, **<span style="color:red">정해진 $n$ 개의 단어만을 참고</span>** 할 수 있습니다. **<span style="color:red">즉, 입력의 길이가 고정된다는 단점</span>** 이 있습니다.

이 한계를 극복할 수 있는 언어 모델이 있는데, 바로 **RNN(Recurrent Neural Network)** 을 사용한 **RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)** 입니다.


<br>





# 6. RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)

**<span style="color:red">RNNLM(Recurrent Neural Network Language Model)</span>** 은 RNN으로 만든 언어 모델입니다.

<br>

**n-gram** 언어 모델과 **NNLM** 은 **<span style="color:red">고정된 개수의 단어만을 입력으로 받아야한다는 단점</span>** 이 있었습니다. **<span style="background-color: #fff5b1">하지만 시점(time step)이라는 개념이 도입된 RNN으로 언어 모델을 만들면 입력의 길이를 고정하지 않을 수 있습니다.</span>**


예를 들어 훈련 코퍼스에 'what will the fat cat sit on' 과 같은 문장이 있다고 해보겠습니다. 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측하는 모델입니다. 아래의 그림은 **RNNLM** 이 어떻게 이전 시점의 단어들과 현재 시점의 단어로 다음 단어를 예측하는지를 보여줍니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/3a361756-38f0-4036-975c-bc4a027ebc13">
</p>

<br>




**<span style="color:red">RNNLM</span>** 은 **기본적으로 예측 과정에서 이전 시점의 출력을 현재 시점의 입력으로 합니다.** RNNLM은 what을 입력받으면, will을 예측하고 이 will은 다음 시점의 입력이 되어 the를 예측합니다. 그리고 이 또한 다시 다음 시점의 입력이 됩니다. 결과적으로 네번째 시점에서 cat은 앞서 나온 what, will, the, fat이라는 시퀀스로 인해 결정된 단어입니다. 사실 **<span style="color:red">이 과정은 훈련이 끝난 모델의 테스트 과정 동안(실제 사용할 때)</span>** 의 이야기입니다.

훈련 과정에서는 이전 시점의 예측 결과를 다음 시점의 입력으로 넣으면서 예측하는 것이 아니라, what will the fat cat sit on라는 훈련 샘플이 있다면, what will the fat cat sit 시퀀스를 모델의 입력으로 넣으면, will the fat cat sit on를 예측하도록 훈련됩니다. will, the, fat, cat, sit, on는 각 시점의 레이블입니다. 이러한 RNN 훈련 기법을 **<span style="color:red">교사 강요(teacher forcing)</span>** 라고 합니다.

<br>

**<span style="color:red">교사 강요(teacher forcing)</span>** 란, **<u>테스트 과정에서 $t$ 시점의 출력이 $t+1$ 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 때 사용하는 훈련 기법</u>** 입니다.

**<span style="color:red">훈련할 때 교사 강요를 사용</span>** 할 경우, **모델이 $t$ 시점에서 예측한 값을 $t+1$ 시점에 입력으로 사용하지 않고**, **<span style="background-color: #fff5b1">$t$ 시점의 레이블. 즉, 실제 알고있는 정답을 $t+1$ 시점의 입력으로 사용합니다.</span>** 물론, 훈련 과정에서도 이전 시점의 출력을 다음 시점의 입력으로 사용하면서 훈련 시킬 수도 있지만 이는 한 번 잘못 예측하면 뒤에서의 예측까지 영향을 미쳐 훈련 시간이 느려지게 되므로, **<span style="color:red">교사 강요를 사용하여 RNN을 좀 더 빠르고 효과적으로 훈련시킬 수 있습니다.</span>**

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/0629fbf8-6fb7-4027-9414-f7b18c4ed562">
</p>

<br>


훈련 과정 동안 출력층에서 사용하는 활성화 함수는 소프트맥스 함수입니다. 그리고 모델이 예측한 값과 실제 레이블과의 오차를 계산하기 위해서 손실 함수로 크로스 엔트로피 함수를 사용합니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/50163768-19d0-404e-8e4f-645ada014b63">
</p>

<br>

RNNLM의 구조에서 우선 **입력층(input layer)** 을 보겠습니다. RNNLM의 현 시점(timestep)은 4로 가정합니다. 그래서 4번째 입력 단어인 **fat의 원-핫 벡터가 입력** 이 됩니다.

**출력층(output layer)** 을 보면, 모델이 예측해야하는 정답에 해당되는 단어 **cat의 원-핫 벡터** 는 출력층에서 모델이 예측한 값의 오차를 구하기 위해 사용될 예정입니다. 그리고 이 오차로부터 손실 함수를 사용해 인공 신경망이 학습을 하게 됩니다. 조금 더 구체적으로 살펴보겠습니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/53d20920-7317-4842-8a7d-9a6bd9648202">
</p>

<br>


현 시점의 입력 단어의 원-핫 벡터 $x_t$ 를 입력 받은 RNNLM은 우선 임베딩층(embedding layer)을 지납니다. 이 임베딩층은 임베딩 벡터를 얻는 투사층(projection layer)입니다. 단어 집합의 크기가 $V$ 일 때, 임베딩 벡터의 크기를 $M$ 으로 설정하면, 각 입력 단어들은 임베딩층에서 $V × M$ 크기의 임베딩 행렬과 곱해집니다. 만약 원-핫 벡터의 차원이 7이고, M이 5라면 임베딩 행렬은 7 × 5 행렬이 됩니다. 그리고 이 임베딩 행렬은 역전파 과정에서 다른 가중치들과 함께 학습됩니다.

$$
e_t(임베딩층) = lookup(x_t)
$$

<br>

이 임베딩 벡터는 은닉층에서 이전 시점의 은닉 상태인 $h_{t-1}$ 과 함께 다음의 연산을 하여 현재 시점의 은닉 상태 $h_t$ 를 계산하게 됩니다.

$$
h_t(은닉층) = tanh(W_x e_t + W_h h_{t-1} + b)
$$

<br>

출력층에서는 활성화 함수로 소프트맥스(softmax) 함수를 사용하는데, $V$ 차원의 벡터는 소프트맥스 함수를 지나면서 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀝니다. 이렇게 나온 벡터를 RNNLM의 $t$ 시점의 예측값이라는 의미에서 $\hat{y_t}$ 라고 합시다. 이를 식으로 표현하면 아래와 같습니다.

$$
\hat{y_t}(은닉층) = softmax(W_y h_t + b)
$$

<br>


벡터 $\hat{y_t}$ 의 각 차원 안에서의 값이 의미하는 것은 이와 같습니다. $\hat{y_t}$ 의 $j$ 번째 인덱스가 가진 0과 1사이의 값은 $j$ 번째 단어가 다음 단어일 확률을 나타냅니다. 그리고 $\hat{y_t}$ 는 실제값. 즉, 실제 정답에 해당되는 단어인 원-핫 벡터의 값에 가까워져야 합니다. 실제값에 해당되는 다음 단어를 $y$ 라고 했을 때, 이 두 벡터가 가까워지게 하기위해서 RNNLM는 손실 함수로 cross-entropy 함수를 사용합니다. 그리고 역전파가 이루어지면서 가중치 행렬들이 학습되는데, 이 과정에서 임베딩 벡터값들도 학습이 됩니다.


룩업 테이블의 대상이 되는 테이블인 임베딩 행렬을 $E$ 라고 하였을 때, 결과적으로 RNNLM에서 학습 과정에서 학습되는 가중치 행렬은 다음의 $E$, $W_x$, $W_h$, $W_y$ 4개 입니다.























