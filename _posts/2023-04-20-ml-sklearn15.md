---
layout: post
title: 회귀(regression)의 이해
category: Machine Learning
tag: Machine-Learning
---






지도학습은 크게 두 가지 유형(분류와 회귀)으로 나뉘는데, 이 두 가지 기법의 가장 큰 차이는 **<span style="color:red">분류</span>** 는 예측값이 카테고리와 같은 **<span style="background-color: #fff5b1">이산형 클래스 값</span>** 이고, **<span style="color:red">회귀</span>** 는 **<span style="background-color: #fff5b1">연속형 숫자 값</span>** 이라는 것입니다. 회귀(regression)에 대해 알아보겠습니다.

<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/8ffbbf1e-d53f-42e2-8343-166ae56e1c26">
</p>

<br>




# 회귀(regression)


**<span style="color:red">회귀(regression)</span>** 는 **<span style="background-color: #fff5b1">여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭</span>** 합니다.


$$
Y=W_1*X_1+W_2*X_2+...+W_n*X_n
$$


예를 들어 아파트의 방 개수, 방 크기, 주변 학군 등 여러 개의 독립변수에 따라 아파트 가격이라는 종속변수가 어떤 관계를 나타내는지를 모델링하고 예측하는 것입니다.

* $Y$ : **<span style="background-color: #fff5b1">종속변수</span>**, 아파트 가격을 의미
* $X$ : **<span style="background-color: #fff5b1">독립변수</span>**, 방 개수, 방 크기, 주변 학군 등을 의미
* $W$ : 독립변수의 값에 영향을 미치는 **<span style="background-color: #fff5b1">회귀 계수(Regression coefficients)</span>**




머신러닝 관점에서 보면 **<u>독립변수는 피처</u>** 에 해당되며 **<u>종속변수는 결정 값</u>** 입니다. **<span style="color:red">머신러닝 회귀 예측의 핵심</span>** 은 주어진 피처와 결정 값 데이터 기반에서 **<span style="color:red">학습을 통해 최적의 회귀 계수를 찾아내는 것</span>** 입니다.


<br>




회귀는 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 여러 가지 유형으로 나눌 수 있습니다.

| 독립변수 개수 | 회귀 계수의 결합 |
| ---------  | ------------ |
| 1개: 단일 회귀 | 선형: 선형 회귀 |
| 여러 개: 다중 회귀 | 비선형: 비선형 회귀 |



여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용되며, 선형 회귀는 실제 값과 예측값의 차이(오류의 제곱 값)를 최소화하는 직선형 회귀선을 최적화하는 방식입니다. 대표적인 선형 회귀모델은 다음과 같습니다.

* **일반 선형 회귀** : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화한 모델.
* **릿지(Ridge)** : L2 규제를 추가한 회귀 모델. L2 규제는 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해서 회귀 계수값을 더 작게 만드는 규제 모델.
* **라쏘(Lasso)** : L1 규제를 추가한 회귀 모델. L1 규제는 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 규제 모델.
* **엘라스틱넷(ElasticNet)** : L2, L1 규제를 함께 결합한 모델입니다. L1 규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값의 크기를 조정 하는 규제 모델.
* **로지스틱 회귀(Logistic Regression)** : 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형 모델.



단순 선형 회귀를 예로 들어 회귀를 살펴보겠습니다.

<br>





# 단순 선형 회귀(Simple linear regression)



**<span style="color:red">단순 선형 회귀</span>** 는 **<span style="background-color: #fff5b1">독립변수도 하나</span>**, **<span style="background-color: #fff5b1">종속변수도 하나</span>** 인 선형 회귀입니다. 예를 들어, 주택 가격($\hat{Y}$)이 주택의 크기($X$)로만 결정된다고 할 때, 일반적으로 주택의 크기가 크면 가격이 높아지는 경향이 있기 때문에 주택 가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현할 수 있습니다. 즉, **<u>특정 기울기와 절편을 가진 1차 함수식으로 모델링(독립변수가 1개인 단순 선형 회귀 모델)</u>** 할 수 있습니다.(**<span style="background-color: #fff5b1">$\hat{Y}=w_0+w_1*X$</span>**)


<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/fb44aa9b-f02f-4a58-81c4-fa38c14f7853">
</p>

<br>



**실제 값($Y$)은** 예측 값($\hat{Y}$)에서 실제 값만큼의 오류 값을 뺀(또는 더한) 값이 됩니다.(**<span style="background-color: #fff5b1">$\hat{Y}=w_0+w_1*X+오류값$</span>**) 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, 즉 **<span style="color:red">잔차(오류 값, Error)</span>** 라고 지칭합니다.


**<span style="color:red">최적의 회귀모델을 만든다는 것</span>** 은 바로 **<span style="background-color: #fff5b1">전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델</span>** 을 만든다는 의미입니다. 동시에 **<span style="background-color: #fff5b1">오류 값 합이 최소가 될 수 있는 최적의 회귀 계수</span>** 를 찾는다는 의미도 됩니다.


<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/05e2fea8-41ad-41ab-afe5-f19abb6930ab">
</p>

<br>



오류 값은 $+$ 나 $-$ 가 될 수 있기에, **<u>전체 데이터의 오류 합을 구하기 위해 단순히 더했다가는 뜻하지 않게 오류의 합이 크게 줄어들 수 있습니다.</u>** 따라서 보통 오류 합을 계산할 때는 오류 값의 제곱을 구해서 더하는 방식(**<span style="color:red">$RSS$</span>**, Residual Sum of Square)을 취합니다. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 $RSS$ 방식으로 오류 합을 구합니다. 즉, **<span style="background-color: #fff5b1">$Error^2 = RSS$</span>**


<p align="center">
<img alt="image" src="https://github.com/museonghwang/museonghwang.github.io/assets/77891754/e89c146b-2aa2-4b66-9504-cdcb4a3bbe2c">
</p>

<br>





$RSS$ 는 이제 변수가 $w_0, w_1$ 인 식으로 표현할 수 있으며, 이 **<span style="color:red">$RSS$ 를 최소로 하는 $w_0, w_1$, 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항</span>** 입니다. $RSS$ 는 회귀식의 독립변수 $X$, 종속변수 $Y$ 가 중심 변수가 아니라, **<span style="background-color: #fff5b1">$w$ 변수(회귀 계수)가 중심 변수임을 인지하는 것이 매우 중요</span>** 합니다(학습 데이터로 입력되는 독립변수와 종속변수는 $RSS$ 에서 모두 상수로 간주). 일반적으로 $RSS$ 는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.

<br>

$$
RSS(w_0, W_1) = \frac{1}{N}	\sum_{k=1}^N (y_i - (w_0 + w_1 * x_i))^2
$$

<br>

회귀에서 이 **<span style="color:red">$RSS$</span>** 는 **<span style="background-color: #fff5b1">비용(Cost)</span>** 이며 $w$ 변수(회귀 계수)로 구성되는 $RSS$ 를 **<span style="background-color: #fff5b1">비용함수</span>** 또는 **<span style="background-color: #fff5b1">손실함수(loss function)</span>** 라고 합니다.

**<span style="color:red">머신러닝 회귀 알고리즘</span>** 은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류 값)을 지속해서 감소시키고, **<span style="background-color: #fff5b1">최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것</span>** 입니다.






